{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problems\n",
    "\n",
    "**Regression problems** are problems where we try to predict a numerical output value given some input datapoint based on some examples of input datapoints that have known outputs.\n",
    "\n",
    "## Examples\n",
    "\n",
    "1. If we have a dataset of workers in a certain position of numbers of years of experience as an input and salary, we might want to predict salaries we don't know based on the years of experience a person has.\n",
    "\n",
    "1. If we have a dataset of variables about houses (floorspace, number of bedrooms, number of bathrooms, number of stories, age of the house) along with their selling prices, we might want to predict selling prices of homes not in the dataset based on the other variables about the house.\n",
    "\n",
    "1. If we have a dataset of variables about countries (average salary, average education of citizens, death rate, birth rate, infant mortality rate, etc.) along with their GDP, we might want to predict the GDP of countires not in the dataset based on the other variables about the country.\n",
    "\n",
    "1. If we have a dataset of seasonal variables about NBA basketball teams (points per game, turnovers per game, point differential, rebounds per game, blocks per game, etc.) and the numbers of wins they had in different seasons, we might want to take the statistics of a team early in a season to try to predict the number of wins they will have.\n",
    "\n",
    "## Types of Regression Problems\n",
    "\n",
    "There are different types of regression problems based on the numbers of input variables and output variables.\n",
    "\n",
    "* A **simple** regression problem predicts an output variable with just one input variable like Example 1.\n",
    "\n",
    "* A **multiple** regression problem predicts an output variable with more than one input variable like Examples 2 and 3.\n",
    "\n",
    "* A **multivariate** regression problem predicts more than one output variables\n",
    "\n",
    "We will not discuss multivariate regression this week, but we will show how neural networks can be used for these problems in the future.\n",
    "\n",
    "## The Math of a Regression Problem\n",
    "\n",
    "All of these regression problems have some things in common: there are example datapoints with outputs and we want to predict the outputs for new datapoints. Consider a $d$-dimensional point, or vector, $x_1\\in\\mathbb{R}^d$ and denote $x_1=(x_{11},x_{12},...,x_{1d})$. $x_1$ maps to an output $y_1\\in\\mathbb{R}$. We call the point $x_1$ an **example** and we call $y_1$ the **output** of $x_1$. (In statistics, the components of the vector $x_1$ are more frequently called **predictors** or **independent variables** and the $y_i$ values are more frequently called the **response** or **dependent variable**.)\n",
    "\n",
    "In a perfect world, a solution to a (univariate) regression problem will find a function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that can do two things:\n",
    "\n",
    "1. Mapping each example $x_i$ in a dataset to its output $y_i=f(x_i)$\n",
    "1. Generalize to successfully predict outputs of new datapoints\n",
    "\n",
    "In reality, $f$ will not always map each input $x_i$ values to each $y_i$ value perfectly or generalize to new inputs perfectly, but we try to get as close to this ideal as possible.\n",
    "\n",
    "## Regression Algorithms\n",
    "\n",
    "There are a number of popular approaches to regression problems, including the following.\n",
    "\n",
    "* least squares regression\n",
    "* lasso regression\n",
    "* ridge regression\n",
    "* support vector machines\n",
    "* neural networks\n",
    "\n",
    "This week, we will consider the first three approaches as they are quite similar. As an added bonus, they all use a numerical optimization scheme called stochastic gradient descent, which is one of the main algorithms needed to make neural networks work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "We will assume the function $f$ we aim to predict is linear in some parameters $\\beta_0,...,\\beta_d$ so that our predicted function will be\n",
    "\n",
    "$$\n",
    "f(x_i)=\\beta_0 + \\sum\\limits_{k=1}^d \\beta_k x_{ik}=\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_d x_{id}\n",
    "$$\n",
    "\n",
    "and we will try to choose $\\beta_0,...,\\beta_d$ that will minimize a loss function on a training dataset $(x_1,y_1),...,(x_n,y_n)$. This loss function will be small if each $f(x_i)$ is near each $y_i$, which is what we want\n",
    "\n",
    "### Note\n",
    "\n",
    "It is a common misconception that \"linear regression\" must fit linear functions to data, but the \"linear\" part of linear regression refers to the fact that $f$ is linear with respect to $\\beta_0,...,\\beta_d$, not with respect to $x_i$, so it is certainly possible to apply some preprocessing to the datapoints.\n",
    "\n",
    "For example, if each $x_i\\in\\mathbb{R}^1$, we can fit a parabola by manipulating each $x_i$ into $x_i^*=(x_i,x_i^2)$ so that our predicted function would be\n",
    "\n",
    "$$f(x_i)=\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2.$$\n",
    "\n",
    "While we will discuss only $x=(x_1,...,x_d)$, keep in mind that we can always create new variables from the data, preprocess the data into different forms, and so on. This topics go beyond the scope of this course, but I want you to realize linear regression can learn to represent functions far beyond simply lines and planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression by Ordinary Least Squares\n",
    "\n",
    "Recall that we have a labeled dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with each $x_i\\in\\mathbb{R}^d$ and each $y_i\\in\\mathbb{R}$ and our goal is to find a function $f$ that maps the $x_i$'s to the $y_i$'s as well as possible, and, we hope, effective at mapping unknown datapoints to appropriate outputs.\n",
    "\n",
    "In the **ordinary least squares** method, we try to minimize a the **sum of squared differences** between the real outputs $y_1, ..., y_n$ and the predicted outputs $f(x_1)$, ..., $f(x_n)$. In other words, the loss function in this method is\n",
    "\n",
    "$$L(\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2 = \\sum\\limits_{i=1}^n \\left(x_i^T\\beta-y_i\\right)^2,$$\n",
    "\n",
    "which we will call a **loss function**, where\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\\beta=\\begin{pmatrix}\n",
    "\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is the same as what we used with classification problems except we added a column of ones to the left. Each row is a datapoint $x_i\\in\\mathbb{R}^d$ representing one example and each column represents one variable (also known as a predictor or a feature). Note that we will only add the column of ones within the class so that we can continue with the norms we established for $X$ last week. Given this, we can rewrite the loss function as\n",
    "\n",
    "$$L(\\beta)=\\|X\\beta-y\\|^2_2=(X\\beta-y)^T(X\\beta-y),$$\n",
    "\n",
    "where the $T$ subscript represents the **transpose** of a matrix. Here, $X$ and $y$ are constants derived from the dataset, so the $\\beta$ values are the only unknowns, so we need to find the $\\beta$ values that minimize the loss function $L$. In other words, we need to solve a minimization problem\n",
    "\n",
    "$$\\min\\limits_\\beta\\,L(\\beta)$$\n",
    "\n",
    "or, more specifically,\n",
    "\n",
    "$$\\min\\limits_\\beta\\,(X\\beta - y)^T(X\\beta - y).$$\n",
    "\n",
    "We should note that these $\\beta$ values are called **parameters** of the model. They are NOT hyperparameters. The difference is:\n",
    "\n",
    "1. Parameters are values that are estimated by the model automatically from the data.\n",
    "1. Hyperparameters are values that we set before the model learns from the data.\n",
    "\n",
    "There are no hyperparameters in our loss function. So, in contrast to the $k$-nearest neighbors classifier, which had a hyperparameter but no parameters, ordinary least squares has some parameters but no hyperparameters. However, choosing a way to normalize the data (or not) could be considered a hyperparameter.\n",
    "\n",
    "In multivariate calculus, the approach to minimize a differentiable function is to take derivatives with respect to $\\beta_0, ..., \\beta_d$, set them all equal to zero, and solve. This may seem difficult, but we can actually do it for ordinary least squares. Before we take derivatives, let's convert the los function into a (longer but) simpler form. Transposes can be applied to sums of matrices separately, so\n",
    "\n",
    "$$L(\\beta)=((X\\beta)^T-y^T)(X\\beta-y).$$\n",
    "\n",
    "Using the distributive property of matrix multiplication,\n",
    "\n",
    "$$L(\\beta)=(X\\beta)^T X\\beta-(X\\beta)^T y-y^T X\\beta+y^T y$$\n",
    "\n",
    "If we realize $y$ and $X\\beta$ are both matrices of shape $n\\times 1$, then we should have $(X\\beta)^T y=y^T X\\beta$, so the loss function is\n",
    "\n",
    "$$L(\\beta)=(X\\beta)^T X\\beta-2(X\\beta)^T y+y^T y$$\n",
    "\n",
    "Since $(AB)^T=B^TA^T$ for matrices, we can simplify the terms as\n",
    "\n",
    "$$L(\\beta)=\\beta^T X^T X\\beta-2\\beta^T X^T y+y^T y$$\n",
    "\n",
    "Now, of course, this is a scalar (because, in the end, the loss is just a number--the sum of squared errors), so we can take the derivatives with respect to $\\beta_1$, ..., $\\beta_d$ and put those into a vector as\n",
    "\n",
    "$$\\nabla L(\\beta)=2X^T X\\beta-2X^T y$$\n",
    "\n",
    "As in multivariate calculus, we set this whole vector equal to 0 and solve for the estimated version of $\\beta$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2X^T X\\beta-2X^T y &= 0\\\\\n",
    "X^T X\\beta &= X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, if $X^T X$ is an invertible matrix, then we can multiply both sides of the equation by its inverse to solve for $\\beta$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X^T X)^{-1}(X^T X\\beta) &= (X^T X)^{-1}X^T y\\\\\n",
    "\\beta &=(X^T X)^{-1}X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "At long last, we have a formula for the exact solution for the $\\beta$ values that minimize the loss function.\n",
    "\n",
    "#### Linear Algebra Notes\n",
    "\n",
    "* The formula above holds only if the inverse of the matrix $X^T X$ exists. Assuming $n\\geq d$, the inverse exists when the columns of $X$ are linearly independent. (See <a href=\"https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/lin-alg-showing-that-a-transpose-x-a-is-invertible\">this video</a> or pretty much any linear algebra book.)\n",
    "\n",
    "* If you have studied linear algebra, you might recognize $(X^T X)^{-1}X^T$ is the <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">Moore-Penrose pseudoinverse</a> of $X$.\n",
    "\n",
    "Anyway, the formula for $\\beta$ does not look so nice to do by hand since it requires a matrix multiplication, a matrix inverse, and two more matrix multiplications, but a computer can complete these tasks quickly. (The <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">best algorithms</a> for matrix multiplication and matrix inverses for $n\\times n$ matrices each have computational complexity less than $O(n^3)$, so doing a few of these is no problem, even for quite large matrices.)\n",
    "\n",
    "## Performance Metrics for Linear Regression\n",
    "\n",
    "Before we use this class to apply the ordinary least squares method on some examples, let's consider a few performance metrics in common usage for regression.\n",
    "\n",
    "With linear regression, we generally have the unfortunate situation that the model we construct is not perfect even on the training set. Therefore, we first need to consider the performance on the training data to which it was fit just to see how well the model fits to the training data. The formulas for the common metrics are not particularly interesting, so we just state what they represent and what value they should ideally have:\n",
    "\n",
    "* **Coefficient of determination** $R^2$ - the fraction of the variation in the data explained by the model. It is, in a sense, a measure of the strength of the linear relationship between the variables. Ideally, it will be near 1.\n",
    "\n",
    "* **Mean squared error (MSE)** - the mean of the squared errors between the points and the fitted function, i.e. the loss function of the ordinary least squares loss function divided by the number of examples in the training data, $n$. Ideally, it will be as small as possible.\n",
    "\n",
    "* **Mean absolute error (MAE)** - the mean of the absolute errors between the points and the fitted function. Ideally, it will be as small as possible.\n",
    "\n",
    "If these values are far from their ideal values for the training set, the model does not even fit the training data well, so it probably will not fit the testing data well. The ordinary least squares solution finds the optimal parameters for a linear fit, so poor performance on the training set means the data do not have a strong linear relationship.\n",
    "\n",
    "Some preprocessing of the data might make it work better. For example, you can apply a logarithm to a variable if there's a linear relationship with that variable on a log scale. It's beyond the scope of our class, but see, for example, the (free) classic book <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">*Elements of Statistical Learning*</a> by Hastie, et. al., section 2.6.3 for an introduction on linear basis expansions.\n",
    "\n",
    "Second, we need to consider the performance on the testing data. We generally should consider the MSE or MAE.\n",
    "\n",
    "## Ordinary Least Squares Code\n",
    "\n",
    "Before we write some code for ordinary least squares, let's import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
    "np.set_printoptions(linewidth=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class for using ordinary least squares in this way to fit the model and to predict outputs for unknown inputs. We will use the scikit-learn pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresExact:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Example\n",
    "\n",
    "Let's make up some 1D data and test the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted y values are [1.2 1.9 2.6 3.3 4. ]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The beta values are [-3.   0.7]\n",
      "The r^2 score is 0.9423076923076923\n",
      "The mean squared error is 0.060000000000000074\n",
      "The mean absolute error is 0.20000000000000212\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = model.beta\n",
    "print('The beta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Example\n",
    "\n",
    "Let's make up some 2D data and test to see if our method works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.9642679900744415\n",
      "The mean squared error on the training set is 4.608000000000017\n",
      "The mean absolute error on the training set is 1.5360000000000298\n",
      "The predicted y values for the test set are [-6.52 19.08  6.6  32.2 ]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The beta values are [-3.56 -6.24  9.52]\n",
      "The mean squared error on the test set is 149.3792000000069\n",
      "The mean absolute error on the test set is 9.799999999999962\n"
     ]
    }
   ],
   "source": [
    "trainX = np.array([[2, 2], [2, 3], [5, 6], [6, 7], [9, 10]])\n",
    "trainY = np.array([3, 13, 19, 29, 35])\n",
    "\n",
    "testX = np.array([[2, 1], [4, 5], [6, 5], [8, 9]])\n",
    "testY = np.array([9, 15, 25, 31])\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a low dimensional problem like this one, we can plot the points along with the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'y')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "# scatter plot of the test data\n",
    "x1 = testX[:,0]\n",
    "x2 = testX[:,1]\n",
    "y = testY\n",
    "ax.scatter(x1, x2, y, marker = 'o')\n",
    "\n",
    "# scatter plot of the training data\n",
    "x1 = trainX[:,0]\n",
    "x2 = trainX[:,1]\n",
    "y = trainY\n",
    "ax.scatter(x1, x2, y, marker = '^')\n",
    "\n",
    "# plot the plane we fit to the data\n",
    "beta = model.beta\n",
    "\n",
    "# surface plot\n",
    "x1 = np.linspace(0,10,10)\n",
    "x2 = np.linspace(0,10,10)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "#Y = beta[0]*X1 + beta[1]*X2\n",
    "Y = beta[0] + beta[1]*X1 + beta[2]*X2\n",
    "\n",
    "surf = ax.plot_wireframe(X1, X2, Y)\n",
    "ax.view_init(10, 50)\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: High School Graduation Rates in US States\n",
    "\n",
    "Let's try to use ordinary least squares on a real dataset. The CSV file in '/data/US_State_data.csv' contains data from each U.S. state.\n",
    "\n",
    "We would like to predict the output variable included, the high school graduation rate, from some input variables: including the crime rate (per 100,000 persons), the violent crime rate (per 100,000 persons), average teacher salary, student-to-teacher ratio, education expenditure per student, population density, and median household income.\n",
    "\n",
    "This means we have 50 examples (one for each state), 7 input (predictor) variables, and one output (response) variable. In order to use the formula we derived above to attack the problem with ordinary least squares, we need to find the matrices $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.3921626378110833\n",
      "The mean squared error on the training set is 19.59820392039356\n",
      "The mean absolute error on the training set is 3.796456104985703\n",
      "The predicted y values for the test set are [80. 80. 89. 89. 80. 81. 85. 93. 75. 84. 84. 79. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The beta values are [ 1.16286460e+02 -6.09008374e-03  3.98286852e-03 -1.63697058e-04 -2.97328939e-01 -4.74750536e-04  1.09164594e-02]\n",
      "The mean squared error on the test set is 28.314919261809344\n",
      "The mean absolute error on the test set is 4.64393583870603\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pandas.read_csv('data/US_State_Data.csv', sep=',').to_numpy()\n",
    "#print(data)\n",
    "X = np.array(data[:,1:7], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "#trainX = normalize(trainX)\n",
    "#testX = normalize(testX)\n",
    "\n",
    "#trainX = scale(trainX)\n",
    "#testX = scale(testX)\n",
    "\n",
    "# run the model (same code as above)\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares by Gradient Descent\n",
    "\n",
    "While this approach to ordinary least squares does provide the global minimum of the loss function, there is another approach that uses numerical optimization called **gradient descent**, which is a very quick method exploiting some pretty simple ideas from multivariable calculus. SGD can approximately solve a minimization problem for a differentiable function, such as the ordinary least squares . While it will only find an approximate answer at best, it is practically good enough in most cases. The benefits are that it is computationally cheap and can be used for many other useful loss functions.\n",
    "\n",
    "Gradient descent (and it's sped-up version, **stochastic gradient descent**) is *heavily* used in machine learning. Along with backpropagation, SGD is the primary method used for training neural networks. In fact, the first practical neural networks were written about in the machine learning literature under a class of methods called \"gradient-based learning\" due to the primacy of SGD and related methods.\n",
    "\n",
    "### Some Ideas from Multivariate Calculus\n",
    "\n",
    "First, we need just a few ideas from multivariate calculus. This is quite minimal, but you learn more details about these topics in sections 4.3 (partial derivatives), 4.6 (gradients), and 4.7 (multivariate optimization) of <a href=\"https://openstax.org/details/books/calculus-volume-3\">*Calculus Volume 3*</a> by Strang.\n",
    "\n",
    "The ideas we need for gradient descent include:\n",
    "\n",
    "* If we have a differentiable function of several variables, like our loss function $L(\\beta) = L(\\beta_0, ..., \\beta_d)$, we can define the **partial derivatives** with respect to each of these variables as\n",
    "\n",
    "    $$L_{\\beta_i}=\\frac{\\partial L}{\\partial \\beta_i}=\\lim\\limits_{h\\to 0}\\frac{L(\\beta+he_i) - L(\\beta)}{h},$$\n",
    "\n",
    "    where $e_i$ is a $(d+1)$-vector with all 0s except for a 1 in the $i$th component.  Geometrically, this partial derivative is the slope of $L$ if we go in the direction of $e_i$.\n",
    "    \n",
    "* To **minimize a multivariable function** by hand, we need to find critical points, which are points $\\beta$ where *all* partial derivatives are 0 and compare which ones give the lowest outputs. In numerical algorithms, must settle for approximations that are \"nearly\" critical points.\n",
    "    \n",
    "* If we put these partial derivatives into a vector of $d+1$ variables, we call that a **gradient**, which we denote\n",
    "    \n",
    "    $$\n",
    "    \\nabla L(\\beta)\n",
    "    =\\begin{pmatrix}\n",
    "    L_{\\beta_0}(\\beta) \\\\\n",
    "    \\vdots \\\\\n",
    "    L_{\\beta_d}(\\beta)\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    \n",
    "* The **directional derivative** of a function in the direction of a unit vector $u$ starting from a point $\\beta$.\n",
    "\n",
    "    $$D_u L(\\beta) = \\lim\\limits_{h\\to 0}\\frac{L(\\beta+hu) - L(\\beta)}{h},$$\n",
    "    \n",
    "    which is the slope in the direction of the vector $u$. Since $L$ is differentiable, this directional derivative will be defined for all directions leaving from $\\beta$. In 1D, there are just two directions: left of right. In 2D, we have directional derivatives at every angle in a circle around the point.\n",
    "    \n",
    "* A common theorem says the **directional derivative is maximized in the direction of the gradient** at each point, so the gradient gives the direction of the *steepest ascent* in the function $L$. Similarly, the direction of the *steepest descent* is $-\\nabla L(\\beta)$, the opposite direction.\n",
    "\n",
    "### The Geometry of Gradient Descent\n",
    "\n",
    "We will discuss the geometry of gradient-based methods in class, but let's discuss a general outline of how gradient descent works, setting aside the stochastic version for now. The goal of gradient descent is to approximately solve the minimization problem\n",
    "\n",
    "$$\\min\\limits_{\\beta}\\,L(\\beta)$$\n",
    "\n",
    "by finding (approximate) critical values by making a guess for the location of a critical value, taking a small step in the opposite direction as the gradient, and repeating this over and over until, hopefully, we reach a minimum value.\n",
    "\n",
    "The steps are:\n",
    "\n",
    "0. Make a guess for the critical value -- $\\beta^0$\n",
    "1. Compute the gradient of $L$ at $\\beta^0$\n",
    "2. Take a small step to $\\beta^1 = \\beta^0 - \\alpha\\nabla L\\left(\\beta^0\\right)$\n",
    "3. Compute the gradient of $L$ at $\\beta^1$\n",
    "4. Take a small step to $\\beta^2 = \\beta^1 - \\alpha\\nabla L\\left(\\beta^1\\right)$\n",
    "5. (repeat until the gradient gets close to $(0, ..., 0)$)\n",
    "\n",
    "This $\\alpha>0$ is a number that will be used in the algorithm as a multiplier of the steps the method will take. This is called the **learning rate**.\n",
    "\n",
    "This idea seems plausible from the calculus ideas above because we just keep switching directions and making a step in the direction of the steepest downward path--the opposite direction as the gradient--until we reach a good place. This is a \"greedy\" algorithm because it just picks the quickest step in each iteration, which is fast, but it is likely to land in the first minimum it finds, which may or may not be optimal.\n",
    "\n",
    "If you had two parameters, $L$ would be like a 3D curved surface. A nice visual to have in mind is a rain drop falling on a huge leaf. The droplet of water will move in the steepest downward direction due to gravity--but this direction *changes* as the drip follows the contours of the leaf. This is what gradient descent does.\n",
    "\n",
    "Will the drip land in the physically lowest altitude part of the leaf? Maybe, but maybe not. If the rain drop lands on the edge, it will probably just roll off the edge. If the leaf has a few different \"sinks,\" different initial locations of the rain drop might cause it to land in these different ones, some of which have lower altitudes than others. Now, if there is heavy rain and lots of rain drops land on the leaf, we can be pretty sure *some* of them will reach the lowest-altitude sink.\n",
    "\n",
    "From this analogy, you might get the idea that we can make several initial guesses and run it to be more confident we will find the global minimum and not just a local minimum.\n",
    "\n",
    "In the end, if some of our initial guesses are good choices, the step size $\\alpha$ is not too big or too small, and the loss function is pretty well-behaved, the method will converge approximately to a local minimum.\n",
    "\n",
    "### Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient\n",
    "def computeGradient(f, x, h):\n",
    "    n = len(x)\n",
    "    gradient = np.zeros(n)\n",
    "    \n",
    "    for counter in range(n):\n",
    "        xUp = x.copy()\n",
    "        xUp[counter] += h\n",
    "        gradient[counter] = (f(xUp) - f(x))/h\n",
    "            \n",
    "    return gradient\n",
    "\n",
    "# run gradient descent ant output the \n",
    "def gradientDescent(f, x0, alpha, h, tolerance, maxIterations):\n",
    "    # set x equal to the initial guess\n",
    "    x = x0\n",
    "                \n",
    "    # take up to maxIterations number of steps\n",
    "    for counter in range(maxIterations):\n",
    "        # update the gradient\n",
    "        gradient = computeGradient(f, x, h)\n",
    "        \n",
    "        # stop if the norm of the gradient is near 0\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            print('Gradient descent took', counter, 'iterations to converge')\n",
    "            print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "            # return the approximate critical value x\n",
    "            return x\n",
    "        \n",
    "        # if we do not converge, print a message\n",
    "        elif counter == maxIterations-1:\n",
    "            print(\"Gradient descent failed\")\n",
    "            print('The gradient is', gradient)\n",
    "            # return x, sometimes it is still pretty good\n",
    "            return x\n",
    "        \n",
    "        # take a step in the opposite direction as the gradient\n",
    "        x -= alpha*gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on some simple functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 10 iterations to converge\n",
      "The norm of the gradient is 4.5055999998641627e-07\n",
      "[-0.19999977] 0.03999990988805076\n"
     ]
    }
   ],
   "source": [
    "f = lambda x : x[0]**2\n",
    "\n",
    "x = gradientDescent(f,[2],0.4,0.4,0.000001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient descent took 18 iterations to converge\n",
      "The norm of the gradient is 6.221664077488143e-05\n",
      "[4.46232611] [-0.96889687]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: np.sin(x)\n",
    "\n",
    "x = gradientDescent(f,[2],0.5,0.5,0.0001,10000)\n",
    "print(x, f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a new class for regression using gradient descent rather than the exact solution for $\\beta$. What we will hopefully see is that gradient descent will yield similar results on the examples above.\n",
    "\n",
    "We would never want to trade the computationally cheap exact answer above for a gradient-based approximation for ordinary least squares, but gradient descent has wide applicability beyond this setting. As we will see this week, there are common variations of the ordinary least squares loss function that can help reduce the dimension of the data and improve the method's fit to test data.\n",
    "\n",
    "However, it's good to have some problems where we know the exact solutions to verify the results are the same before moving on to more complex situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresGradient:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, x0, alpha, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.initialGuess = x0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)\n",
    "        # self.beta = self.gradientDescent(L,(self.d +  1) * [0], h, tolerance, maxIterations)\n",
    "        self.beta = self.gradientDescent(L, self.initialGuess, self.alpha, self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, h, tolerance, maxIterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == maxIterations-1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "    # compute the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - f(x))/h\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D Example (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "Gradient descent took 17180 iterations to converge\n",
      "The norm of the gradient is 0.009999533018135179\n",
      "\n",
      "The predicted y values are [1.88927362 2.39728623 2.90529883 3.41331144 2.39728623]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The beta values are [-1.15880201  0.5080126 ]\n",
      "The r^2 score is 0.2890134587732638\n",
      "The mean squared error is 0.7394260028758058\n",
      "The mean absolute error is 0.6794572453892247 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The predicted y values are [1.84615385 2.38461538 2.92307692 3.46153846 2.38461538]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The beta values are [-1.38461538  0.53846154]\n",
      "The r^2 score is 0.289940828402367\n",
      "The mean squared error is 0.7384615384615385\n",
      "The mean absolute error is 0.6769230769230757\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [7]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS (gradient) object, fit to data, predict data\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "model.fit(X, y, [0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('\\nThe predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = model.beta\n",
    "print('The beta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions),'\\n')\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS (exact) object, fit to data, predict data\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = model.beta\n",
    "print('The beta values are', parameters)\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'g', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Example (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "Gradient descent took 12984 iterations to converge\n",
      "The norm of the gradient is 0.009998702083919297\n",
      "\n",
      "The r^2 score is 0.9642661609185357\n",
      "The mean squared error on the training set is 4.608235887945642\n",
      "The predicted y values for the test set are [-6.44748058 19.07004759  6.67858093 32.19610911]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The beta values are [-3.53326263 -6.19573333  9.47724871]\n",
      "The mean squared error on the test set is 148.07375435455714 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The r^2 score is 0.9642679900744415\n",
      "The mean squared error on the training set is 4.608000000000017\n",
      "The predicted y values for the test set are [-6.52 19.08  6.6  32.2 ]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The beta values are [-3.56 -6.24  9.52]\n",
      "The mean squared error on the test set is 149.3792000000069\n"
     ]
    }
   ],
   "source": [
    "trainX = np.array([[2, 2], [2, 3], [5, 6], [6, 7], [9, 10]])\n",
    "trainY = np.array([3, 13, 19, 29, 35])\n",
    "\n",
    "testX = np.array([[2, 1], [4, 5], [6, 5], [8, 9]])\n",
    "testY = np.array([9, 15, 25, 31])\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY, [0, 0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the beta values\n",
    "betaApprox = model.beta\n",
    "print('The beta values are', betaApprox)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions), '\\n')\n",
    "\n",
    "##############################################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the beta values\n",
    "betaExact = model.beta\n",
    "print('The beta values are', betaExact)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: High School Graduate Rates in the US (Revisited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "Gradient descent took 786 iterations to converge\n",
      "The norm of the gradient is 0.009904894592102644\n",
      "\n",
      "The r^2 score is 0.39216258491472866\n",
      "The mean absolute error on the training set is 3.7965640305341184\n",
      "The predicted y values for the test set are [81. 80. 89. 86. 81. 82. 85. 90. 76. 84. 84. 80. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The beta values are [82.97247297 -3.79844358  0.52284534 -1.27291957 -1.296429   -1.71354363  2.21637078]\n",
      "The mean absolute error on the test set is 4.050315901439818 \n",
      "\n",
      "FOR THE EXACT ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "The r^2 score is 0.3921626378110832\n",
      "The mean absolute error on the training set is 3.7964561049855736\n",
      "The predicted y values for the test set are [81. 80. 89. 86. 81. 82. 85. 90. 76. 84. 84. 80. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The beta values are [82.97297297 -3.79814523  0.52333579 -1.27337115 -1.2954766  -1.71237515  2.21674878]\n",
      "The mean absolute error on the test set is 4.050567187865438\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pandas.read_csv('data/US_State_Data.csv', sep=',').to_numpy()\n",
    "#print(data)\n",
    "X = np.array(data[:,1:7], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "trainX = scale(trainX)\n",
    "testX = scale(testX)\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY, [0, 0, 0, 0, 0, 0, 0], alpha = 0.001, h = 0.001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "##################################################################\n",
    "\n",
    "print('FOR THE EXACT ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Gradient Descent\n",
    "\n",
    "* We get almost identical results in all of these examples in using a gradient-based method for ordinary least squares.\n",
    "\n",
    "* It runs a little slower, but not much.\n",
    "\n",
    "* We must be careful with the $h$ and tolerance hyperparameters to be sure gradient descent will converge.\n",
    "\n",
    "* Gradient descent in our implementation above does not actually require any derivatives since we only used approximate derivatives.\n",
    "\n",
    "* If we knew formulas for the derivatives, we could compute them exactly to let the step size be exactly proportional to $\\nabla L$. This would drastically reduce the number of times we compute the loss function.\n",
    "\n",
    "* Gradient descent and related methods are the main driver of many machine learning problems that are based on to minimizing a loss function (least squares and neural networks, among others), although we will later need some variants to reduce the computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Dependence: When Ordinary Least Squares Fails\n",
    "\n",
    "Recall the global minimum of the ordinary least squares loss function we found previously, $\\beta=(X^T X)^{-1}X^T y$, relied on the assumption that the columns of $X$ are **linearly independent**, which means no column is a linear combination of any set of other columns--i.e. that we do not have\n",
    "\n",
    "$$c_1(\\text{column }n_1) + \\cdots + c_k(\\text{column }n_k)=0$$\n",
    "\n",
    "for any nonzero constants $c_1, ..., c_k$ or any subset of the columns of $X$.\n",
    "\n",
    "This may seem to be an unlikely scenario since we deal with some large datasets, but it turns out, this is a more frequent occurrence than one might think.\n",
    "\n",
    "* In the high school graduation data above, we had violent crime rates and property crime rates. If we had the overall crime rate as well, ordinary least squares would have failed because we classify all crime as violent crime or property crime:\n",
    "\n",
    "$$(\\text{crime}) = (\\text{violent crime}) + (\\text{property crime})$$\n",
    "\n",
    "    Therefore, the columns of $X$ are not linearly independent.\n",
    "\n",
    "* Suppose we have demographic data from several small countries including an age distribution giving us the number of people in the population of ages 0-17, 18-35, 36-65, and 66+, and also the population of the country.\n",
    "\n",
    "    | Country | Age 0-17 | Age 18-35 | Age 36-65 | Age 66+ | Population|\n",
    "    |:----------|:------|:------|:------|:------|:------|\n",
    "    | Country 1 | 20000 | 30000 | 30000 | 20000 | 100000 | \n",
    "    | Country 2 | 50000 | 70000 | 60000 | 20000 | 200000 |\n",
    "    | Country 2 | 30000 | 30000 | 50000 | 10000 | 120000 |\n",
    "\n",
    "    Then, we have columns that are not independent since\n",
    "    \n",
    "$$(\\text{Age 0-17}) + (\\text{Age 18-35}) + (\\text{Age 36-65}) + (\\text{Age 65+}) = (\\text{Population}).$$\n",
    "\n",
    "* If we have a dataset of houses and their characteristics, it is common practice for realtors to categorize bathrooms as half bathrooms (bathrooms with no showers/bath tubs) or full bathrooms and then also report the total number of bathrooms. Here, again, we naturally get non-independent columns because\n",
    "\n",
    "$$\\text{bathrooms} = \\frac{1}{2}(\\text{half bathrooms}) + (\\text{full bathrooms}).$$\n",
    "\n",
    "From these examples, we see this phenomenon of linear dependence, sometimes called collinearity in regression analysis, naturally occurs in datasets all over the place. When this happens, again, we cannot find the minimum of the ordinary least squares loss function, so we cannot use the method in a rather common scenario. As you might suspect, this sort of pattern tends to be even more likely for high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Regularization** is an approach of adjusting a model in various ways to deal with collinearity among other problems. Another important effect of regularization is that it can sometimes reduce over-fitting, a problem where we fit the model to the training data too strongly causing it to perform badly on test data.\n",
    "\n",
    "For example, in the image below, a regularized model would look more like the green curve, which is much simpler and *probably* captures the real dynamics of the system generating the data better than the blue curve, even though they both fit the training data (the red dots) perfectly.\n",
    "\n",
    "<img src=\"images/regularization.png\" width=\"400\" />\n",
    "\n",
    "In general, regularization typically sacrifices some training accuracy through approaches that reduce the dimension of the parameter space, shrink some parameters, and adjust loss functions in such a way that it improves how well the model generalizes to test data and other unknown data.\n",
    "\n",
    "We will want to add regularization to our toolbox of machine learning as it is a common technique, especially whenever we turn our learning problem into a loss function minimization problem, which is what neural networks will do. There are a few common regularization methods use in linear regression, which mirror its usage in neural networks, that we will discuss below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "**Ridge regression** (also called **Tikhonov regularization** or $L^2$ **regularization**) offers a partial remedy for the problems of linear dependence and overfitting by adjusting the loss function so we can solve it in such a way that we, in effect, reduce the dimension of the space.\n",
    "\n",
    "Now, we could reduce the dimension by setting $\\beta_i=0$ for certain variables manually, but we generally do not know which variables are important, so selecting these is not easy and generally requires some iterative procedure, which would be computationally expensive if the dimension is high. Further, removing variables entirely does not allow us to see what effect they have (even if the effect is small).\n",
    "\n",
    "Instead, we would like to \"encourage\" the $\\beta_i$'s to be small in a continuous sort of way by **penalizing** large $\\beta_i$'s. To do this, instead of the ordinary least squares loss function\n",
    "\n",
    "$$L(\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2= \\|X\\beta-y\\|_2^2,$$\n",
    "\n",
    "we add another term to the loss function to get\n",
    "\n",
    "$$L_{\\text{ridge}}(\\lambda,\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2 + \\lambda\\sum\\limits_{i=1}^d\\beta_i^2=\\|X\\beta-y\\|_2^2+\\lambda\\|\\beta\\|_2^2,$$\n",
    "\n",
    "for some constant $\\lambda\\geq 0$. So, here, if $\\beta_i$ is large, the loss will be larger. Therefore, when we minimize the loss function, it will push $\\beta_i$'s toward 0 unless there is a really good reason not to do so. That's why we say we *penalize* large $\\beta$ values.\n",
    "\n",
    "The value $\\lambda$ is a hyperparameter of ridge regression. As $\\lambda\\to 0$, ridge regression approaches ordinary least squares. If $\\lambda$ is very large, optimizing the loss function will force the $\\beta$ values to be small. So, the larger we make $\\lambda$, the more pressure we put on the $\\beta$ parameters to shrink.\n",
    "\n",
    "### Note\n",
    "\n",
    "For ridge regression, you typically should normalize the data before minimizing the loss function. Otherwise, different scaling will cause minimization to penalize some variables more than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LASSO Regression and Elastic Net Regression\n",
    "\n",
    "**LASSO** (least absolute shrinkage and selection operator) regression is very similar to ridge regression, but it can otherwise be called $L^1$ regularization because it adds an $L^1$ penalty to the size of the $\\beta$ parameters, so the loss function is\n",
    "\n",
    "$$L_{\\text{lasso}}(\\lambda,\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2 + \\lambda\\sum\\limits_{i=1}^d |\\beta_i|=\\|X\\beta-y\\|_2^2+\\lambda\\|\\beta\\|_1.$$\n",
    "\n",
    "**Elastic net** regression combines both $L^1$ and $L^2$ regularization as a linear combination. So, the elastic net loss function is\n",
    "\n",
    "$$L_{\\text{elastic}}(\\lambda_1,\\lambda_2,\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2 + \\lambda_1\\sum\\limits_{i=1}^d |\\beta_i| \\lambda_2\\sum\\limits_{i=1}^d \\beta_i^2=\\|X\\beta-y\\|_2^2+\\lambda_1\\|\\beta\\|_1+\\lambda_2\\|\\beta\\|_2^2,$$\n",
    "\n",
    "so it contains two hyperparameters $\\lambda_1\\geq 0$ and $\\lambda_2\\geq 0$ controlling how large the $L^1$ and $L^2$ penalties are.\n",
    "\n",
    "## Ridge vs. Lasso vs. Elastic Net Regression\n",
    "\n",
    "Ridge, lasso, and elastic net regression aim to accomplish most of the same tasks:\n",
    "\n",
    "1. Predict outputs when there are linearly dependent variables\n",
    "2. Reduce the dimension of the data (practically speaking)\n",
    "3. improve how well a model can generalize to test data and beyond\n",
    "\n",
    "So, which one should we use? That's not really an easy question to answer because the usefulness of a model depends on how well it can generalize to unknown datapoints, but they are unknown, so... we don't know much about them!\n",
    "\n",
    "There are some differences between $L^1$ and $L^2$ penalties that can guide our testing, although only testing is likely to tell us what will work better, practically speaking.\n",
    "\n",
    "* A loss function with an $L^1$ penalty is NOT differentiable when any $\\beta_i=0$, so we have to be careful with gradient-based methods. A method called soft-thresholding is often used to send parameters directly to 0 if the gradient method brings a $\\beta_i$ sufficiently close close to 0.\n",
    "\n",
    "* A loss function with an $L^2$ penalty will not cause parameters to go to 0, but $L^1$ can. (We will talk about some of the geometry of why this is true in class. It's also discussed in some videos about <a href=\"https://www.youtube.com/watch?v=5asL5Eq2x0A\">ridge</a> and <a href=\"https://www.youtube.com/watch?v=jbwSCwoT51M\">lasso</a> regression.)\n",
    "\n",
    "* There is no simple formula for $\\beta$ minimizing the loss functions in lasso or elastic net regression, so numerical optimization, such as gradient descent, must be used.\n",
    "\n",
    "* There is a matrix expression for $\\beta$ minimizing the loss function in ridge regression (see Homework 2), but gradient descent can be used as well.\n",
    "\n",
    "This means $L^1$ can totally eliminate variables, which can be good or bad depending on what we are modeling. If the output is well-predicted by only a few variables, this is good. If we need lots of variables to predict the output, this is bad.\n",
    "\n",
    "These notes only provide a short intro to regularization techniques, but you can read more from the (free) classic book <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">*Elements of Statistical Learning*</a> by Hastie, et. al., in section 3.4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
