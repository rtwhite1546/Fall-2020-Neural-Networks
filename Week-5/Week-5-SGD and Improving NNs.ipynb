{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving How Neural Networks Learn\n",
    "\n",
    "For the next couple of weeks, we will look at some ways to improve how feedforward neural networks learn. In all cases, we will want to use stochastic gradient descent (SGD) rather than ordinary gradient descent because it will speed up training.\n",
    "\n",
    "Once we implement SGD with backpropagation, we will have constructed a \"vanilla\" neural network, which is probably the simplest one that is practical.\n",
    "\n",
    "Now, many aspects of the neural networks we have implemented are actually customizable. There are many, many adaptations that have been made in various problems, but we will cover some of the most effective known adjustments, including the following.\n",
    "\n",
    "* Alternative loss functions -- sometimes improves training speed and accuracy\n",
    "* Regularization methods -- helps with over-fitting\n",
    "* Alternative activation functions -- sometimes improves training speed\n",
    "* Initialization strategies -- sometimes improves training speed and convergence\n",
    "\n",
    "First, let's import some packages we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Nets with SGD\n",
    "\n",
    "The method from last week would incredibly slow for the full full-size MNIST dataset, but there is one piece we did not implement: stochastic gradient descent. We are feeding every single example into the net, feeding it forward, running backprop, and making a weight update. Instead, we will make weight updates based on random mini-batches of datapoints.\n",
    "\n",
    "Below, we add in a function to create mini-batches of images to read to the class we wrote last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    \n",
    "    # input a vector [a, b, c, ...] with the number of nodes in each layer\n",
    "    def __init__(self, layers, alpha = 0.1, batchSize = 32):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # initialize the weights (randomly) -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            self.W.append(np.random.randn(layers[i] + 1, layers[i + 1] + 1))\n",
    "            \n",
    "        # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "        self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
    "        \n",
    "    # define the sigmoid activation\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    # define the sigmoid derivative (where z is the output of a sigmoid)\n",
    "    def sigmoidDerivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, epochs = 10000, update = 1000):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "\n",
    "        for epoch in np.arange(0,epochs):\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0,X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.sigmoid(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    \n",
    "                # backpropagation (coming soon!)\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                D = [error * self.sigmoidDerivative(A[-1])]\n",
    "                \n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.sigmoidDerivative(A[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
    "                    \n",
    "            if (epoch + 1) % update == 0:\n",
    "                loss = self.computeLoss(X,y)\n",
    "                print(\"[INFO] epoch = {}, loss = {:.6f}\".format(epoch + 1, loss))\n",
    "                \n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        loss = np.sum((predictions - y)**2) / 2.0\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 1006.665768\n",
      "[INFO] epoch = 20, loss = 715.632151\n",
      "[INFO] epoch = 30, loss = 469.793882\n",
      "[INFO] epoch = 40, loss = 427.946123\n",
      "[INFO] epoch = 50, loss = 101.818410\n",
      "[INFO] epoch = 60, loss = 200.957703\n",
      "[INFO] epoch = 70, loss = 75.889761\n",
      "[INFO] epoch = 80, loss = 50.939743\n",
      "[INFO] epoch = 90, loss = 43.231690\n",
      "[INFO] epoch = 100, loss = 41.476832\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       751\n",
      "           1       1.00      0.98      0.99       835\n",
      "           2       0.98      0.99      0.98       760\n",
      "           3       0.99      0.99      0.99       771\n",
      "           4       0.99      1.00      0.99       756\n",
      "           5       0.99      0.99      0.99       648\n",
      "           6       0.99      1.00      0.99       753\n",
      "           7       0.99      0.99      0.99       802\n",
      "           8       0.99      0.99      0.99       702\n",
      "           9       0.99      0.98      0.99       722\n",
      "\n",
      "    accuracy                           0.99      7500\n",
      "   macro avg       0.99      0.99      0.99      7500\n",
      "weighted avg       0.99      0.99      0.99      7500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       250\n",
      "           1       0.97      0.97      0.97       292\n",
      "           2       0.90      0.90      0.90       231\n",
      "           3       0.93      0.91      0.92       261\n",
      "           4       0.93      0.96      0.94       224\n",
      "           5       0.92      0.91      0.91       215\n",
      "           6       0.93      0.97      0.95       261\n",
      "           7       0.94      0.94      0.94       268\n",
      "           8       0.92      0.89      0.90       242\n",
      "           9       0.92      0.91      0.92       256\n",
      "\n",
      "    accuracy                           0.93      2500\n",
      "   macro avg       0.93      0.93      0.93      2500\n",
      "weighted avg       0.93      0.93      0.93      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,100,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train for more epochs and see if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 680.959643\n",
      "[INFO] epoch = 20, loss = 204.763718\n",
      "[INFO] epoch = 30, loss = 175.239784\n",
      "[INFO] epoch = 40, loss = 152.383397\n",
      "[INFO] epoch = 50, loss = 73.337793\n",
      "[INFO] epoch = 60, loss = 61.043240\n",
      "[INFO] epoch = 70, loss = 48.287076\n",
      "[INFO] epoch = 80, loss = 44.288870\n",
      "[INFO] epoch = 90, loss = 41.800286\n",
      "[INFO] epoch = 100, loss = 53.489791\n",
      "[INFO] epoch = 110, loss = 245.309725\n",
      "[INFO] epoch = 120, loss = 160.131553\n",
      "[INFO] epoch = 130, loss = 81.348472\n",
      "[INFO] epoch = 140, loss = 42.997131\n",
      "[INFO] epoch = 150, loss = 35.503840\n",
      "[INFO] epoch = 160, loss = 34.130473\n",
      "[INFO] epoch = 170, loss = 55.296433\n",
      "[INFO] epoch = 180, loss = 38.633015\n",
      "[INFO] epoch = 190, loss = 30.294394\n",
      "[INFO] epoch = 200, loss = 29.276146\n",
      "[INFO] epoch = 210, loss = 28.271499\n",
      "[INFO] epoch = 220, loss = 27.940187\n",
      "[INFO] epoch = 230, loss = 27.761435\n",
      "[INFO] epoch = 240, loss = 27.299578\n",
      "[INFO] epoch = 250, loss = 27.196518\n",
      "[INFO] epoch = 260, loss = 26.999102\n",
      "[INFO] epoch = 270, loss = 26.804968\n",
      "[INFO] epoch = 280, loss = 26.602077\n",
      "[INFO] epoch = 290, loss = 26.546572\n",
      "[INFO] epoch = 300, loss = 26.509001\n",
      "[INFO] epoch = 310, loss = 26.477657\n",
      "[INFO] epoch = 320, loss = 26.450822\n",
      "[INFO] epoch = 330, loss = 26.427595\n",
      "[INFO] epoch = 340, loss = 26.404768\n",
      "[INFO] epoch = 350, loss = 26.385710\n",
      "[INFO] epoch = 360, loss = 26.367674\n",
      "[INFO] epoch = 370, loss = 26.351884\n",
      "[INFO] epoch = 380, loss = 26.336235\n",
      "[INFO] epoch = 390, loss = 26.322241\n",
      "[INFO] epoch = 400, loss = 26.307786\n",
      "[INFO] epoch = 410, loss = 26.292394\n",
      "[INFO] epoch = 420, loss = 26.264250\n",
      "[INFO] epoch = 430, loss = 25.957510\n",
      "[INFO] epoch = 440, loss = 25.861160\n",
      "[INFO] epoch = 450, loss = 25.808829\n",
      "[INFO] epoch = 460, loss = 25.775943\n",
      "[INFO] epoch = 470, loss = 25.722981\n",
      "[INFO] epoch = 480, loss = 25.442755\n",
      "[INFO] epoch = 490, loss = 25.348436\n",
      "[INFO] epoch = 500, loss = 25.316355\n",
      "[INFO] epoch = 510, loss = 25.293577\n",
      "[INFO] epoch = 520, loss = 25.274411\n",
      "[INFO] epoch = 530, loss = 25.250335\n",
      "[INFO] epoch = 540, loss = 25.215847\n",
      "[INFO] epoch = 550, loss = 25.150307\n",
      "[INFO] epoch = 560, loss = 25.146553\n",
      "[INFO] epoch = 570, loss = 24.929017\n",
      "[INFO] epoch = 580, loss = 24.831232\n",
      "[INFO] epoch = 590, loss = 24.826609\n",
      "[INFO] epoch = 600, loss = 24.789611\n",
      "[INFO] epoch = 610, loss = 24.770011\n",
      "[INFO] epoch = 620, loss = 24.754644\n",
      "[INFO] epoch = 630, loss = 24.742748\n",
      "[INFO] epoch = 640, loss = 24.732400\n",
      "[INFO] epoch = 650, loss = 24.723429\n",
      "[INFO] epoch = 660, loss = 24.715255\n",
      "[INFO] epoch = 670, loss = 24.707843\n",
      "[INFO] epoch = 680, loss = 24.700804\n",
      "[INFO] epoch = 690, loss = 24.693967\n",
      "[INFO] epoch = 700, loss = 24.686537\n",
      "[INFO] epoch = 710, loss = 24.676415\n",
      "[INFO] epoch = 720, loss = 24.646236\n",
      "[INFO] epoch = 730, loss = 24.312893\n",
      "[INFO] epoch = 740, loss = 24.246267\n",
      "[INFO] epoch = 750, loss = 24.224228\n",
      "[INFO] epoch = 760, loss = 24.210515\n",
      "[INFO] epoch = 770, loss = 24.200028\n",
      "[INFO] epoch = 780, loss = 24.189825\n",
      "[INFO] epoch = 790, loss = 24.174497\n",
      "[INFO] epoch = 800, loss = 24.080127\n",
      "[INFO] epoch = 810, loss = 23.806196\n",
      "[INFO] epoch = 820, loss = 23.751905\n",
      "[INFO] epoch = 830, loss = 23.728517\n",
      "[INFO] epoch = 840, loss = 23.712327\n",
      "[INFO] epoch = 850, loss = 23.701068\n",
      "[INFO] epoch = 860, loss = 23.692058\n",
      "[INFO] epoch = 870, loss = 23.684802\n",
      "[INFO] epoch = 880, loss = 23.678588\n",
      "[INFO] epoch = 890, loss = 23.673107\n",
      "[INFO] epoch = 900, loss = 23.668106\n",
      "[INFO] epoch = 910, loss = 23.663242\n",
      "[INFO] epoch = 920, loss = 23.658911\n",
      "[INFO] epoch = 930, loss = 23.655198\n",
      "[INFO] epoch = 940, loss = 23.651792\n",
      "[INFO] epoch = 950, loss = 23.648583\n",
      "[INFO] epoch = 960, loss = 23.645578\n",
      "[INFO] epoch = 970, loss = 23.642756\n",
      "[INFO] epoch = 980, loss = 23.640081\n",
      "[INFO] epoch = 990, loss = 23.637489\n",
      "[INFO] epoch = 1000, loss = 23.635043\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       738\n",
      "           1       1.00      0.99      0.99       834\n",
      "           2       1.00      0.99      0.99       742\n",
      "           3       1.00      0.99      0.99       791\n",
      "           4       0.99      1.00      0.99       730\n",
      "           5       0.99      1.00      0.99       648\n",
      "           6       1.00      0.99      1.00       760\n",
      "           7       0.99      1.00      0.99       776\n",
      "           8       0.99      1.00      0.99       723\n",
      "           9       1.00      0.99      0.99       758\n",
      "\n",
      "    accuracy                           0.99      7500\n",
      "   macro avg       0.99      0.99      0.99      7500\n",
      "weighted avg       0.99      0.99      0.99      7500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       263\n",
      "           1       0.96      0.98      0.97       293\n",
      "           2       0.92      0.93      0.92       249\n",
      "           3       0.93      0.91      0.92       241\n",
      "           4       0.94      0.93      0.93       250\n",
      "           5       0.89      0.89      0.89       215\n",
      "           6       0.95      0.96      0.95       254\n",
      "           7       0.94      0.94      0.94       294\n",
      "           8       0.92      0.84      0.88       221\n",
      "           9       0.89      0.92      0.91       220\n",
      "\n",
      "    accuracy                           0.93      2500\n",
      "   macro avg       0.93      0.93      0.93      2500\n",
      "weighted avg       0.93      0.93      0.93      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach leads to a testing accuracy of 93% for the full MNIST dataset.\n",
    "\n",
    "Sometimes we get a better result by adding layers to the network or increasing the number of nodes in layers, but sometimes it leads to more computation for not much gain, so let's see if it helps for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 1245.671410\n",
      "[INFO] epoch = 20, loss = 472.016254\n",
      "[INFO] epoch = 30, loss = 405.070409\n",
      "[INFO] epoch = 40, loss = 98.840420\n",
      "[INFO] epoch = 50, loss = 51.696259\n",
      "[INFO] epoch = 60, loss = 33.487088\n",
      "[INFO] epoch = 70, loss = 30.991897\n",
      "[INFO] epoch = 80, loss = 28.566550\n",
      "[INFO] epoch = 90, loss = 27.811007\n",
      "[INFO] epoch = 100, loss = 27.183523\n",
      "[INFO] epoch = 110, loss = 26.921858\n",
      "[INFO] epoch = 120, loss = 26.246120\n",
      "[INFO] epoch = 130, loss = 25.538169\n",
      "[INFO] epoch = 140, loss = 24.126143\n",
      "[INFO] epoch = 150, loss = 23.405379\n",
      "[INFO] epoch = 160, loss = 22.832374\n",
      "[INFO] epoch = 170, loss = 22.591054\n",
      "[INFO] epoch = 180, loss = 22.010312\n",
      "[INFO] epoch = 190, loss = 20.923296\n",
      "[INFO] epoch = 200, loss = 20.832632\n",
      "[INFO] epoch = 210, loss = 20.361917\n",
      "[INFO] epoch = 220, loss = 19.808306\n",
      "[INFO] epoch = 230, loss = 19.761570\n",
      "[INFO] epoch = 240, loss = 19.727841\n",
      "[INFO] epoch = 250, loss = 19.239642\n",
      "[INFO] epoch = 260, loss = 19.211704\n",
      "[INFO] epoch = 270, loss = 19.193265\n",
      "[INFO] epoch = 280, loss = 19.178030\n",
      "[INFO] epoch = 290, loss = 19.163176\n",
      "[INFO] epoch = 300, loss = 19.127452\n",
      "[INFO] epoch = 310, loss = 18.840401\n",
      "[INFO] epoch = 320, loss = 18.734493\n",
      "[INFO] epoch = 330, loss = 18.699379\n",
      "[INFO] epoch = 340, loss = 18.679392\n",
      "[INFO] epoch = 350, loss = 18.665221\n",
      "[INFO] epoch = 360, loss = 18.654177\n",
      "[INFO] epoch = 370, loss = 18.645009\n",
      "[INFO] epoch = 380, loss = 18.636946\n",
      "[INFO] epoch = 390, loss = 18.629545\n",
      "[INFO] epoch = 400, loss = 18.622716\n",
      "[INFO] epoch = 410, loss = 18.616886\n",
      "[INFO] epoch = 420, loss = 18.611965\n",
      "[INFO] epoch = 430, loss = 18.607653\n",
      "[INFO] epoch = 440, loss = 18.603763\n",
      "[INFO] epoch = 450, loss = 18.600218\n",
      "[INFO] epoch = 460, loss = 18.596951\n",
      "[INFO] epoch = 470, loss = 18.593924\n",
      "[INFO] epoch = 480, loss = 18.591098\n",
      "[INFO] epoch = 490, loss = 18.588456\n",
      "[INFO] epoch = 500, loss = 18.585973\n",
      "[INFO] epoch = 510, loss = 18.583632\n",
      "[INFO] epoch = 520, loss = 18.581418\n",
      "[INFO] epoch = 530, loss = 18.579320\n",
      "[INFO] epoch = 540, loss = 18.577323\n",
      "[INFO] epoch = 550, loss = 18.575415\n",
      "[INFO] epoch = 560, loss = 18.573587\n",
      "[INFO] epoch = 570, loss = 18.571822\n",
      "[INFO] epoch = 580, loss = 18.570098\n",
      "[INFO] epoch = 590, loss = 18.568379\n",
      "[INFO] epoch = 600, loss = 18.566579\n",
      "[INFO] epoch = 610, loss = 18.564411\n",
      "[INFO] epoch = 620, loss = 18.560446\n",
      "[INFO] epoch = 630, loss = 18.540120\n",
      "[INFO] epoch = 640, loss = 18.144908\n",
      "[INFO] epoch = 650, loss = 18.107827\n",
      "[INFO] epoch = 660, loss = 18.094843\n",
      "[INFO] epoch = 670, loss = 18.086813\n",
      "[INFO] epoch = 680, loss = 18.081190\n",
      "[INFO] epoch = 690, loss = 18.076809\n",
      "[INFO] epoch = 700, loss = 18.073364\n",
      "[INFO] epoch = 710, loss = 18.070508\n",
      "[INFO] epoch = 720, loss = 18.068024\n",
      "[INFO] epoch = 730, loss = 18.065855\n",
      "[INFO] epoch = 740, loss = 18.063918\n",
      "[INFO] epoch = 750, loss = 18.062144\n",
      "[INFO] epoch = 760, loss = 18.060516\n",
      "[INFO] epoch = 770, loss = 18.059010\n",
      "[INFO] epoch = 780, loss = 18.057598\n",
      "[INFO] epoch = 790, loss = 18.056269\n",
      "[INFO] epoch = 800, loss = 18.055004\n",
      "[INFO] epoch = 810, loss = 18.053795\n",
      "[INFO] epoch = 820, loss = 18.052630\n",
      "[INFO] epoch = 830, loss = 18.051497\n",
      "[INFO] epoch = 840, loss = 18.050381\n",
      "[INFO] epoch = 850, loss = 18.049265\n",
      "[INFO] epoch = 860, loss = 18.048120\n",
      "[INFO] epoch = 870, loss = 18.046896\n",
      "[INFO] epoch = 880, loss = 18.045493\n",
      "[INFO] epoch = 890, loss = 18.043651\n",
      "[INFO] epoch = 900, loss = 18.040461\n",
      "[INFO] epoch = 910, loss = 18.029284\n",
      "[INFO] epoch = 920, loss = 17.767201\n",
      "[INFO] epoch = 930, loss = 17.616295\n",
      "[INFO] epoch = 940, loss = 17.591858\n",
      "[INFO] epoch = 950, loss = 17.580117\n",
      "[INFO] epoch = 960, loss = 17.572858\n",
      "[INFO] epoch = 970, loss = 17.567756\n",
      "[INFO] epoch = 980, loss = 17.563959\n",
      "[INFO] epoch = 990, loss = 17.560931\n",
      "[INFO] epoch = 1000, loss = 17.558439\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       763\n",
      "           1       1.00      1.00      1.00       860\n",
      "           2       0.99      0.99      0.99       752\n",
      "           3       1.00      0.98      0.99       761\n",
      "           4       0.99      1.00      1.00       738\n",
      "           5       0.99      1.00      0.99       647\n",
      "           6       1.00      1.00      1.00       750\n",
      "           7       0.99      1.00      0.99       800\n",
      "           8       0.99      1.00      1.00       707\n",
      "           9       1.00      0.99      1.00       722\n",
      "\n",
      "    accuracy                           1.00      7500\n",
      "   macro avg       1.00      1.00      1.00      7500\n",
      "weighted avg       1.00      1.00      1.00      7500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       238\n",
      "           1       0.96      0.97      0.96       267\n",
      "           2       0.94      0.93      0.94       239\n",
      "           3       0.90      0.91      0.90       271\n",
      "           4       0.92      0.93      0.93       242\n",
      "           5       0.93      0.92      0.93       216\n",
      "           6       0.93      0.98      0.96       264\n",
      "           7       0.94      0.96      0.95       270\n",
      "           8       0.90      0.84      0.87       237\n",
      "           9       0.91      0.91      0.91       256\n",
      "\n",
      "    accuracy                           0.93      2500\n",
      "   macro avg       0.93      0.93      0.93      2500\n",
      "weighted avg       0.93      0.93      0.93      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 64, 32, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the loss is smaller here after training, performance didn't improve, but the extra size of the layers makes the computational cost higher for no gain. Let's try it with a third hidden layer to see if it helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 360.139740\n",
      "[INFO] epoch = 20, loss = 230.782219\n",
      "[INFO] epoch = 30, loss = 222.605312\n",
      "[INFO] epoch = 40, loss = 139.824198\n",
      "[INFO] epoch = 50, loss = 168.668274\n",
      "[INFO] epoch = 60, loss = 75.938406\n",
      "[INFO] epoch = 70, loss = 105.838271\n",
      "[INFO] epoch = 80, loss = 78.799024\n",
      "[INFO] epoch = 90, loss = 93.641459\n",
      "[INFO] epoch = 100, loss = 74.981546\n",
      "[INFO] epoch = 110, loss = 100.000989\n",
      "[INFO] epoch = 120, loss = 87.472935\n",
      "[INFO] epoch = 130, loss = 140.494931\n",
      "[INFO] epoch = 140, loss = 87.791402\n",
      "[INFO] epoch = 150, loss = 142.094736\n",
      "[INFO] epoch = 160, loss = 62.246703\n",
      "[INFO] epoch = 170, loss = 105.862143\n",
      "[INFO] epoch = 180, loss = 72.593607\n",
      "[INFO] epoch = 190, loss = 32.340485\n",
      "[INFO] epoch = 200, loss = 52.976588\n",
      "[INFO] epoch = 210, loss = 219.410054\n",
      "[INFO] epoch = 220, loss = 90.712570\n",
      "[INFO] epoch = 230, loss = 60.990585\n",
      "[INFO] epoch = 240, loss = 47.452495\n",
      "[INFO] epoch = 250, loss = 85.998142\n",
      "[INFO] epoch = 260, loss = 49.826051\n",
      "[INFO] epoch = 270, loss = 55.008830\n",
      "[INFO] epoch = 280, loss = 44.943335\n",
      "[INFO] epoch = 290, loss = 49.652790\n",
      "[INFO] epoch = 300, loss = 37.615586\n",
      "[INFO] epoch = 310, loss = 74.634881\n",
      "[INFO] epoch = 320, loss = 25.315991\n",
      "[INFO] epoch = 330, loss = 47.624361\n",
      "[INFO] epoch = 340, loss = 59.762642\n",
      "[INFO] epoch = 350, loss = 25.010708\n",
      "[INFO] epoch = 360, loss = 32.363472\n",
      "[INFO] epoch = 370, loss = 45.628160\n",
      "[INFO] epoch = 380, loss = 29.477367\n",
      "[INFO] epoch = 390, loss = 44.748059\n",
      "[INFO] epoch = 400, loss = 47.606013\n",
      "[INFO] epoch = 410, loss = 95.522620\n",
      "[INFO] epoch = 420, loss = 117.884712\n",
      "[INFO] epoch = 430, loss = 135.284451\n",
      "[INFO] epoch = 440, loss = 97.437332\n",
      "[INFO] epoch = 450, loss = 87.288496\n",
      "[INFO] epoch = 460, loss = 25.050574\n",
      "[INFO] epoch = 470, loss = 41.377609\n",
      "[INFO] epoch = 480, loss = 65.260334\n",
      "[INFO] epoch = 490, loss = 73.482913\n",
      "[INFO] epoch = 500, loss = 31.834406\n",
      "[INFO] epoch = 510, loss = 21.771805\n",
      "[INFO] epoch = 520, loss = 19.186510\n",
      "[INFO] epoch = 530, loss = 11.948479\n",
      "[INFO] epoch = 540, loss = 11.543350\n",
      "[INFO] epoch = 550, loss = 11.389384\n",
      "[INFO] epoch = 560, loss = 11.067763\n",
      "[INFO] epoch = 570, loss = 10.749516\n",
      "[INFO] epoch = 580, loss = 10.381407\n",
      "[INFO] epoch = 590, loss = 10.312952\n",
      "[INFO] epoch = 600, loss = 10.279970\n",
      "[INFO] epoch = 610, loss = 10.255344\n",
      "[INFO] epoch = 620, loss = 10.232218\n",
      "[INFO] epoch = 630, loss = 10.195132\n",
      "[INFO] epoch = 640, loss = 10.158925\n",
      "[INFO] epoch = 650, loss = 10.101420\n",
      "[INFO] epoch = 660, loss = 9.712509\n",
      "[INFO] epoch = 670, loss = 9.293634\n",
      "[INFO] epoch = 680, loss = 9.244159\n",
      "[INFO] epoch = 690, loss = 9.220679\n",
      "[INFO] epoch = 700, loss = 9.203546\n",
      "[INFO] epoch = 710, loss = 9.190902\n",
      "[INFO] epoch = 720, loss = 9.179318\n",
      "[INFO] epoch = 730, loss = 9.169700\n",
      "[INFO] epoch = 740, loss = 9.160817\n",
      "[INFO] epoch = 750, loss = 9.151614\n",
      "[INFO] epoch = 760, loss = 9.121132\n",
      "[INFO] epoch = 770, loss = 8.692754\n",
      "[INFO] epoch = 780, loss = 8.669973\n",
      "[INFO] epoch = 790, loss = 8.657472\n",
      "[INFO] epoch = 800, loss = 8.648521\n",
      "[INFO] epoch = 810, loss = 8.641274\n",
      "[INFO] epoch = 820, loss = 8.635071\n",
      "[INFO] epoch = 830, loss = 8.629655\n",
      "[INFO] epoch = 840, loss = 8.624792\n",
      "[INFO] epoch = 850, loss = 8.620363\n",
      "[INFO] epoch = 860, loss = 8.616266\n",
      "[INFO] epoch = 870, loss = 8.612475\n",
      "[INFO] epoch = 880, loss = 8.608939\n",
      "[INFO] epoch = 890, loss = 8.605624\n",
      "[INFO] epoch = 900, loss = 8.602496\n",
      "[INFO] epoch = 910, loss = 8.599539\n",
      "[INFO] epoch = 920, loss = 8.596733\n",
      "[INFO] epoch = 930, loss = 8.594070\n",
      "[INFO] epoch = 940, loss = 8.591543\n",
      "[INFO] epoch = 950, loss = 8.589139\n",
      "[INFO] epoch = 960, loss = 8.586858\n",
      "[INFO] epoch = 970, loss = 8.584687\n",
      "[INFO] epoch = 980, loss = 8.582620\n",
      "[INFO] epoch = 990, loss = 8.580648\n",
      "[INFO] epoch = 1000, loss = 8.578761\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       756\n",
      "           1       1.00      1.00      1.00       855\n",
      "           2       1.00      1.00      1.00       739\n",
      "           3       1.00      1.00      1.00       767\n",
      "           4       1.00      1.00      1.00       742\n",
      "           5       1.00      1.00      1.00       643\n",
      "           6       1.00      1.00      1.00       747\n",
      "           7       1.00      1.00      1.00       810\n",
      "           8       1.00      1.00      1.00       721\n",
      "           9       1.00      1.00      1.00       720\n",
      "\n",
      "    accuracy                           1.00      7500\n",
      "   macro avg       1.00      1.00      1.00      7500\n",
      "weighted avg       1.00      1.00      1.00      7500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96       245\n",
      "           1       0.98      0.98      0.98       272\n",
      "           2       0.93      0.94      0.93       252\n",
      "           3       0.95      0.90      0.92       265\n",
      "           4       0.94      0.91      0.93       238\n",
      "           5       0.90      0.89      0.89       220\n",
      "           6       0.95      0.93      0.94       267\n",
      "           7       0.93      0.93      0.93       260\n",
      "           8       0.88      0.94      0.91       223\n",
      "           9       0.90      0.91      0.91       258\n",
      "\n",
      "    accuracy                           0.93      2500\n",
      "   macro avg       0.93      0.93      0.93      2500\n",
      "weighted avg       0.93      0.93      0.93      2500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 64, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not gain much here, as the accuracy is just about the same as some of the smaller architectures we used above, but this one is more expensive to train, so its size is excessive.\n",
    "\n",
    "Let's run the entire MNIST dataset with the smaller net where computing was cheapest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 1539.695055\n",
      "[INFO] epoch = 20, loss = 1340.591629\n",
      "[INFO] epoch = 30, loss = 1228.037576\n",
      "[INFO] epoch = 40, loss = 1060.644886\n",
      "[INFO] epoch = 50, loss = 1077.800345\n",
      "[INFO] epoch = 60, loss = 775.697297\n",
      "[INFO] epoch = 70, loss = 783.264038\n",
      "[INFO] epoch = 80, loss = 828.792723\n",
      "[INFO] epoch = 90, loss = 792.173548\n",
      "[INFO] epoch = 100, loss = 661.837392\n",
      "[INFO] epoch = 110, loss = 737.947187\n",
      "[INFO] epoch = 120, loss = 589.155947\n",
      "[INFO] epoch = 130, loss = 592.780400\n",
      "[INFO] epoch = 140, loss = 586.565699\n",
      "[INFO] epoch = 150, loss = 527.812983\n",
      "[INFO] epoch = 160, loss = 567.250350\n",
      "[INFO] epoch = 170, loss = 674.730026\n",
      "[INFO] epoch = 180, loss = 517.040487\n",
      "[INFO] epoch = 190, loss = 702.193719\n",
      "[INFO] epoch = 200, loss = 585.979991\n",
      "[INFO] epoch = 210, loss = 485.325440\n",
      "[INFO] epoch = 220, loss = 541.430099\n",
      "[INFO] epoch = 230, loss = 443.590380\n",
      "[INFO] epoch = 240, loss = 398.573405\n",
      "[INFO] epoch = 250, loss = 514.149681\n",
      "[INFO] epoch = 260, loss = 478.496308\n",
      "[INFO] epoch = 270, loss = 634.627057\n",
      "[INFO] epoch = 280, loss = 468.454967\n",
      "[INFO] epoch = 290, loss = 412.669733\n",
      "[INFO] epoch = 300, loss = 364.058919\n",
      "[INFO] epoch = 310, loss = 590.527723\n",
      "[INFO] epoch = 320, loss = 603.694733\n",
      "[INFO] epoch = 330, loss = 456.094769\n",
      "[INFO] epoch = 340, loss = 373.645323\n",
      "[INFO] epoch = 350, loss = 332.062556\n",
      "[INFO] epoch = 360, loss = 402.081850\n",
      "[INFO] epoch = 370, loss = 532.108972\n",
      "[INFO] epoch = 380, loss = 492.682591\n",
      "[INFO] epoch = 390, loss = 570.167159\n",
      "[INFO] epoch = 400, loss = 530.937191\n",
      "[INFO] epoch = 410, loss = 469.130369\n",
      "[INFO] epoch = 420, loss = 376.426352\n",
      "[INFO] epoch = 430, loss = 341.025695\n",
      "[INFO] epoch = 440, loss = 418.341091\n",
      "[INFO] epoch = 450, loss = 335.254538\n",
      "[INFO] epoch = 460, loss = 332.584582\n",
      "[INFO] epoch = 470, loss = 552.636627\n",
      "[INFO] epoch = 480, loss = 431.241259\n",
      "[INFO] epoch = 490, loss = 483.223998\n",
      "[INFO] epoch = 500, loss = 294.784585\n",
      "[INFO] epoch = 510, loss = 285.013125\n",
      "[INFO] epoch = 520, loss = 432.094745\n",
      "[INFO] epoch = 530, loss = 349.758845\n",
      "[INFO] epoch = 540, loss = 402.535510\n",
      "[INFO] epoch = 550, loss = 396.171572\n",
      "[INFO] epoch = 560, loss = 348.366594\n",
      "[INFO] epoch = 570, loss = 437.847345\n",
      "[INFO] epoch = 580, loss = 662.773029\n",
      "[INFO] epoch = 590, loss = 588.119230\n",
      "[INFO] epoch = 600, loss = 524.690018\n",
      "[INFO] epoch = 610, loss = 652.699677\n",
      "[INFO] epoch = 620, loss = 439.688383\n",
      "[INFO] epoch = 630, loss = 409.733809\n",
      "[INFO] epoch = 640, loss = 488.915257\n",
      "[INFO] epoch = 650, loss = 452.799784\n",
      "[INFO] epoch = 660, loss = 556.836962\n",
      "[INFO] epoch = 670, loss = 374.934568\n",
      "[INFO] epoch = 680, loss = 554.315693\n",
      "[INFO] epoch = 690, loss = 557.559680\n",
      "[INFO] epoch = 700, loss = 494.628131\n",
      "[INFO] epoch = 710, loss = 332.573968\n",
      "[INFO] epoch = 720, loss = 371.407835\n",
      "[INFO] epoch = 730, loss = 384.736792\n",
      "[INFO] epoch = 740, loss = 294.435582\n",
      "[INFO] epoch = 750, loss = 378.442767\n",
      "[INFO] epoch = 760, loss = 429.622845\n",
      "[INFO] epoch = 770, loss = 337.012378\n",
      "[INFO] epoch = 780, loss = 240.561221\n",
      "[INFO] epoch = 790, loss = 337.994092\n",
      "[INFO] epoch = 800, loss = 259.671793\n",
      "[INFO] epoch = 810, loss = 268.780214\n",
      "[INFO] epoch = 820, loss = 252.039461\n",
      "[INFO] epoch = 830, loss = 265.066371\n",
      "[INFO] epoch = 840, loss = 286.787297\n",
      "[INFO] epoch = 850, loss = 373.937326\n",
      "[INFO] epoch = 860, loss = 349.324400\n",
      "[INFO] epoch = 870, loss = 352.633989\n",
      "[INFO] epoch = 880, loss = 245.597796\n",
      "[INFO] epoch = 890, loss = 294.297310\n",
      "[INFO] epoch = 900, loss = 205.259662\n",
      "[INFO] epoch = 910, loss = 204.356777\n",
      "[INFO] epoch = 920, loss = 198.946260\n",
      "[INFO] epoch = 930, loss = 473.020434\n",
      "[INFO] epoch = 940, loss = 349.278668\n",
      "[INFO] epoch = 950, loss = 375.019394\n",
      "[INFO] epoch = 960, loss = 370.329782\n",
      "[INFO] epoch = 970, loss = 427.340359\n",
      "[INFO] epoch = 980, loss = 290.474487\n",
      "[INFO] epoch = 990, loss = 313.906701\n",
      "[INFO] epoch = 1000, loss = 424.268307\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      3734\n",
      "           1       0.99      0.99      0.99      4255\n",
      "           2       0.99      0.98      0.99      3746\n",
      "           3       0.98      0.97      0.98      3793\n",
      "           4       0.98      0.99      0.99      3656\n",
      "           5       0.98      0.98      0.98      3387\n",
      "           6       0.99      0.99      0.99      3707\n",
      "           7       0.99      0.99      0.99      3906\n",
      "           8       0.99      0.99      0.99      3567\n",
      "           9       0.99      0.99      0.99      3749\n",
      "\n",
      "    accuracy                           0.99     37500\n",
      "   macro avg       0.99      0.99      0.99     37500\n",
      "weighted avg       0.99      0.99      0.99     37500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96      1198\n",
      "           1       0.97      0.97      0.97      1423\n",
      "           2       0.95      0.94      0.94      1222\n",
      "           3       0.93      0.92      0.92      1308\n",
      "           4       0.94      0.96      0.95      1203\n",
      "           5       0.93      0.92      0.93      1119\n",
      "           6       0.96      0.96      0.96      1244\n",
      "           7       0.94      0.96      0.95      1269\n",
      "           8       0.94      0.93      0.93      1275\n",
      "           9       0.94      0.92      0.93      1239\n",
      "\n",
      "    accuracy                           0.94     12500\n",
      "   macro avg       0.94      0.94      0.94     12500\n",
      "weighted avg       0.94      0.94      0.94     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 50000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:50000].reshape([50000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:50000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if more training helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 100, loss = 686.141739\n",
      "[INFO] epoch = 200, loss = 420.820100\n",
      "[INFO] epoch = 300, loss = 476.385929\n",
      "[INFO] epoch = 400, loss = 362.086947\n",
      "[INFO] epoch = 500, loss = 281.548244\n",
      "[INFO] epoch = 600, loss = 455.982912\n",
      "[INFO] epoch = 700, loss = 295.032399\n",
      "[INFO] epoch = 800, loss = 268.125648\n",
      "[INFO] epoch = 900, loss = 904.469981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\ipykernel_launcher.py:28: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 1000, loss = 625.717588\n",
      "[INFO] epoch = 1100, loss = 261.739117\n",
      "[INFO] epoch = 1200, loss = 362.273303\n",
      "[INFO] epoch = 1300, loss = 384.625477\n",
      "[INFO] epoch = 1400, loss = 152.174230\n",
      "[INFO] epoch = 1500, loss = 148.864956\n",
      "[INFO] epoch = 1600, loss = 147.173036\n",
      "[INFO] epoch = 1700, loss = 146.361086\n",
      "[INFO] epoch = 1800, loss = 705.951894\n",
      "[INFO] epoch = 1900, loss = 378.794403\n",
      "[INFO] epoch = 2000, loss = 321.649614\n",
      "[INFO] epoch = 2100, loss = 201.125134\n",
      "[INFO] epoch = 2200, loss = 173.434265\n",
      "[INFO] epoch = 2300, loss = 151.807856\n",
      "[INFO] epoch = 2400, loss = 165.208958\n",
      "[INFO] epoch = 2500, loss = 484.326458\n",
      "[INFO] epoch = 2600, loss = 272.031896\n",
      "[INFO] epoch = 2700, loss = 216.653339\n",
      "[INFO] epoch = 2800, loss = 398.076728\n",
      "[INFO] epoch = 2900, loss = 322.041964\n",
      "[INFO] epoch = 3000, loss = 317.480161\n",
      "[INFO] epoch = 3100, loss = 585.719710\n",
      "[INFO] epoch = 3200, loss = 334.106392\n",
      "[INFO] epoch = 3300, loss = 188.229697\n",
      "[INFO] epoch = 3400, loss = 186.582060\n",
      "[INFO] epoch = 3500, loss = 161.402518\n",
      "[INFO] epoch = 3600, loss = 165.341568\n",
      "[INFO] epoch = 3700, loss = 429.910776\n",
      "[INFO] epoch = 3800, loss = 575.412479\n",
      "[INFO] epoch = 3900, loss = 464.740091\n",
      "[INFO] epoch = 4000, loss = 242.913075\n",
      "[INFO] epoch = 4100, loss = 205.050591\n",
      "[INFO] epoch = 4200, loss = 218.541236\n",
      "[INFO] epoch = 4300, loss = 174.697413\n",
      "[INFO] epoch = 4400, loss = 161.422294\n",
      "[INFO] epoch = 4500, loss = 158.313724\n",
      "[INFO] epoch = 4600, loss = 155.686026\n",
      "[INFO] epoch = 4700, loss = 153.552044\n",
      "[INFO] epoch = 4800, loss = 150.938225\n",
      "[INFO] epoch = 4900, loss = 148.931915\n",
      "[INFO] epoch = 5000, loss = 148.636803\n",
      "[INFO] epoch = 5100, loss = 148.253727\n",
      "[INFO] epoch = 5200, loss = 901.794558\n",
      "[INFO] epoch = 5300, loss = 949.375137\n",
      "[INFO] epoch = 5400, loss = 771.824137\n",
      "[INFO] epoch = 5500, loss = 813.553597\n",
      "[INFO] epoch = 5600, loss = 580.089820\n",
      "[INFO] epoch = 5700, loss = 683.137266\n",
      "[INFO] epoch = 5800, loss = 377.904098\n",
      "[INFO] epoch = 5900, loss = 304.433885\n",
      "[INFO] epoch = 6000, loss = 312.611729\n",
      "[INFO] epoch = 6100, loss = 377.875913\n",
      "[INFO] epoch = 6200, loss = 234.746922\n",
      "[INFO] epoch = 6300, loss = 268.843113\n",
      "[INFO] epoch = 6400, loss = 324.923110\n",
      "[INFO] epoch = 6500, loss = 560.101944\n",
      "[INFO] epoch = 6600, loss = 585.090974\n",
      "[INFO] epoch = 6700, loss = 290.218396\n",
      "[INFO] epoch = 6800, loss = 377.651239\n",
      "[INFO] epoch = 6900, loss = 434.227644\n",
      "[INFO] epoch = 7000, loss = 511.294132\n",
      "[INFO] epoch = 7100, loss = 258.709151\n",
      "[INFO] epoch = 7200, loss = 246.046173\n",
      "[INFO] epoch = 7300, loss = 237.933605\n",
      "[INFO] epoch = 7400, loss = 214.345293\n",
      "[INFO] epoch = 7500, loss = 186.710003\n",
      "[INFO] epoch = 7600, loss = 179.577904\n",
      "[INFO] epoch = 7700, loss = 183.701563\n",
      "[INFO] epoch = 7800, loss = 502.322761\n",
      "[INFO] epoch = 7900, loss = 414.467086\n",
      "[INFO] epoch = 8000, loss = 715.106956\n",
      "[INFO] epoch = 8100, loss = 670.005873\n",
      "[INFO] epoch = 8200, loss = 435.644365\n",
      "[INFO] epoch = 8300, loss = 530.640952\n",
      "[INFO] epoch = 8400, loss = 382.149554\n",
      "[INFO] epoch = 8500, loss = 319.353895\n",
      "[INFO] epoch = 8600, loss = 419.947175\n",
      "[INFO] epoch = 8700, loss = 532.064681\n",
      "[INFO] epoch = 8800, loss = 421.619230\n",
      "[INFO] epoch = 8900, loss = 539.309443\n",
      "[INFO] epoch = 9000, loss = 406.451505\n",
      "[INFO] epoch = 9100, loss = 283.209668\n",
      "[INFO] epoch = 9200, loss = 227.785058\n",
      "[INFO] epoch = 9300, loss = 223.570304\n",
      "[INFO] epoch = 9400, loss = 217.916538\n",
      "[INFO] epoch = 9500, loss = 217.282275\n",
      "[INFO] epoch = 9600, loss = 515.743270\n",
      "[INFO] epoch = 9700, loss = 571.768448\n",
      "[INFO] epoch = 9800, loss = 690.190754\n",
      "[INFO] epoch = 9900, loss = 549.119415\n",
      "[INFO] epoch = 10000, loss = 383.216841\n",
      "Training set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3677\n",
      "           1       0.99      0.99      0.99      4235\n",
      "           2       0.99      0.98      0.98      3736\n",
      "           3       0.98      0.98      0.98      3793\n",
      "           4       0.99      0.99      0.99      3641\n",
      "           5       0.99      0.99      0.99      3410\n",
      "           6       0.99      0.99      0.99      3750\n",
      "           7       0.99      0.99      0.99      3909\n",
      "           8       0.98      0.99      0.98      3605\n",
      "           9       0.99      0.98      0.99      3744\n",
      "\n",
      "    accuracy                           0.99     37500\n",
      "   macro avg       0.99      0.99      0.99     37500\n",
      "weighted avg       0.99      0.99      0.99     37500\n",
      "\n",
      "Test set accuracy\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1255\n",
      "           1       0.97      0.97      0.97      1443\n",
      "           2       0.93      0.93      0.93      1232\n",
      "           3       0.92      0.93      0.93      1308\n",
      "           4       0.95      0.95      0.95      1218\n",
      "           5       0.93      0.93      0.93      1096\n",
      "           6       0.97      0.95      0.96      1201\n",
      "           7       0.94      0.96      0.95      1266\n",
      "           8       0.92      0.93      0.92      1237\n",
      "           9       0.95      0.93      0.94      1244\n",
      "\n",
      "    accuracy                           0.95     12500\n",
      "   macro avg       0.95      0.95      0.95     12500\n",
      "weighted avg       0.95      0.95      0.95     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 50000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:50000].reshape([50000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:50000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training data\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.5, 32)\n",
    "model.fit(trainX,trainY,10000,100)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice accuracy went up a little to 95% with the full dataset. More data frequently improves results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate Loss Functions\n",
    "\n",
    "Another part of a neural net we can customize is the loss function.\n",
    "\n",
    "One problem with the squared error loss function is that it tends to approach very slowly when the current output is far from the correct answer. (See the animated graphics in <a href=\"http://neuralnetworksanddeeplearning.com/chap3.html#the_cross-entropy_cost_function\">Nielsen's book</a> in Chapter 3.) Ideally, we would like to see more drastic changes when the net is more incorrect in its answers.\n",
    "\n",
    "The **cross-entropy** loss function accomplishes this (see mathematical details in class or in Nielsen, Ch 3). It is computed as\n",
    "\n",
    "$$ L(W)=-\\frac{1}{n}\\sum\\limits_x\\sum\\limits_j \\left[y_j\\ln a_j^L + (1-y_j)\\ln\\left(1-a_j^L\\right)\\right]$$\n",
    "\n",
    "Cross-entrpy not only speeds up training, but can help with overfitting sometimes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNeuralNetworkSGD:\n",
    "    \n",
    "    # input a vector [a, b, c, ...] with the number of nodes in each layer\n",
    "    def __init__(self, layers, alpha = 0.1, batchSize = 32, loss = \"sum-of-squares\"):\n",
    "        # list of weight matrices between layers\n",
    "        self.W = []\n",
    "        \n",
    "        # network architecture will be a vector of numbers of nodes for each layer\n",
    "        self.layers = layers\n",
    "        \n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        # batch size\n",
    "        self.batchSize = batchSize\n",
    "        \n",
    "        # loss function\n",
    "        self.loss = loss\n",
    "        \n",
    "        # initialize the weights (randomly) -- this is our initial guess for gradient descent\n",
    "        \n",
    "        # initialize the weights between layers (up to the next-to-last one) as normal random variables\n",
    "        for i in np.arange(0, len(layers) - 2):\n",
    "            self.W.append(np.random.randn(layers[i] + 1, layers[i + 1] + 1))\n",
    "            \n",
    "        # initialize weights between the last two layers (we don't want bias for the last one)\n",
    "        self.W.append(np.random.randn(layers[-2] + 1, layers[-1]))\n",
    "        \n",
    "    # define the sigmoid activation\n",
    "    def sigmoid(self, x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "    \n",
    "    # define the sigmoid derivative (where z is the output of a sigmoid)\n",
    "    def sigmoidDerivative(self, z):\n",
    "        return z * (1 - z)\n",
    "    \n",
    "    def getNextBatch(self, X, y, batchSize):\n",
    "        for i in np.arange(0, X.shape[0], batchSize):\n",
    "            yield (X[i:i + batchSize], y[i:i + batchSize])\n",
    "    \n",
    "    # fit the model\n",
    "    def fit(self, X, y, epochs = 10000, update = 1000):\n",
    "        # add a column of ones to the end of X\n",
    "        X = np.hstack((X, np.ones([X.shape[0],1])))\n",
    "\n",
    "        for epoch in np.arange(0,epochs):\n",
    "            \n",
    "            # randomize the examples\n",
    "            p = np.arange(0,X.shape[0])\n",
    "            np.random.shuffle(p)\n",
    "            X = X[p]\n",
    "            y = y[p]\n",
    "\n",
    "            # feed forward, backprop, and weight update\n",
    "            for (x, target) in self.getNextBatch(X, y, self.batchSize):\n",
    "                # make a list of output activations from the first layer\n",
    "                # (just the original x values)\n",
    "                A = [np.atleast_2d(x)]\n",
    "                \n",
    "                # feed forward\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    \n",
    "                    # feed through one layer and apply sigmoid activation\n",
    "                    net = A[layer].dot(self.W[layer])\n",
    "                    out = self.sigmoid(net)\n",
    "                    \n",
    "                    # add our network output to the list of activations\n",
    "                    A.append(out)\n",
    "                    \n",
    "                # backpropagation\n",
    "                error = A[-1] - target\n",
    "                \n",
    "                if self.loss == \"sum-of-squares\":\n",
    "                    D = [error * self.sigmoidDerivative(A[-1])]\n",
    "                    \n",
    "                if self.loss == \"cross-entropy\":\n",
    "                    D = [error]\n",
    "\n",
    "                # loop backwards over the layers to build up deltas\n",
    "                for layer in np.arange(len(A) - 2, 0, -1):\n",
    "                    delta = D[-1].dot(self.W[layer].T)\n",
    "                    delta = delta * self.sigmoidDerivative(A[layer])\n",
    "                    D.append(delta)\n",
    "                    \n",
    "                # reverse the deltas since we looped in reverse\n",
    "                D = D[::-1]\n",
    "                \n",
    "                # weight update\n",
    "                for layer in np.arange(0, len(self.W)):\n",
    "                    self.W[layer] -= self.alpha * A[layer].T.dot(D[layer])\n",
    "                    \n",
    "            if (epoch + 1) % update == 0:\n",
    "                loss = self.computeLoss(X,y)\n",
    "                print(\"[INFO] epoch = {}, loss = {:.6f}\".format(epoch + 1, loss))\n",
    "                \n",
    "    def predict(self, X, addOnes = True):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        p = np.atleast_2d(X)\n",
    "        \n",
    "        # add a column of 1s for bias\n",
    "        if addOnes:\n",
    "            p = np.hstack((p, np.ones([X.shape[0],1])))\n",
    "        \n",
    "        # feed forward!\n",
    "        for layer in np.arange(0, len(self.W)):\n",
    "            p = self.sigmoid(np.dot(p, self.W[layer]))\n",
    "            \n",
    "        return p\n",
    "    \n",
    "    def computeLoss(self, X, y):\n",
    "        # initialize data, be sure it's the right dimension\n",
    "        y = np.atleast_2d(y)\n",
    "        \n",
    "        # feed the datapoints through the network to get predicted outputs\n",
    "        predictions = self.predict(X, addOnes = False)\n",
    "        \n",
    "        if self.loss == \"sum-of-squares\":\n",
    "            loss = np.sum((predictions - y)**2) / 2.0\n",
    "            \n",
    "        if self.loss == \"cross-entropy\":\n",
    "            loss = np.sum(np.nan_to_num(-y*np.log(predictions)-(1-y)*np.log(1-predictions)))\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 2718.886280\n",
      "[INFO] epoch = 20, loss = 1675.718220\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-868177bcf04f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# fit the model to the training datak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeedforwardNeuralNetworkSGD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cross-entropy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# print the classification performance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-d3031f630835>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, epochs, update)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training datak\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.1, 32, \"cross-entropy\")\n",
    "model.fit(trainX,trainY,100,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 10000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:10000].reshape([10000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:10000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training datak\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.1, 32, \"cross-entropy\")\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] epoch = 10, loss = 13558.998781\n",
      "[INFO] epoch = 20, loss = 10709.692479\n",
      "[INFO] epoch = 30, loss = 9473.620639\n",
      "[INFO] epoch = 40, loss = 8475.926553\n",
      "[INFO] epoch = 50, loss = 6198.636888\n",
      "[INFO] epoch = 60, loss = 6167.389599\n",
      "[INFO] epoch = 70, loss = 5282.109816\n",
      "[INFO] epoch = 80, loss = 5610.474029\n",
      "[INFO] epoch = 90, loss = 5791.741880\n",
      "[INFO] epoch = 100, loss = 4932.725486\n",
      "[INFO] epoch = 110, loss = 5067.825925\n",
      "[INFO] epoch = 120, loss = 5705.379545\n",
      "[INFO] epoch = 130, loss = 5217.864736\n",
      "[INFO] epoch = 140, loss = 4704.094505\n",
      "[INFO] epoch = 150, loss = 3087.366765\n",
      "[INFO] epoch = 160, loss = 4519.813385\n",
      "[INFO] epoch = 170, loss = 3414.704367\n",
      "[INFO] epoch = 180, loss = 3500.955825\n"
     ]
    }
   ],
   "source": [
    "### CLASSIFY MNIST PICTURES\n",
    "\n",
    "# create a dataset of 50000 MNIST images, reshaped as single vectors, and labels\n",
    "data = mnist.load_data()\n",
    "\n",
    "# The datapoints are in mnistData[0][0]\n",
    "X = data[0][0][:50000].reshape([50000,28*28])\n",
    "X = X/255.0\n",
    "\n",
    "# The labels are in mnistData[0][1]\n",
    "Y = data[0][1][:50000]\n",
    "\n",
    "# randomly choose 75% of the data to be the training set and 25% for the testing set\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, Y, test_size = 0.25)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# fit the model to the training datak\n",
    "model = FeedforwardNeuralNetworkSGD([784, 32, 16, 10], 0.1, 32, \"cross-entropy\")\n",
    "model.fit(trainX,trainY,1000,10)\n",
    "\n",
    "# print the classification performance\n",
    "print(\"Training set accuracy\")\n",
    "predictedY = model.predict(trainX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "trainY = trainY.argmax(axis=1)\n",
    "print(classification_report(trainY, predictedY))\n",
    "\n",
    "print(\"Test set accuracy\")\n",
    "predictedY = model.predict(testX)\n",
    "predictedY = predictedY.argmax(axis=1)\n",
    "\n",
    "testY = testY.argmax(axis=1)\n",
    "print(classification_report(testY, predictedY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
