{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Training\n",
    "\n",
    "This week, we investigate some useful methods for improving the performance of neural nets by ensemble methods, where we train several nets on the same datapoints and allow them to \"vote\" on the correct classification or regression prediction. Another helpful approach is to monitor training carefully. Lastly, we will speeding up training with transfer learning, where we start with pre-trained models and try to specialize them for our datasets.\n",
    "\n",
    "## Ensemble Methods\n",
    "\n",
    "A common technique used with classification methods (both non-neural and neural) is to take an ensemble of models and combine them to make classification decisions. For example, we could run 5 neural nets, each with comparable accuracy overall, and classify each datapoint based on the majority vote of the 5 networks.\n",
    "\n",
    "This almost always improves results compared to using just one net because different nets have unique talents and may make errors on different datapoints, but, assuming all the nets have good accuracy, they are typically correct, so these mistakes are frequently restricted to just a minority of models.\n",
    "\n",
    "Let's bring in a few (mini) modern CNN achitectures we wrote in the past to use for some ensembling. Note that any architecture would work for the forthcoming experiments, but the following nets can run relatively quickly.\n",
    "\n",
    "First, we import some things we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import imutils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import progressbar\n",
    "import random\n",
    "\n",
    "# sklearn functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# keras functions\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications import imagenet_utils\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import add\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class for a mini version of VGGNet (Simonyan and Zisserman, 2015)\n",
    "class MiniVGGNet:\n",
    "    def build(height, width, depth, classes):\n",
    "        # create the model and name it MiniVGGNet\n",
    "        model = Sequential(name = 'MiniVGGNet')\n",
    "                \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same', input_shape = (height, width, depth)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 32 3x3 feature maps\n",
    "        model.add(Conv2D(32, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # convolutional layer with 64 3x3 feature maps\n",
    "        model.add(Conv2D(64, (3, 3), padding = 'same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "        # 2x2 max pooling layer with stride 2x2\n",
    "        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        # flatten the activations from a square to a vector\n",
    "        model.add(Flatten())\n",
    "        \n",
    "        # fully-connected layer\n",
    "        model.add(Dense(512))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # fully-connected layer with softmax classifier\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniGoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGoogLeNet:\n",
    "    def convolution_module(x, K, kX, kY, stride, channelsDim, padding=\"same\"):\n",
    "        # create a CONV -> BN -> RELU sequence\n",
    "        x = Conv2D(K, (kX, kY), strides = stride, padding = padding)(x)\n",
    "        x = BatchNormalization(axis = channelsDim)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        # return the output\n",
    "        return x\n",
    "    \n",
    "    def inception_module(x, numberOf1x1Kernels, numberOf3x3Kernels, channelsDim):\n",
    "        # define two \"parallel\" convolutions of size 1x1 and 3x3 concatenated across the channels dimension\n",
    "        convolution_1x1 = MiniGoogLeNet.convolution_module(x, numberOf1x1Kernels, 1, 1, (1, 1), channelsDim)\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, numberOf3x3Kernels, 3, 3, (1, 1), channelsDim)\n",
    "        x = concatenate([convolution_1x1, convolution_3x3], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    def downsample_module(x, K, channelsDim):\n",
    "        # define a CONV and POOL and then concatenate across the channels dimension\n",
    "        convolution_3x3 = MiniGoogLeNet.convolution_module(x, K, 3, 3, (2, 2), channelsDim, padding = 'valid')\n",
    "        pool = MaxPooling2D((3, 3), strides = (2, 2))(x)\n",
    "        x = concatenate([convolution_3x3, pool], axis = channelsDim)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == \"channels_first\":\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "        \n",
    "        # define the model input and first CONV module\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = MiniGoogLeNet.convolution_module(inputs, 96, 3, 3, (1, 1), channelsDim)\n",
    "        \n",
    "        # two inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 32, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 32, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 80, channelsDim)\n",
    "        \n",
    "        # four inception modules followed by a downsample module\n",
    "        x = MiniGoogLeNet.inception_module(x, 112, 48, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 96, 64, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 80, 80, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 48, 96, channelsDim)\n",
    "        x = MiniGoogLeNet.downsample_module(x, 96, channelsDim)\n",
    "        \n",
    "        # two inception modules followed by global POOL and dropout\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = MiniGoogLeNet.inception_module(x, 176, 160, channelsDim)\n",
    "        x = AveragePooling2D((7, 7))(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create a model\n",
    "        model = Model(inputs, x, name='MiniGoogLeNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet:\n",
    "    def residual_module(data, K, stride, channelsDim, reduce = False, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9):\n",
    "        shortcut = data\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn1 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(data)\n",
    "        act1 = Activation('relu')(bn1)\n",
    "        conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "        \n",
    "        # 3x3 CONVs\n",
    "        bn2 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv1)\n",
    "        act2 = Activation('relu')(bn2)\n",
    "        conv2 = Conv2D(int(K * 0.25), (3, 3), strides = stride, padding = 'same', use_bias = False, kernel_regularizer = l2(reg))(act2)\n",
    "        \n",
    "        # 1x1 CONVs\n",
    "        bn3 = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(conv2)\n",
    "        act3 = Activation('relu')(bn3)\n",
    "        conv3 = Conv2D(K, (1, 1), use_bias = False, kernel_regularizer = l2(reg))(act3)\n",
    "        \n",
    "        # if we reduce the spatial size, apply a CONV layer to the shortcut\n",
    "        if reduce:\n",
    "            shortcut = Conv2D(K, (1, 1), strides = stride, use_bias = False, kernel_regularizer = l2(reg))(act1)\n",
    "            \n",
    "        # add the shortcut and the final CONV\n",
    "        x = add([conv3, shortcut])\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def build(width, height, depth, classes, stages, filters, reg = 0.0001, bnEpsilon = 0.00002, bnMomentum = 0.9, dataset='cifar'):\n",
    "        inputShape = (height, width, depth)\n",
    "        channelsDim = -1\n",
    "        \n",
    "        if backend.image_data_format() == 'channels_first':\n",
    "            inputShape = (depth, height, width)\n",
    "            channelsDim = 1\n",
    "            \n",
    "        # set the input and apply BN\n",
    "        inputs = Input(shape = inputShape)\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(inputs)\n",
    "        \n",
    "        if dataset == 'cifar':\n",
    "            # apply a single CONV layer\n",
    "            x = Conv2D(filters[0], (3, 3), use_bias = False, padding = 'same',\n",
    "                       kernel_regularizer = l2(reg))(x)\n",
    "        \n",
    "        # loop over the number of stages\n",
    "        for counter in range(0, len(stages)):\n",
    "            # initialize the stride\n",
    "            if counter == 0:\n",
    "                stride = (1, 1)\n",
    "            else:\n",
    "                stride = (2, 2)\n",
    "                    \n",
    "            # apply a residual module to reduce the spatial dimension of the image volume\n",
    "            x = ResNet.residual_module(x, filters[counter + 1], stride, channelsDim, reduce = True, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "            \n",
    "            # loop over the number of layers in the current stage\n",
    "            for j in range(0, stages[counter] - 1):\n",
    "                # apply a residual module\n",
    "                x = ResNet.residual_module(x, filters[counter + 1], (1, 1), channelsDim, bnEpsilon = bnEpsilon, bnMomentum = bnMomentum)\n",
    "                    \n",
    "        # apply BN -> ACT -> POOL\n",
    "        x = BatchNormalization(axis = channelsDim, epsilon = bnEpsilon, momentum = bnMomentum)(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = AveragePooling2D((8, 8))(x)\n",
    "        \n",
    "        # softmax classifier\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(classes, kernel_regularizer = l2(reg))(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        \n",
    "        # create the model\n",
    "        model = Model(inputs, x, name = 'ResNet')\n",
    "        \n",
    "        # return the model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Ensembles\n",
    "\n",
    "Now, let's look at some ensembling methods. In the simplest case, we train several nets and average the classifications at the end. If the nets have similar performance, but make mistakes on *different* examples, this approach improves performance in most cases.\n",
    "\n",
    "**Quick GPU Check**: Before we start training models, let's check our GPU resources. If you have a GPU set up to work with TensorFlow, its name will be output and it will be used in training. If not, none will be output and your training will use your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n",
      "device: 0, name: GeForce RTX 2070, pci bus id: 0000:04:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "numGPUs = len(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "print(\"Num GPUs Available: \", numGPUs)\n",
    "\n",
    "if numGPUs > 0:\n",
    "    print(tf.test.gpu_device_name())\n",
    "    print(device_lib.list_local_devices()[1].physical_device_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the Same Hyperparameters Multiple Times\n",
    "\n",
    "Next, let's train several MiniVGGNets on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net 0 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 84s 107ms/step - loss: 1.6116 - accuracy: 0.4564 - val_loss: 1.2383 - val_accuracy: 0.5816\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1182 - accuracy: 0.6013 - val_loss: 1.0832 - val_accuracy: 0.6394\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.9926 - accuracy: 0.6504 - val_loss: 1.1733 - val_accuracy: 0.6197\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9169 - accuracy: 0.6774 - val_loss: 0.9389 - val_accuracy: 0.6803\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8591 - accuracy: 0.7008 - val_loss: 0.9506 - val_accuracy: 0.6894\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8172 - accuracy: 0.7133 - val_loss: 0.9262 - val_accuracy: 0.7068\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7842 - accuracy: 0.7266 - val_loss: 0.7594 - val_accuracy: 0.7410\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7524 - accuracy: 0.7370 - val_loss: 0.7740 - val_accuracy: 0.7374\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7269 - accuracy: 0.7465 - val_loss: 0.6835 - val_accuracy: 0.7693\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7019 - accuracy: 0.7543 - val_loss: 0.6582 - val_accuracy: 0.7753\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models1\\model_0.model\\assets\n",
      "Net 1 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6192 - accuracy: 0.4509 - val_loss: 1.3308 - val_accuracy: 0.5245\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1391 - accuracy: 0.5927 - val_loss: 1.2024 - val_accuracy: 0.6017\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0025 - accuracy: 0.6438 - val_loss: 0.8963 - val_accuracy: 0.6919\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.9254 - accuracy: 0.6734 - val_loss: 1.1087 - val_accuracy: 0.6293\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8765 - accuracy: 0.6916 - val_loss: 0.9029 - val_accuracy: 0.6954\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 52s 67ms/step - loss: 0.8258 - accuracy: 0.7100 - val_loss: 0.7703 - val_accuracy: 0.7392\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7918 - accuracy: 0.7225 - val_loss: 0.7755 - val_accuracy: 0.7409\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7604 - accuracy: 0.7352 - val_loss: 0.7618 - val_accuracy: 0.7477\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7346 - accuracy: 0.7435 - val_loss: 0.9406 - val_accuracy: 0.6951\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.7119 - accuracy: 0.7522 - val_loss: 0.6782 - val_accuracy: 0.7703\n",
      "INFO:tensorflow:Assets written to: models1\\model_1.model\\assets\n",
      "Net 2 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 1.6559 - accuracy: 0.4438 - val_loss: 1.3548 - val_accuracy: 0.5395\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.1530 - accuracy: 0.5895 - val_loss: 1.1518 - val_accuracy: 0.6143\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0131 - accuracy: 0.6426 - val_loss: 1.0806 - val_accuracy: 0.6433\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 23s 30ms/step - loss: 0.9361 - accuracy: 0.6689 - val_loss: 0.8892 - val_accuracy: 0.6956\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8819 - accuracy: 0.6906 - val_loss: 1.0689 - val_accuracy: 0.6605\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8275 - accuracy: 0.7087 - val_loss: 0.7808 - val_accuracy: 0.7314\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.7920 - accuracy: 0.7227 - val_loss: 0.9672 - val_accuracy: 0.6828\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.7565 - accuracy: 0.7352 - val_loss: 0.7960 - val_accuracy: 0.7260\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7312 - accuracy: 0.7434 - val_loss: 0.6682 - val_accuracy: 0.7725\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.7028 - accuracy: 0.7537 - val_loss: 0.7692 - val_accuracy: 0.7390\n",
      "INFO:tensorflow:Assets written to: models1\\model_2.model\\assets\n",
      "Net 3 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6405 - accuracy: 0.4501 - val_loss: 1.1384 - val_accuracy: 0.5946\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 1.1410 - accuracy: 0.5934 - val_loss: 1.1916 - val_accuracy: 0.6036\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 1.0070 - accuracy: 0.6436 - val_loss: 0.8387 - val_accuracy: 0.7070\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9190 - accuracy: 0.6749 - val_loss: 0.8973 - val_accuracy: 0.7024\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8706 - accuracy: 0.6921 - val_loss: 0.8266 - val_accuracy: 0.7229\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.8229 - accuracy: 0.7106 - val_loss: 0.8346 - val_accuracy: 0.7135\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7775 - accuracy: 0.7270 - val_loss: 0.8318 - val_accuracy: 0.7185\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7532 - accuracy: 0.7357 - val_loss: 0.7186 - val_accuracy: 0.7557\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7193 - accuracy: 0.7483 - val_loss: 0.7679 - val_accuracy: 0.7396\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.6959 - accuracy: 0.7565 - val_loss: 0.7396 - val_accuracy: 0.7480\n",
      "INFO:tensorflow:Assets written to: models1\\model_3.model\\assets\n",
      "Net 4 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 1.6184 - accuracy: 0.4567 - val_loss: 1.1779 - val_accuracy: 0.5928\n",
      "Epoch 2/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 1.1337 - accuracy: 0.5987 - val_loss: 1.0753 - val_accuracy: 0.6284\n",
      "Epoch 3/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9915 - accuracy: 0.6490 - val_loss: 0.9227 - val_accuracy: 0.6799\n",
      "Epoch 4/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.9260 - accuracy: 0.6723 - val_loss: 0.9472 - val_accuracy: 0.6884 - l\n",
      "Epoch 5/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.8618 - accuracy: 0.6978 - val_loss: 0.8872 - val_accuracy: 0.7051\n",
      "Epoch 6/10\n",
      "781/781 [==============================] - 23s 29ms/step - loss: 0.8198 - accuracy: 0.7124 - val_loss: 0.8868 - val_accuracy: 0.709920 - ETA\n",
      "Epoch 7/10\n",
      "781/781 [==============================] - 22s 29ms/step - loss: 0.7797 - accuracy: 0.7254 - val_loss: 0.8310 - val_accuracy: 0.7232\n",
      "Epoch 8/10\n",
      "781/781 [==============================] - 22s 28ms/step - loss: 0.7476 - accuracy: 0.7375 - val_loss: 0.8495 - val_accuracy: 0.7227\n",
      "Epoch 9/10\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.7253 - accuracy: 0.7459 - val_loss: 0.7911 - val_accuracy: 0.7426\n",
      "Epoch 10/10\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 0.7023 - accuracy: 0.7541 - val_loss: 0.7094 - val_accuracy: 0.7671\n",
      "INFO:tensorflow:Assets written to: models1\\model_4.model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "numberOfModels = 5\n",
    "epochs = 10\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('Net', i, 'is being trained...')\n",
    "    \n",
    "    # choose the optimizer\n",
    "    #opt = SGD(lr = 0.01, decay = 0.1 / epochs, momentum = 0.9, nesterov = True)\n",
    "    opt = Adam()\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniVGGNet.build(32, 32, 3, 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs, steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models1', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output1', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), \"w\")\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output1', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.81      0.86      0.83      1000\n",
      "  automobile       0.90      0.93      0.91      1000\n",
      "        bird       0.79      0.68      0.73      1000\n",
      "         cat       0.74      0.51      0.60      1000\n",
      "        deer       0.74      0.81      0.77      1000\n",
      "         dog       0.86      0.57      0.69      1000\n",
      "        frog       0.64      0.96      0.77      1000\n",
      "       horse       0.85      0.85      0.85      1000\n",
      "        ship       0.92      0.89      0.90      1000\n",
      "       truck       0.83      0.93      0.87      1000\n",
      "\n",
      "    accuracy                           0.80     10000\n",
      "   macro avg       0.81      0.80      0.79     10000\n",
      "weighted avg       0.81      0.80      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models1', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size=64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis=0)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we actually ran the network with the very same hyperparameters 5 times and found that the best ones performed at around **77%**, but the ensemble of all 5 averaged together reseults in a higher accuracy rate of **80%**!\n",
    "\n",
    "Mathematically, what has happened is likely that the different nets converged to *different* local minima so that, while their individual accuracy rates were all similar, the different nets were, apparently misclassifying *different* test examples, resulting in the average classification of all the models being correct more frequently than any individual net.\n",
    "\n",
    "### Ensemble Training with Different Nets\n",
    "\n",
    "It stands to reason that nets that are more significantly different are more likely to make significantly different classification mistakes since they may work in very different ways. For example, our past GoogLeNet and ResNet experiments had about 90% success on CIFAR-10, but they are very different architectures. Let's try to create an ensemble of those!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "    \n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "trainX = trainX.astype('float')\n",
    "testX = testX.astype('float')\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "mean = np.mean(trainX, axis = 0)\n",
    "trainX -= mean\n",
    "testX -= mean\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogLeNet 0 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 31s 40ms/step - loss: 1.3488 - accuracy: 0.5128 - val_loss: 2.0277 - val_accuracy: 0.4694\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.8970 - accuracy: 0.6853 - val_loss: 0.8755 - val_accuracy: 0.6964\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.7277 - accuracy: 0.7491 - val_loss: 0.7028 - val_accuracy: 0.7612\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6233 - accuracy: 0.7872 - val_loss: 0.8016 - val_accuracy: 0.7301\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5506 - accuracy: 0.8116 - val_loss: 0.7213 - val_accuracy: 0.7692\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.5000 - accuracy: 0.8286 - val_loss: 0.8530 - val_accuracy: 0.7382\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4637 - accuracy: 0.8422 - val_loss: 0.7102 - val_accuracy: 0.7776\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4242 - accuracy: 0.8535 - val_loss: 0.6242 - val_accuracy: 0.7977\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3949 - accuracy: 0.8652 - val_loss: 0.6870 - val_accuracy: 0.7858\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3681 - accuracy: 0.8736 - val_loss: 0.7251 - val_accuracy: 0.7773\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3466 - accuracy: 0.8814 - val_loss: 0.5081 - val_accuracy: 0.8294\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3285 - accuracy: 0.8879 - val_loss: 0.3737 - val_accuracy: 0.8785\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3062 - accuracy: 0.8941 - val_loss: 0.6040 - val_accuracy: 0.8086\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2903 - accuracy: 0.8997 - val_loss: 0.4948 - val_accuracy: 0.8446\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2767 - accuracy: 0.9044 - val_loss: 0.4553 - val_accuracy: 0.8526\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2605 - accuracy: 0.9100 - val_loss: 0.6283 - val_accuracy: 0.8136\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2481 - accuracy: 0.9139 - val_loss: 0.6131 - val_accuracy: 0.8231\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2390 - accuracy: 0.9157 - val_loss: 0.5179 - val_accuracy: 0.8468\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2252 - accuracy: 0.9225 - val_loss: 0.4201 - val_accuracy: 0.8695\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2129 - accuracy: 0.9260 - val_loss: 0.3841 - val_accuracy: 0.8845\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2037 - accuracy: 0.9290 - val_loss: 0.3718 - val_accuracy: 0.8872\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1973 - accuracy: 0.9311 - val_loss: 0.3948 - val_accuracy: 0.8789\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1892 - accuracy: 0.9334 - val_loss: 0.4779 - val_accuracy: 0.8653\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1803 - accuracy: 0.9361 - val_loss: 0.3896 - val_accuracy: 0.8844\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1724 - accuracy: 0.9398 - val_loss: 0.4099 - val_accuracy: 0.8811\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1605 - accuracy: 0.9436 - val_loss: 0.6079 - val_accuracy: 0.8448\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1623 - accuracy: 0.9433 - val_loss: 0.5330 - val_accuracy: 0.8596\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1525 - accuracy: 0.9465 - val_loss: 0.5566 - val_accuracy: 0.8589\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1442 - accuracy: 0.9489 - val_loss: 0.4193 - val_accuracy: 0.8784\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1437 - accuracy: 0.9488 - val_loss: 0.4500 - val_accuracy: 0.8752\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1379 - accuracy: 0.9509 - val_loss: 0.4172 - val_accuracy: 0.8877\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1319 - accuracy: 0.9537 - val_loss: 0.4287 - val_accuracy: 0.8822\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1311 - accuracy: 0.9538 - val_loss: 0.3604 - val_accuracy: 0.8974\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1230 - accuracy: 0.9562 - val_loss: 0.3859 - val_accuracy: 0.8932\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1165 - accuracy: 0.9592 - val_loss: 0.4670 - val_accuracy: 0.8830\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1211 - accuracy: 0.9572 - val_loss: 0.3966 - val_accuracy: 0.8970\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1132 - accuracy: 0.9607 - val_loss: 0.3746 - val_accuracy: 0.9002\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1078 - accuracy: 0.9618 - val_loss: 0.6814 - val_accuracy: 0.8572\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1076 - accuracy: 0.9628 - val_loss: 0.3823 - val_accuracy: 0.9018\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1014 - accuracy: 0.9641 - val_loss: 0.4393 - val_accuracy: 0.8873\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1022 - accuracy: 0.9645 - val_loss: 0.4026 - val_accuracy: 0.8961\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0984 - accuracy: 0.9655 - val_loss: 0.3988 - val_accuracy: 0.8964\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0936 - accuracy: 0.9668 - val_loss: 0.4271 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0919 - accuracy: 0.9675 - val_loss: 0.4038 - val_accuracy: 0.8981\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0923 - accuracy: 0.9684 - val_loss: 0.4136 - val_accuracy: 0.8918\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0902 - accuracy: 0.9686 - val_loss: 0.4377 - val_accuracy: 0.8920\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0885 - accuracy: 0.9692 - val_loss: 0.4277 - val_accuracy: 0.9000\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0803 - accuracy: 0.9719 - val_loss: 0.6035 - val_accuracy: 0.8737\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0812 - accuracy: 0.9717 - val_loss: 0.4338 - val_accuracy: 0.9006\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0814 - accuracy: 0.9723 - val_loss: 0.5015 - val_accuracy: 0.8899\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0830 - accuracy: 0.9714 - val_loss: 0.4891 - val_accuracy: 0.8872\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0793 - accuracy: 0.9719 - val_loss: 0.4138 - val_accuracy: 0.8938\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0747 - accuracy: 0.9736 - val_loss: 0.5171 - val_accuracy: 0.8854\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0746 - accuracy: 0.9750 - val_loss: 0.3745 - val_accuracy: 0.9050\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0781 - accuracy: 0.9728 - val_loss: 0.4465 - val_accuracy: 0.8981\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0675 - accuracy: 0.9762 - val_loss: 0.4920 - val_accuracy: 0.8895\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0696 - accuracy: 0.9764 - val_loss: 0.4358 - val_accuracy: 0.8966\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0700 - accuracy: 0.9752 - val_loss: 0.4157 - val_accuracy: 0.9024\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 0.4410 - val_accuracy: 0.8967\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0651 - accuracy: 0.9772 - val_loss: 0.4720 - val_accuracy: 0.8952\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0654 - accuracy: 0.9771 - val_loss: 0.4376 - val_accuracy: 0.9006\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0648 - accuracy: 0.9778 - val_loss: 0.3928 - val_accuracy: 0.9085\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0675 - accuracy: 0.9759 - val_loss: 0.4342 - val_accuracy: 0.9053\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9782 - val_loss: 0.5436 - val_accuracy: 0.8835\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0646 - accuracy: 0.9784 - val_loss: 0.5166 - val_accuracy: 0.8970\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0609 - accuracy: 0.9792 - val_loss: 0.3889 - val_accuracy: 0.9059\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0574 - accuracy: 0.9806 - val_loss: 0.4567 - val_accuracy: 0.8981\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0581 - accuracy: 0.9808 - val_loss: 0.4143 - val_accuracy: 0.9039\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0590 - accuracy: 0.9801 - val_loss: 0.5653 - val_accuracy: 0.8836\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0589 - accuracy: 0.9804 - val_loss: 0.4405 - val_accuracy: 0.9047\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0590 - accuracy: 0.9796 - val_loss: 0.4100 - val_accuracy: 0.9061\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0546 - accuracy: 0.9818 - val_loss: 0.5131 - val_accuracy: 0.8944\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0549 - accuracy: 0.9809 - val_loss: 0.4243 - val_accuracy: 0.9061\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0517 - accuracy: 0.9818 - val_loss: 0.4303 - val_accuracy: 0.9055\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0544 - accuracy: 0.9811 - val_loss: 0.5365 - val_accuracy: 0.8950\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0541 - accuracy: 0.9817 - val_loss: 0.4692 - val_accuracy: 0.9038\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0501 - accuracy: 0.9832 - val_loss: 0.4319 - val_accuracy: 0.9034\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0521 - accuracy: 0.9820 - val_loss: 0.4811 - val_accuracy: 0.9052\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0488 - accuracy: 0.9834 - val_loss: 0.4772 - val_accuracy: 0.9039\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9838 - val_loss: 0.4740 - val_accuracy: 0.8959\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0514 - accuracy: 0.9823 - val_loss: 0.4223 - val_accuracy: 0.9082\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0453 - accuracy: 0.9847 - val_loss: 0.4666 - val_accuracy: 0.9010\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0508 - accuracy: 0.9826 - val_loss: 0.4438 - val_accuracy: 0.9054\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0491 - accuracy: 0.9833 - val_loss: 0.4725 - val_accuracy: 0.9090\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0437 - accuracy: 0.9852 - val_loss: 0.4488 - val_accuracy: 0.9125\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0484 - accuracy: 0.9835 - val_loss: 0.5380 - val_accuracy: 0.8917\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0452 - accuracy: 0.9841 - val_loss: 0.4374 - val_accuracy: 0.9057\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0443 - accuracy: 0.9846 - val_loss: 0.4322 - val_accuracy: 0.9062\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0492 - accuracy: 0.9835 - val_loss: 0.4872 - val_accuracy: 0.9016\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0424 - accuracy: 0.9853 - val_loss: 0.4733 - val_accuracy: 0.9060\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0461 - accuracy: 0.9839 - val_loss: 0.4844 - val_accuracy: 0.9014\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0430 - accuracy: 0.9849 - val_loss: 0.4510 - val_accuracy: 0.9072\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0426 - accuracy: 0.9855 - val_loss: 0.4207 - val_accuracy: 0.9089\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0419 - accuracy: 0.9854 - val_loss: 0.5389 - val_accuracy: 0.8987\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.4578 - val_accuracy: 0.9028\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0367 - accuracy: 0.9874 - val_loss: 0.5120 - val_accuracy: 0.9025\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0428 - accuracy: 0.9856 - val_loss: 0.4898 - val_accuracy: 0.9060\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0433 - accuracy: 0.9852 - val_loss: 0.4704 - val_accuracy: 0.9066\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0406 - accuracy: 0.9861 - val_loss: 0.4434 - val_accuracy: 0.9079\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0390 - accuracy: 0.9867 - val_loss: 0.4889 - val_accuracy: 0.9017\n",
      "INFO:tensorflow:Assets written to: models2\\model_0.model\\assets\n",
      "GoogLeNet 1 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 30s 39ms/step - loss: 1.4514 - accuracy: 0.4696 - val_loss: 1.1475 - val_accuracy: 0.5822\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.9995 - accuracy: 0.6465 - val_loss: 0.8182 - val_accuracy: 0.7164\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.8152 - accuracy: 0.7158 - val_loss: 0.7897 - val_accuracy: 0.7260\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6848 - accuracy: 0.7633 - val_loss: 1.0064 - val_accuracy: 0.6853\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.6049 - accuracy: 0.7944 - val_loss: 0.8445 - val_accuracy: 0.7147\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.5470 - accuracy: 0.8129 - val_loss: 0.7423 - val_accuracy: 0.7560\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4947 - accuracy: 0.8300 - val_loss: 0.5878 - val_accuracy: 0.8004\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4626 - accuracy: 0.8426 - val_loss: 0.4870 - val_accuracy: 0.8400\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.4276 - accuracy: 0.8528 - val_loss: 0.5702 - val_accuracy: 0.8038\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.4044 - accuracy: 0.8608 - val_loss: 0.4216 - val_accuracy: 0.8578\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3732 - accuracy: 0.8712 - val_loss: 0.5204 - val_accuracy: 0.8282\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3515 - accuracy: 0.8795 - val_loss: 0.7327 - val_accuracy: 0.7776\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3372 - accuracy: 0.8847 - val_loss: 0.4795 - val_accuracy: 0.8410\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.3158 - accuracy: 0.8906 - val_loss: 0.4133 - val_accuracy: 0.8595\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.3029 - accuracy: 0.8952 - val_loss: 0.4774 - val_accuracy: 0.8533\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2821 - accuracy: 0.9027 - val_loss: 0.4983 - val_accuracy: 0.8467\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2663 - accuracy: 0.9072 - val_loss: 0.4748 - val_accuracy: 0.8485\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2581 - accuracy: 0.9111 - val_loss: 0.3640 - val_accuracy: 0.8853\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2446 - accuracy: 0.9159 - val_loss: 0.4759 - val_accuracy: 0.8546\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2332 - accuracy: 0.9191 - val_loss: 0.5409 - val_accuracy: 0.8473\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.2279 - accuracy: 0.9208 - val_loss: 0.4160 - val_accuracy: 0.8697\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2119 - accuracy: 0.9263 - val_loss: 0.4622 - val_accuracy: 0.8653\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.2026 - accuracy: 0.9294 - val_loss: 0.4333 - val_accuracy: 0.8758\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1943 - accuracy: 0.9317 - val_loss: 0.4421 - val_accuracy: 0.8703\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1862 - accuracy: 0.9353 - val_loss: 0.5245 - val_accuracy: 0.8541\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1825 - accuracy: 0.9362 - val_loss: 0.3994 - val_accuracy: 0.8784\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 29s 38ms/step - loss: 0.1714 - accuracy: 0.9403 - val_loss: 0.4550 - val_accuracy: 0.8789\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 29s 37ms/step - loss: 0.1680 - accuracy: 0.9415 - val_loss: 0.4356 - val_accuracy: 0.8686\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1586 - accuracy: 0.9438 - val_loss: 0.4311 - val_accuracy: 0.8722\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1558 - accuracy: 0.9452 - val_loss: 0.6042 - val_accuracy: 0.8439\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1476 - accuracy: 0.9480 - val_loss: 0.5505 - val_accuracy: 0.8573\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1447 - accuracy: 0.9482 - val_loss: 0.4867 - val_accuracy: 0.8783\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1397 - accuracy: 0.9502 - val_loss: 0.4765 - val_accuracy: 0.8745\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1334 - accuracy: 0.9534 - val_loss: 0.3944 - val_accuracy: 0.8906\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1308 - accuracy: 0.9542 - val_loss: 0.3801 - val_accuracy: 0.8889\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1269 - accuracy: 0.9554 - val_loss: 0.4125 - val_accuracy: 0.8895\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1224 - accuracy: 0.9567 - val_loss: 0.4900 - val_accuracy: 0.8715\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1153 - accuracy: 0.9585 - val_loss: 0.3544 - val_accuracy: 0.8955\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1178 - accuracy: 0.9583 - val_loss: 0.4350 - val_accuracy: 0.8805\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1125 - accuracy: 0.9604 - val_loss: 0.4197 - val_accuracy: 0.8911\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1097 - accuracy: 0.9615 - val_loss: 0.4528 - val_accuracy: 0.8832\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1062 - accuracy: 0.9621 - val_loss: 0.4377 - val_accuracy: 0.8825\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1030 - accuracy: 0.9640 - val_loss: 0.4928 - val_accuracy: 0.8810\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.1024 - accuracy: 0.9635 - val_loss: 0.4442 - val_accuracy: 0.8837\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0966 - accuracy: 0.9654 - val_loss: 0.4523 - val_accuracy: 0.8853\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0922 - accuracy: 0.9680 - val_loss: 0.4071 - val_accuracy: 0.8969\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0950 - accuracy: 0.9662 - val_loss: 0.5126 - val_accuracy: 0.8787\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0865 - accuracy: 0.9694 - val_loss: 0.4044 - val_accuracy: 0.8988\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0910 - accuracy: 0.9677 - val_loss: 0.4438 - val_accuracy: 0.8875\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0890 - accuracy: 0.9685 - val_loss: 0.4485 - val_accuracy: 0.8894\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0849 - accuracy: 0.9704 - val_loss: 0.5904 - val_accuracy: 0.8694\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0855 - accuracy: 0.9698 - val_loss: 0.4282 - val_accuracy: 0.8943\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0850 - accuracy: 0.9708 - val_loss: 0.4532 - val_accuracy: 0.8824\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0796 - accuracy: 0.9728 - val_loss: 0.4631 - val_accuracy: 0.8867\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0788 - accuracy: 0.9723 - val_loss: 0.4536 - val_accuracy: 0.8931\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0793 - accuracy: 0.9721 - val_loss: 0.4542 - val_accuracy: 0.8945\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0771 - accuracy: 0.9718 - val_loss: 0.4347 - val_accuracy: 0.9004\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0735 - accuracy: 0.9746 - val_loss: 0.4219 - val_accuracy: 0.8927\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0736 - accuracy: 0.9752 - val_loss: 0.5068 - val_accuracy: 0.8897\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0705 - accuracy: 0.9757 - val_loss: 0.4158 - val_accuracy: 0.9033\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0732 - accuracy: 0.9749 - val_loss: 0.4575 - val_accuracy: 0.8934\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0684 - accuracy: 0.9768 - val_loss: 0.5284 - val_accuracy: 0.8849\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0726 - accuracy: 0.9742 - val_loss: 0.4812 - val_accuracy: 0.8999\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0635 - accuracy: 0.9781 - val_loss: 0.5953 - val_accuracy: 0.8830\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0665 - accuracy: 0.9769 - val_loss: 0.5687 - val_accuracy: 0.8851\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0699 - accuracy: 0.9762 - val_loss: 0.4570 - val_accuracy: 0.8974\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0636 - accuracy: 0.9776 - val_loss: 0.5207 - val_accuracy: 0.8901\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9788 - val_loss: 0.5135 - val_accuracy: 0.8960\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0615 - accuracy: 0.9781 - val_loss: 0.4663 - val_accuracy: 0.8971\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0603 - accuracy: 0.9795 - val_loss: 0.4484 - val_accuracy: 0.9007\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.4567 - val_accuracy: 0.9005\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0579 - accuracy: 0.9801 - val_loss: 0.6181 - val_accuracy: 0.8819\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0574 - accuracy: 0.9806 - val_loss: 0.4929 - val_accuracy: 0.9036\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0548 - accuracy: 0.9809 - val_loss: 0.4473 - val_accuracy: 0.9059\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0565 - accuracy: 0.9800 - val_loss: 0.4874 - val_accuracy: 0.9017\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0551 - accuracy: 0.9810 - val_loss: 0.4378 - val_accuracy: 0.9028\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0560 - accuracy: 0.9804 - val_loss: 0.4813 - val_accuracy: 0.8956\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0553 - accuracy: 0.9805 - val_loss: 0.5308 - val_accuracy: 0.8963\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0542 - accuracy: 0.9813 - val_loss: 0.4860 - val_accuracy: 0.8967\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0495 - accuracy: 0.9831 - val_loss: 0.5194 - val_accuracy: 0.8922\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.4334 - val_accuracy: 0.9076\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0534 - accuracy: 0.9815 - val_loss: 0.4690 - val_accuracy: 0.9027\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0541 - accuracy: 0.9813 - val_loss: 0.4667 - val_accuracy: 0.8981\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0486 - accuracy: 0.9829 - val_loss: 0.4436 - val_accuracy: 0.9011\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0498 - accuracy: 0.9829 - val_loss: 0.4872 - val_accuracy: 0.8948\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.5638 - val_accuracy: 0.8925\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0492 - accuracy: 0.9832 - val_loss: 0.4495 - val_accuracy: 0.9027\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0440 - accuracy: 0.9852 - val_loss: 0.5623 - val_accuracy: 0.8910\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0505 - accuracy: 0.9827 - val_loss: 0.5257 - val_accuracy: 0.8925\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0465 - accuracy: 0.9840 - val_loss: 0.5290 - val_accuracy: 0.8972\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0476 - accuracy: 0.9833 - val_loss: 0.5412 - val_accuracy: 0.8868\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0454 - accuracy: 0.9846 - val_loss: 0.5121 - val_accuracy: 0.9006\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0471 - accuracy: 0.9839 - val_loss: 0.5071 - val_accuracy: 0.9028\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0440 - accuracy: 0.9850 - val_loss: 0.4853 - val_accuracy: 0.8978\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0451 - accuracy: 0.9845 - val_loss: 0.5159 - val_accuracy: 0.8963\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0437 - accuracy: 0.9845 - val_loss: 0.4623 - val_accuracy: 0.9061\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0422 - accuracy: 0.9852 - val_loss: 0.5638 - val_accuracy: 0.8927\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0454 - accuracy: 0.9848 - val_loss: 0.5611 - val_accuracy: 0.8914\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0453 - accuracy: 0.9846 - val_loss: 0.5295 - val_accuracy: 0.8981\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 28s 36ms/step - loss: 0.0407 - accuracy: 0.9862 - val_loss: 0.4609 - val_accuracy: 0.9045\n",
      "INFO:tensorflow:Assets written to: models2\\model_1.model\\assets\n",
      "ResNet 2 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 76s 97ms/step - loss: 1.8631 - accuracy: 0.4380 - val_loss: 1.8144 - val_accuracy: 0.4840\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.3074 - accuracy: 0.6275 - val_loss: 1.2244 - val_accuracy: 0.6608\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 1.1097 - accuracy: 0.6984 - val_loss: 1.1312 - val_accuracy: 0.7021\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 0.9826 - accuracy: 0.7422 - val_loss: 1.0237 - val_accuracy: 0.7336\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9123 - accuracy: 0.7664 - val_loss: 0.9432 - val_accuracy: 0.7675\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8580 - accuracy: 0.7851 - val_loss: 0.9200 - val_accuracy: 0.7747\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 84ms/step - loss: 0.8177 - accuracy: 0.8013 - val_loss: 0.8841 - val_accuracy: 0.7852\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7850 - accuracy: 0.8103 - val_loss: 0.8932 - val_accuracy: 0.7825\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7560 - accuracy: 0.8207 - val_loss: 0.9518 - val_accuracy: 0.7614\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7375 - accuracy: 0.8246 - val_loss: 0.8646 - val_accuracy: 0.7927\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7133 - accuracy: 0.8366 - val_loss: 0.8219 - val_accuracy: 0.8069\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7003 - accuracy: 0.8381 - val_loss: 0.8647 - val_accuracy: 0.7982\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6860 - accuracy: 0.8434 - val_loss: 0.9090 - val_accuracy: 0.7855\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6696 - accuracy: 0.8495 - val_loss: 0.7049 - val_accuracy: 0.8410\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6549 - accuracy: 0.8546 - val_loss: 0.6822 - val_accuracy: 0.8397\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6459 - accuracy: 0.8573 - val_loss: 0.8086 - val_accuracy: 0.8163\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6396 - accuracy: 0.8587 - val_loss: 0.7837 - val_accuracy: 0.8170\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6274 - accuracy: 0.8630 - val_loss: 0.7779 - val_accuracy: 0.8201\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6195 - accuracy: 0.8670 - val_loss: 0.7379 - val_accuracy: 0.8285\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6083 - accuracy: 0.8698 - val_loss: 0.6733 - val_accuracy: 0.8520\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6021 - accuracy: 0.8713 - val_loss: 0.6376 - val_accuracy: 0.8627\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5916 - accuracy: 0.8746 - val_loss: 0.6962 - val_accuracy: 0.8463\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5908 - accuracy: 0.8756 - val_loss: 0.7456 - val_accuracy: 0.8277\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5798 - accuracy: 0.8775 - val_loss: 0.6775 - val_accuracy: 0.8508\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5728 - accuracy: 0.8809 - val_loss: 0.6564 - val_accuracy: 0.8585\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5717 - accuracy: 0.8814 - val_loss: 0.7176 - val_accuracy: 0.8419\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5686 - accuracy: 0.8821 - val_loss: 0.6701 - val_accuracy: 0.8537\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5573 - accuracy: 0.8866 - val_loss: 0.6797 - val_accuracy: 0.8470\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5522 - accuracy: 0.8889 - val_loss: 0.6748 - val_accuracy: 0.8584\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5514 - accuracy: 0.8882 - val_loss: 0.6173 - val_accuracy: 0.8706\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5457 - accuracy: 0.8905 - val_loss: 0.6898 - val_accuracy: 0.8491\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5446 - accuracy: 0.8905 - val_loss: 0.7335 - val_accuracy: 0.8371\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5442 - accuracy: 0.8919 - val_loss: 0.6491 - val_accuracy: 0.8631\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5350 - accuracy: 0.8940 - val_loss: 0.6163 - val_accuracy: 0.8705\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5271 - accuracy: 0.8967 - val_loss: 0.6937 - val_accuracy: 0.8513\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5284 - accuracy: 0.8959 - val_loss: 0.6300 - val_accuracy: 0.8677\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5248 - accuracy: 0.8954 - val_loss: 0.6087 - val_accuracy: 0.8715\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5208 - accuracy: 0.8970 - val_loss: 0.6421 - val_accuracy: 0.8640\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5192 - accuracy: 0.8982 - val_loss: 0.6311 - val_accuracy: 0.8695\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5175 - accuracy: 0.8994 - val_loss: 0.6293 - val_accuracy: 0.8666\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5108 - accuracy: 0.9010 - val_loss: 0.6045 - val_accuracy: 0.8778\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5128 - accuracy: 0.9005 - val_loss: 0.6277 - val_accuracy: 0.8683\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5104 - accuracy: 0.9002 - val_loss: 0.6566 - val_accuracy: 0.8607\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5067 - accuracy: 0.9018 - val_loss: 0.6869 - val_accuracy: 0.8474\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5073 - accuracy: 0.9010 - val_loss: 0.6283 - val_accuracy: 0.8741\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5024 - accuracy: 0.9054 - val_loss: 0.6285 - val_accuracy: 0.8682\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4993 - accuracy: 0.9050 - val_loss: 0.5581 - val_accuracy: 0.8860\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4970 - accuracy: 0.9058 - val_loss: 0.6204 - val_accuracy: 0.8723\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4965 - accuracy: 0.9059 - val_loss: 0.6862 - val_accuracy: 0.8548\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4883 - accuracy: 0.9087 - val_loss: 0.6207 - val_accuracy: 0.8746\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4945 - accuracy: 0.9064 - val_loss: 0.5941 - val_accuracy: 0.8763\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4882 - accuracy: 0.9087 - val_loss: 0.5865 - val_accuracy: 0.8817\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4876 - accuracy: 0.9065 - val_loss: 0.5905 - val_accuracy: 0.8841\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9084 - val_loss: 0.6143 - val_accuracy: 0.8703\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4812 - accuracy: 0.9108 - val_loss: 0.6205 - val_accuracy: 0.8693\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4841 - accuracy: 0.9075 - val_loss: 0.5546 - val_accuracy: 0.8913\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4741 - accuracy: 0.9114 - val_loss: 0.6053 - val_accuracy: 0.8722\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4790 - accuracy: 0.9112 - val_loss: 0.6122 - val_accuracy: 0.8726\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4748 - accuracy: 0.9116 - val_loss: 0.6241 - val_accuracy: 0.8709\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4721 - accuracy: 0.9123 - val_loss: 0.5854 - val_accuracy: 0.8822\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4710 - accuracy: 0.9136 - val_loss: 0.5858 - val_accuracy: 0.8847\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4685 - accuracy: 0.9138 - val_loss: 0.5629 - val_accuracy: 0.8856\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4699 - accuracy: 0.9134 - val_loss: 0.6211 - val_accuracy: 0.8775\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4668 - accuracy: 0.9137 - val_loss: 0.5882 - val_accuracy: 0.8817\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4623 - accuracy: 0.9156 - val_loss: 0.6142 - val_accuracy: 0.8775\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4640 - accuracy: 0.9163 - val_loss: 0.5912 - val_accuracy: 0.8822\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4571 - accuracy: 0.9165 - val_loss: 0.5919 - val_accuracy: 0.8785\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4591 - accuracy: 0.9154 - val_loss: 0.6635 - val_accuracy: 0.8628\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4604 - accuracy: 0.9165 - val_loss: 0.5725 - val_accuracy: 0.8872\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4580 - accuracy: 0.9169 - val_loss: 0.5807 - val_accuracy: 0.8857\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4539 - accuracy: 0.9181 - val_loss: 0.6158 - val_accuracy: 0.8765\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4526 - accuracy: 0.9189 - val_loss: 0.6030 - val_accuracy: 0.8790\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4505 - accuracy: 0.9190 - val_loss: 0.6085 - val_accuracy: 0.8781\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4507 - accuracy: 0.9186 - val_loss: 0.5774 - val_accuracy: 0.8819\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4495 - accuracy: 0.9193 - val_loss: 0.6349 - val_accuracy: 0.8684\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4475 - accuracy: 0.9190 - val_loss: 0.6264 - val_accuracy: 0.8753\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9212 - val_loss: 0.5981 - val_accuracy: 0.8822\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4470 - accuracy: 0.9194 - val_loss: 0.6188 - val_accuracy: 0.8754\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9196 - val_loss: 0.6280 - val_accuracy: 0.8745\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4464 - accuracy: 0.9192 - val_loss: 0.6167 - val_accuracy: 0.8770\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4431 - accuracy: 0.9202 - val_loss: 0.5611 - val_accuracy: 0.8893\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4432 - accuracy: 0.9214 - val_loss: 0.5901 - val_accuracy: 0.8818\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4393 - accuracy: 0.9218 - val_loss: 0.6012 - val_accuracy: 0.8805\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4372 - accuracy: 0.9218 - val_loss: 0.5373 - val_accuracy: 0.8949\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4346 - accuracy: 0.9223 - val_loss: 0.6064 - val_accuracy: 0.8784\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4354 - accuracy: 0.9221 - val_loss: 0.6834 - val_accuracy: 0.8602\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4378 - accuracy: 0.9212 - val_loss: 0.5626 - val_accuracy: 0.8885\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4332 - accuracy: 0.9218 - val_loss: 0.5580 - val_accuracy: 0.8915\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4351 - accuracy: 0.9216 - val_loss: 0.6093 - val_accuracy: 0.8797\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4331 - accuracy: 0.9221 - val_loss: 0.5625 - val_accuracy: 0.8883\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4264 - accuracy: 0.9253 - val_loss: 0.5389 - val_accuracy: 0.8945\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4330 - accuracy: 0.9231 - val_loss: 0.5508 - val_accuracy: 0.8925\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4267 - accuracy: 0.9244 - val_loss: 0.5509 - val_accuracy: 0.8886\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4271 - accuracy: 0.9243 - val_loss: 0.5602 - val_accuracy: 0.8891\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4261 - accuracy: 0.9244 - val_loss: 0.5675 - val_accuracy: 0.8868\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4297 - accuracy: 0.9237 - val_loss: 0.6033 - val_accuracy: 0.8772\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4258 - accuracy: 0.9251 - val_loss: 0.6244 - val_accuracy: 0.8735\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4250 - accuracy: 0.9249 - val_loss: 0.5957 - val_accuracy: 0.8791\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4235 - accuracy: 0.9243 - val_loss: 0.5815 - val_accuracy: 0.8843\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4239 - accuracy: 0.9245 - val_loss: 0.5446 - val_accuracy: 0.8946\n",
      "INFO:tensorflow:Assets written to: models2\\model_2.model\\assets\n",
      "ResNet 3 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 74s 94ms/step - loss: 1.8830 - accuracy: 0.4308 - val_loss: 1.7444 - val_accuracy: 0.4731\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 64s 83ms/step - loss: 1.3313 - accuracy: 0.6195 - val_loss: 1.2948 - val_accuracy: 0.6347\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.1226 - accuracy: 0.6918 - val_loss: 1.1531 - val_accuracy: 0.6876\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.0059 - accuracy: 0.7350 - val_loss: 1.1540 - val_accuracy: 0.6881\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9137 - accuracy: 0.7697 - val_loss: 1.0139 - val_accuracy: 0.7520\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8653 - accuracy: 0.7861 - val_loss: 0.9415 - val_accuracy: 0.7619\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8257 - accuracy: 0.7991 - val_loss: 1.0203 - val_accuracy: 0.7462\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7901 - accuracy: 0.8118 - val_loss: 0.8541 - val_accuracy: 0.7918\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7626 - accuracy: 0.8218 - val_loss: 0.8655 - val_accuracy: 0.7877\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7467 - accuracy: 0.8243 - val_loss: 0.7796 - val_accuracy: 0.8096\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7239 - accuracy: 0.8334 - val_loss: 0.8481 - val_accuracy: 0.7958\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7044 - accuracy: 0.8395 - val_loss: 0.8144 - val_accuracy: 0.8086\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6922 - accuracy: 0.8429 - val_loss: 0.7682 - val_accuracy: 0.8213\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6747 - accuracy: 0.8483 - val_loss: 0.8615 - val_accuracy: 0.7948\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6619 - accuracy: 0.8539 - val_loss: 0.7023 - val_accuracy: 0.8388\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6528 - accuracy: 0.8553 - val_loss: 0.7099 - val_accuracy: 0.8355\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6453 - accuracy: 0.8582 - val_loss: 0.7411 - val_accuracy: 0.8271\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6298 - accuracy: 0.8629 - val_loss: 0.7629 - val_accuracy: 0.8240\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6206 - accuracy: 0.8659 - val_loss: 0.7072 - val_accuracy: 0.8385\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6125 - accuracy: 0.8678 - val_loss: 0.7787 - val_accuracy: 0.8176\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6067 - accuracy: 0.8706 - val_loss: 0.8452 - val_accuracy: 0.8081\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5978 - accuracy: 0.8730 - val_loss: 0.7179 - val_accuracy: 0.8400\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5916 - accuracy: 0.8764 - val_loss: 0.6601 - val_accuracy: 0.8516\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5811 - accuracy: 0.8777 - val_loss: 0.7291 - val_accuracy: 0.8392\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5778 - accuracy: 0.8791 - val_loss: 0.6543 - val_accuracy: 0.8627\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5723 - accuracy: 0.8813 - val_loss: 0.7420 - val_accuracy: 0.8335\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5679 - accuracy: 0.8820 - val_loss: 0.6640 - val_accuracy: 0.8526\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5591 - accuracy: 0.8858 - val_loss: 0.7656 - val_accuracy: 0.8283\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5550 - accuracy: 0.8861 - val_loss: 0.6517 - val_accuracy: 0.8582\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5508 - accuracy: 0.8885 - val_loss: 0.7887 - val_accuracy: 0.8228\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5461 - accuracy: 0.8901 - val_loss: 0.6778 - val_accuracy: 0.8517\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5413 - accuracy: 0.8910 - val_loss: 0.6209 - val_accuracy: 0.8670\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5388 - accuracy: 0.8918 - val_loss: 0.6643 - val_accuracy: 0.8535\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5346 - accuracy: 0.8922 - val_loss: 0.7164 - val_accuracy: 0.8404\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5316 - accuracy: 0.8940 - val_loss: 0.6788 - val_accuracy: 0.8515\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5307 - accuracy: 0.8930 - val_loss: 0.6903 - val_accuracy: 0.8499\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5252 - accuracy: 0.8958 - val_loss: 0.5977 - val_accuracy: 0.8762\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5154 - accuracy: 0.9002 - val_loss: 0.6262 - val_accuracy: 0.8700\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5202 - accuracy: 0.8948 - val_loss: 0.6134 - val_accuracy: 0.8713\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5163 - accuracy: 0.8987 - val_loss: 0.6511 - val_accuracy: 0.8593\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5124 - accuracy: 0.8987 - val_loss: 0.6667 - val_accuracy: 0.8605\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5063 - accuracy: 0.9017 - val_loss: 0.6162 - val_accuracy: 0.8706\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5060 - accuracy: 0.8998 - val_loss: 0.7083 - val_accuracy: 0.8517\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5074 - accuracy: 0.9008 - val_loss: 0.6909 - val_accuracy: 0.8521\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5003 - accuracy: 0.9028 - val_loss: 0.6572 - val_accuracy: 0.8616\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4987 - accuracy: 0.9030 - val_loss: 0.6520 - val_accuracy: 0.8649\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4962 - accuracy: 0.9045 - val_loss: 0.6632 - val_accuracy: 0.8633\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4943 - accuracy: 0.9050 - val_loss: 0.6229 - val_accuracy: 0.8704\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4912 - accuracy: 0.9066 - val_loss: 0.6265 - val_accuracy: 0.8725\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4885 - accuracy: 0.9058 - val_loss: 0.6336 - val_accuracy: 0.8685\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9091 - val_loss: 0.6498 - val_accuracy: 0.8590\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4845 - accuracy: 0.9081 - val_loss: 0.6561 - val_accuracy: 0.8655\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4822 - accuracy: 0.9090 - val_loss: 0.5914 - val_accuracy: 0.8780\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4810 - accuracy: 0.9082 - val_loss: 0.6554 - val_accuracy: 0.8607\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4776 - accuracy: 0.9107 - val_loss: 0.6046 - val_accuracy: 0.8779\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4782 - accuracy: 0.9092 - val_loss: 0.5933 - val_accuracy: 0.8794\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4774 - accuracy: 0.9102 - val_loss: 0.6570 - val_accuracy: 0.8670\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4734 - accuracy: 0.9119 - val_loss: 0.5907 - val_accuracy: 0.8781\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4691 - accuracy: 0.9125 - val_loss: 0.6361 - val_accuracy: 0.8663\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4700 - accuracy: 0.9123 - val_loss: 0.6316 - val_accuracy: 0.8704\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4657 - accuracy: 0.9133 - val_loss: 0.6209 - val_accuracy: 0.8714\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4654 - accuracy: 0.9141 - val_loss: 0.6262 - val_accuracy: 0.8703\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4656 - accuracy: 0.9128 - val_loss: 0.6475 - val_accuracy: 0.8655\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4608 - accuracy: 0.9154 - val_loss: 0.6262 - val_accuracy: 0.8709\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4621 - accuracy: 0.9161 - val_loss: 0.6147 - val_accuracy: 0.8763\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4578 - accuracy: 0.9169 - val_loss: 0.5951 - val_accuracy: 0.8792\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4548 - accuracy: 0.9169 - val_loss: 0.6190 - val_accuracy: 0.8736\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4585 - accuracy: 0.9159 - val_loss: 0.5914 - val_accuracy: 0.8781\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4570 - accuracy: 0.9157 - val_loss: 0.6282 - val_accuracy: 0.8724\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4524 - accuracy: 0.9178 - val_loss: 0.6487 - val_accuracy: 0.8693\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4564 - accuracy: 0.9150 - val_loss: 0.6432 - val_accuracy: 0.8629\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4499 - accuracy: 0.9187 - val_loss: 0.5559 - val_accuracy: 0.8872\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4490 - accuracy: 0.9177 - val_loss: 0.6520 - val_accuracy: 0.8635\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4501 - accuracy: 0.9173 - val_loss: 0.5762 - val_accuracy: 0.8855\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4436 - accuracy: 0.9196 - val_loss: 0.5810 - val_accuracy: 0.8828\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4497 - accuracy: 0.9176 - val_loss: 0.6335 - val_accuracy: 0.8700\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4448 - accuracy: 0.9190 - val_loss: 0.5861 - val_accuracy: 0.8796\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4462 - accuracy: 0.9186 - val_loss: 0.5905 - val_accuracy: 0.8817\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4413 - accuracy: 0.9199 - val_loss: 0.6237 - val_accuracy: 0.8754\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4450 - accuracy: 0.9201 - val_loss: 0.5908 - val_accuracy: 0.8793\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4421 - accuracy: 0.9208 - val_loss: 0.6184 - val_accuracy: 0.8782\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4361 - accuracy: 0.9223 - val_loss: 0.5965 - val_accuracy: 0.8778\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4378 - accuracy: 0.9203 - val_loss: 0.5489 - val_accuracy: 0.8902\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4327 - accuracy: 0.9224 - val_loss: 0.5867 - val_accuracy: 0.8846\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4367 - accuracy: 0.9227 - val_loss: 0.5905 - val_accuracy: 0.8801\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4332 - accuracy: 0.9216 - val_loss: 0.6071 - val_accuracy: 0.8762\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4323 - accuracy: 0.9221 - val_loss: 0.5944 - val_accuracy: 0.8824\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4308 - accuracy: 0.9244 - val_loss: 0.5538 - val_accuracy: 0.8882\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4312 - accuracy: 0.9230 - val_loss: 0.6181 - val_accuracy: 0.8746\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4299 - accuracy: 0.9232 - val_loss: 0.5417 - val_accuracy: 0.8977\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4247 - accuracy: 0.9257 - val_loss: 0.5673 - val_accuracy: 0.8879\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4298 - accuracy: 0.9229 - val_loss: 0.5465 - val_accuracy: 0.8941\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4260 - accuracy: 0.9250 - val_loss: 0.5468 - val_accuracy: 0.8942\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4308 - accuracy: 0.9229 - val_loss: 0.5858 - val_accuracy: 0.8834\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4251 - accuracy: 0.9229 - val_loss: 0.6040 - val_accuracy: 0.8746\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4236 - accuracy: 0.9253 - val_loss: 0.6057 - val_accuracy: 0.8775\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4217 - accuracy: 0.9247 - val_loss: 0.6032 - val_accuracy: 0.8776\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4247 - accuracy: 0.9232 - val_loss: 0.6257 - val_accuracy: 0.8711\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4200 - accuracy: 0.9252 - val_loss: 0.5983 - val_accuracy: 0.8791\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4224 - accuracy: 0.9262 - val_loss: 0.5613 - val_accuracy: 0.8876\n",
      "INFO:tensorflow:Assets written to: models2\\model_3.model\\assets\n",
      "ResNet 4 is being trained...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "781/781 [==============================] - 74s 95ms/step - loss: 1.8416 - accuracy: 0.4536 - val_loss: 1.6534 - val_accuracy: 0.5336\n",
      "Epoch 2/100\n",
      "781/781 [==============================] - 64s 83ms/step - loss: 1.3015 - accuracy: 0.6315 - val_loss: 1.2329 - val_accuracy: 0.6622\n",
      "Epoch 3/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 1.1066 - accuracy: 0.7015 - val_loss: 1.1406 - val_accuracy: 0.7065\n",
      "Epoch 4/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9875 - accuracy: 0.7425 - val_loss: 1.1783 - val_accuracy: 0.7026\n",
      "Epoch 5/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.9160 - accuracy: 0.7680 - val_loss: 1.0245 - val_accuracy: 0.7358\n",
      "Epoch 6/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8639 - accuracy: 0.7846 - val_loss: 0.9584 - val_accuracy: 0.7592\n",
      "Epoch 7/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.8259 - accuracy: 0.7976 - val_loss: 0.8884 - val_accuracy: 0.7862\n",
      "Epoch 8/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7935 - accuracy: 0.8085 - val_loss: 0.8591 - val_accuracy: 0.7900\n",
      "Epoch 9/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7658 - accuracy: 0.8164 - val_loss: 0.8839 - val_accuracy: 0.7828\n",
      "Epoch 10/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7462 - accuracy: 0.8242 - val_loss: 0.8030 - val_accuracy: 0.8043\n",
      "Epoch 11/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7259 - accuracy: 0.8323 - val_loss: 0.8719 - val_accuracy: 0.7972\n",
      "Epoch 12/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.7118 - accuracy: 0.8355 - val_loss: 0.7661 - val_accuracy: 0.8199\n",
      "Epoch 13/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6887 - accuracy: 0.8437 - val_loss: 0.7842 - val_accuracy: 0.8142\n",
      "Epoch 14/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6775 - accuracy: 0.8472 - val_loss: 0.7835 - val_accuracy: 0.8197\n",
      "Epoch 15/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6714 - accuracy: 0.8486 - val_loss: 0.7032 - val_accuracy: 0.8418\n",
      "Epoch 16/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6503 - accuracy: 0.8571 - val_loss: 0.8565 - val_accuracy: 0.7938\n",
      "Epoch 17/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6454 - accuracy: 0.8586 - val_loss: 0.6970 - val_accuracy: 0.8434\n",
      "Epoch 18/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6362 - accuracy: 0.8620 - val_loss: 0.6756 - val_accuracy: 0.8534\n",
      "Epoch 19/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6212 - accuracy: 0.8664 - val_loss: 0.7273 - val_accuracy: 0.8321\n",
      "Epoch 20/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6180 - accuracy: 0.8668 - val_loss: 0.7825 - val_accuracy: 0.8193\n",
      "Epoch 21/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.6090 - accuracy: 0.8707 - val_loss: 0.6648 - val_accuracy: 0.8545\n",
      "Epoch 22/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5987 - accuracy: 0.8731 - val_loss: 0.6669 - val_accuracy: 0.8526\n",
      "Epoch 23/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5932 - accuracy: 0.8750 - val_loss: 0.7171 - val_accuracy: 0.8374\n",
      "Epoch 24/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5847 - accuracy: 0.8793 - val_loss: 0.6979 - val_accuracy: 0.8446\n",
      "Epoch 25/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5859 - accuracy: 0.8762 - val_loss: 0.7390 - val_accuracy: 0.8358\n",
      "Epoch 26/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5739 - accuracy: 0.8812 - val_loss: 0.6648 - val_accuracy: 0.8543\n",
      "Epoch 27/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5694 - accuracy: 0.8817 - val_loss: 0.6452 - val_accuracy: 0.8607\n",
      "Epoch 28/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5637 - accuracy: 0.8851 - val_loss: 0.6590 - val_accuracy: 0.8560\n",
      "Epoch 29/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5593 - accuracy: 0.8852 - val_loss: 0.7281 - val_accuracy: 0.8394\n",
      "Epoch 30/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5567 - accuracy: 0.8865 - val_loss: 0.7066 - val_accuracy: 0.8456\n",
      "Epoch 31/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5535 - accuracy: 0.8879 - val_loss: 0.7181 - val_accuracy: 0.8450\n",
      "Epoch 32/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5442 - accuracy: 0.8905 - val_loss: 0.7460 - val_accuracy: 0.8452\n",
      "Epoch 33/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5397 - accuracy: 0.8926 - val_loss: 0.6630 - val_accuracy: 0.8551\n",
      "Epoch 34/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5390 - accuracy: 0.8923 - val_loss: 0.7151 - val_accuracy: 0.8422\n",
      "Epoch 35/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5307 - accuracy: 0.8956 - val_loss: 0.6178 - val_accuracy: 0.8722\n",
      "Epoch 36/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5279 - accuracy: 0.8950 - val_loss: 0.6377 - val_accuracy: 0.8687\n",
      "Epoch 37/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5248 - accuracy: 0.8952 - val_loss: 0.7201 - val_accuracy: 0.8442\n",
      "Epoch 38/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5269 - accuracy: 0.8972 - val_loss: 0.6333 - val_accuracy: 0.8659\n",
      "Epoch 39/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5196 - accuracy: 0.8979 - val_loss: 0.6565 - val_accuracy: 0.8588\n",
      "Epoch 40/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5182 - accuracy: 0.8992 - val_loss: 0.6462 - val_accuracy: 0.8587\n",
      "Epoch 41/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5135 - accuracy: 0.9002 - val_loss: 0.6468 - val_accuracy: 0.8637\n",
      "Epoch 42/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5147 - accuracy: 0.9011 - val_loss: 0.7377 - val_accuracy: 0.8382\n",
      "Epoch 43/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5103 - accuracy: 0.9016 - val_loss: 0.6191 - val_accuracy: 0.8718\n",
      "Epoch 44/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5075 - accuracy: 0.9021 - val_loss: 0.6002 - val_accuracy: 0.8757\n",
      "Epoch 45/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5049 - accuracy: 0.9031 - val_loss: 0.6029 - val_accuracy: 0.8770\n",
      "Epoch 46/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5039 - accuracy: 0.9033 - val_loss: 0.6285 - val_accuracy: 0.8642\n",
      "Epoch 47/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4981 - accuracy: 0.9050 - val_loss: 0.5793 - val_accuracy: 0.8872\n",
      "Epoch 48/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4957 - accuracy: 0.9059 - val_loss: 0.5852 - val_accuracy: 0.8785\n",
      "Epoch 49/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4921 - accuracy: 0.9068 - val_loss: 0.6251 - val_accuracy: 0.8683\n",
      "Epoch 50/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4938 - accuracy: 0.9073 - val_loss: 0.6697 - val_accuracy: 0.8583\n",
      "Epoch 51/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4923 - accuracy: 0.9064 - val_loss: 0.5541 - val_accuracy: 0.8897\n",
      "Epoch 52/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4843 - accuracy: 0.9104 - val_loss: 0.5654 - val_accuracy: 0.8893\n",
      "Epoch 53/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4816 - accuracy: 0.9092 - val_loss: 0.6853 - val_accuracy: 0.8558\n",
      "Epoch 54/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4808 - accuracy: 0.9101 - val_loss: 0.6152 - val_accuracy: 0.8726\n",
      "Epoch 55/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4809 - accuracy: 0.9094 - val_loss: 0.6545 - val_accuracy: 0.8644\n",
      "Epoch 56/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4763 - accuracy: 0.9127 - val_loss: 0.6565 - val_accuracy: 0.8611\n",
      "Epoch 57/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4804 - accuracy: 0.9096 - val_loss: 0.5702 - val_accuracy: 0.8897\n",
      "Epoch 58/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4780 - accuracy: 0.9102 - val_loss: 0.6075 - val_accuracy: 0.8762\n",
      "Epoch 59/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4724 - accuracy: 0.9130 - val_loss: 0.6147 - val_accuracy: 0.8729\n",
      "Epoch 60/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4745 - accuracy: 0.9125 - val_loss: 0.6031 - val_accuracy: 0.8782\n",
      "Epoch 61/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4688 - accuracy: 0.9138 - val_loss: 0.5611 - val_accuracy: 0.8894\n",
      "Epoch 62/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4674 - accuracy: 0.9142 - val_loss: 0.5665 - val_accuracy: 0.8826\n",
      "Epoch 63/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4681 - accuracy: 0.9136 - val_loss: 0.5835 - val_accuracy: 0.8796\n",
      "Epoch 64/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4657 - accuracy: 0.9131 - val_loss: 0.5606 - val_accuracy: 0.8861\n",
      "Epoch 65/100\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.4642 - accuracy: 0.9140 - val_loss: 0.6138 - val_accuracy: 0.8751\n",
      "Epoch 66/100\n",
      "781/781 [==============================] - 66s 85ms/step - loss: 0.4683 - accuracy: 0.9132 - val_loss: 0.6127 - val_accuracy: 0.8745\n",
      "Epoch 67/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4597 - accuracy: 0.9165 - val_loss: 0.6007 - val_accuracy: 0.8800\n",
      "Epoch 68/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4616 - accuracy: 0.9160 - val_loss: 0.6564 - val_accuracy: 0.8604\n",
      "Epoch 69/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4597 - accuracy: 0.9159 - val_loss: 0.6217 - val_accuracy: 0.8721\n",
      "Epoch 70/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4589 - accuracy: 0.9171 - val_loss: 0.5594 - val_accuracy: 0.8876\n",
      "Epoch 71/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4526 - accuracy: 0.9203 - val_loss: 0.5744 - val_accuracy: 0.8855\n",
      "Epoch 72/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4537 - accuracy: 0.9183 - val_loss: 0.6018 - val_accuracy: 0.8805\n",
      "Epoch 73/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4542 - accuracy: 0.9176 - val_loss: 0.5816 - val_accuracy: 0.8797\n",
      "Epoch 74/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4520 - accuracy: 0.9193 - val_loss: 0.6845 - val_accuracy: 0.8525\n",
      "Epoch 75/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4525 - accuracy: 0.9182 - val_loss: 0.6079 - val_accuracy: 0.8755\n",
      "Epoch 76/100\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.4510 - accuracy: 0.9187 - val_loss: 0.5447 - val_accuracy: 0.8959\n",
      "Epoch 77/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4480 - accuracy: 0.9197 - val_loss: 0.5912 - val_accuracy: 0.8798\n",
      "Epoch 78/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4447 - accuracy: 0.9214 - val_loss: 0.5715 - val_accuracy: 0.8895\n",
      "Epoch 79/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4422 - accuracy: 0.9207 - val_loss: 0.6224 - val_accuracy: 0.8756\n",
      "Epoch 80/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4518 - accuracy: 0.9179 - val_loss: 0.5748 - val_accuracy: 0.8809\n",
      "Epoch 81/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4417 - accuracy: 0.9202 - val_loss: 0.5970 - val_accuracy: 0.8765\n",
      "Epoch 82/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4426 - accuracy: 0.9217 - val_loss: 0.6120 - val_accuracy: 0.8773\n",
      "Epoch 83/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4444 - accuracy: 0.9208 - val_loss: 0.6183 - val_accuracy: 0.8769\n",
      "Epoch 84/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4391 - accuracy: 0.9202 - val_loss: 0.7763 - val_accuracy: 0.8390\n",
      "Epoch 85/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4379 - accuracy: 0.9218 - val_loss: 0.5733 - val_accuracy: 0.8865\n",
      "Epoch 86/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4385 - accuracy: 0.9221 - val_loss: 0.5416 - val_accuracy: 0.8926\n",
      "Epoch 87/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4370 - accuracy: 0.9217 - val_loss: 0.5819 - val_accuracy: 0.8844\n",
      "Epoch 88/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4381 - accuracy: 0.9216 - val_loss: 0.5974 - val_accuracy: 0.8803\n",
      "Epoch 89/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4373 - accuracy: 0.9227 - val_loss: 0.5624 - val_accuracy: 0.8869\n",
      "Epoch 90/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4327 - accuracy: 0.9239 - val_loss: 0.5695 - val_accuracy: 0.8845\n",
      "Epoch 91/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4311 - accuracy: 0.9229 - val_loss: 0.5924 - val_accuracy: 0.8797\n",
      "Epoch 92/100\n",
      "781/781 [==============================] - 67s 86ms/step - loss: 0.4298 - accuracy: 0.9241 - val_loss: 0.5492 - val_accuracy: 0.8928\n",
      "Epoch 93/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4247 - accuracy: 0.9257 - val_loss: 0.5546 - val_accuracy: 0.8878\n",
      "Epoch 94/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4307 - accuracy: 0.9228 - val_loss: 0.5753 - val_accuracy: 0.8871\n",
      "Epoch 95/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4289 - accuracy: 0.9242 - val_loss: 0.5417 - val_accuracy: 0.8913\n",
      "Epoch 96/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4281 - accuracy: 0.9226 - val_loss: 0.5654 - val_accuracy: 0.8886\n",
      "Epoch 97/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4269 - accuracy: 0.9251 - val_loss: 0.5671 - val_accuracy: 0.8858\n",
      "Epoch 98/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4246 - accuracy: 0.9256 - val_loss: 0.5907 - val_accuracy: 0.8817\n",
      "Epoch 99/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4269 - accuracy: 0.9245 - val_loss: 0.5637 - val_accuracy: 0.8903\n",
      "Epoch 100/100\n",
      "781/781 [==============================] - 66s 84ms/step - loss: 0.4244 - accuracy: 0.9254 - val_loss: 0.5350 - val_accuracy: 0.8941\n",
      "INFO:tensorflow:Assets written to: models2\\model_4.model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "numberOfModels = 2\n",
    "epochs = 100\n",
    "\n",
    "opt = Adam()\n",
    "\n",
    "for i in range(numberOfModels):\n",
    "    print('GoogLeNet', i, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = MiniGoogLeNet.build(width = 32, height = 32, depth = 3, classes = 10)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "                  steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "for i in range(numberOfModels, 2*numberOfModels+1):\n",
    "    print('ResNet', i, 'is being trained...')\n",
    "    \n",
    "    # compile the model\n",
    "    model = ResNet.build(32, 32, 3, 10, (9, 9, 9), (64, 64, 128, 256), reg=0.0005)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])\n",
    "    \n",
    "    # train the model\n",
    "    H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "                  steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "    # save the model\n",
    "    p = ['models2', 'model_{}.model'.format(i)]\n",
    "    model.save(os.path.sep.join(p))\n",
    "    \n",
    "    # evaluate the network\n",
    "    predictions = model.predict(testX, batch_size=64)\n",
    "    report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)\n",
    "    \n",
    "    # save the classification report to file\n",
    "    p = ['output2', 'model_{}.txt'.format(i)]\n",
    "    f = open(os.path.sep.join(p), 'w')\n",
    "    f.write(report)\n",
    "    f.close()\n",
    "    \n",
    "    # plot the training loss and accuracy\n",
    "    p = ['output2', 'model_{}.png'.format(i)]\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(0, epochs), H.history['loss'], label = 'train_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_loss'], label = 'val_loss')\n",
    "    plt.plot(np.arange(0, epochs), H.history['accuracy'], label = 'train_acc')\n",
    "    plt.plot(np.arange(0, epochs), H.history['val_accuracy'], label = 'val_acc')\n",
    "    \n",
    "    # add labels and legend\n",
    "    plt.title('Training Loss and Accuracy for model {}'.format(i))\n",
    "    plt.xlabel('Epoch #')\n",
    "    plt.ylabel('Loss/Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # save graphs\n",
    "    plt.savefig(os.path.sep.join(p))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model 1/5\n",
      "Loading model 2/5\n",
      "Loading model 3/5\n",
      "Loading model 4/5\n",
      "Loading model 5/5\n",
      "Evaluating ensemble...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    airplane       0.94      0.95      0.95      1000\n",
      "  automobile       0.95      0.98      0.97      1000\n",
      "        bird       0.91      0.90      0.91      1000\n",
      "         cat       0.85      0.86      0.86      1000\n",
      "        deer       0.93      0.93      0.93      1000\n",
      "         dog       0.92      0.85      0.88      1000\n",
      "        frog       0.90      0.98      0.94      1000\n",
      "       horse       0.97      0.95      0.96      1000\n",
      "        ship       0.97      0.96      0.96      1000\n",
      "       truck       0.97      0.95      0.96      1000\n",
      "\n",
      "    accuracy                           0.93     10000\n",
      "   macro avg       0.93      0.93      0.93     10000\n",
      "weighted avg       0.93      0.93      0.93     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['models2', '*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size = 64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis = 0)\n",
    "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snapshot Ensembles\n",
    "\n",
    "Snapshot ensembles (Huang, et. al., 2017) train a single net in such a way that it repeatedly converges to different local minima, save each state of the model, and use (a subset of) those saved states to build an ensemble. While there may be more risk of this one net landing in similar local minima when training one net compared to training several unrelated nets, it can be dramatically cheaper computationally to ensemble in this way.\n",
    "\n",
    "One way of accomplishing this is to use a cyclic learning rate that starts high and anneals to smaller values until the net settles, saves the state of the model, and then returns to a high learning rate to repeat the cycle. This way, it settles down to a state we will use in the ensemble, but then, a high learning rate will let it jump out of the local minimum it has hopefully reached and continue to a new one. This way, our learning algorithms explore more of the parameter space and capture multiple local minima.\n",
    "\n",
    "The immediate question may be, \"Why not just train a new model from a random initialization each time?\" But, what tends to happen is that most training time occurs when a net is trying to reach its first local minimum. Routing from there to a new one has been shown to be much cheaper under the right circumstances.\n",
    "\n",
    "To accomplish this, we simply need to write a learning rate scheduler that applies a cyclical learning rate of the form (Loschilov and Hutter, 2016):\n",
    "\n",
    "$$\\alpha(t) = f\\left(\\text{mod}\\left(t - 1, \\left\\lceil \\frac{T}{M}\\right\\rceil\\right)\\right),$$\n",
    "\n",
    "where $t$ is the iteration number, $T$ is the total number of training epochs, $M$ is the number of snapshots we will capture, and $f$ is a monotonically decreasing function. So here, we specify how many epochs to train and the number of snapshots we want to take, and then the learning rate cycles every $\\frac{\\text{number of training epochs}}{\\text{number of snapshots}}$ number of epochs.\n",
    "\n",
    "Loschilov and Hutter proposed a shifted cosine function of the form\n",
    "\n",
    "$$\\alpha(t)=\\frac{\\alpha_0}{2}\\left(\\cos\\left(\\frac{\\pi\\text{ mod}(t-1,\\lceil T/M\\rceil)}{\\lceil T/M \\rceil}\\right)+1\\right),$$\n",
    "\n",
    "where $\\alpha_0$ is the initial learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# load cifar10 data\n",
    "((trainX, trainY), (testX, testY)) = cifar10.load_data()\n",
    "labelNames = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", 'dog', \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "\n",
    "# preprocess data\n",
    "trainX = trainX.astype('float')/255.0\n",
    "testX = testX.astype('float')/255.0\n",
    "trainY = to_categorical(trainY, 10)\n",
    "testY = to_categorical(testY, 10)\n",
    "\n",
    "# create an image generator for data augmentation with random shifting, rotation, and horizontal flips\n",
    "aug = ImageDataGenerator(rotation_range = 10, width_shift_range = 0.1, height_shift_range = 0.1, horizontal_flip = True, fill_mode = 'nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n",
      "Train for 781 steps, validate on 10000 samples\n",
      "Epoch 1/60\n",
      " 83/781 [==>...........................] - ETA: 6:16 - loss: 3.3040 - accuracy: 0.1431"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-11aa72b02dd5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model{epoch:08d}.model'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n\u001b[1;32m---> 30\u001b[1;33m               callbacks = callbacks, steps_per_epoch = len(trainX) // 64, verbose = 1)\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# evaluate the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    122\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 86\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    488\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1823\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1825\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DL\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "models = 3\n",
    "initialLearningRate = 0.2\n",
    "epochs = 60\n",
    "\n",
    "# code for a learning rate scheduler\n",
    "def shiftedCosineLearning(epoch):\n",
    "    maxEpochs = epochs\n",
    "    baseLearningRate = initialLearningRate\n",
    "\n",
    "    alpha = (initialLearningRate/2)*(np.cos(np.pi*np.mod(epoch - 1, np.ceil(epochs/models))/np.ceil(epochs/models)) + 1)\n",
    "    \n",
    "    # return the learning rate\n",
    "    return alpha\n",
    "\n",
    "callbacks = [LearningRateScheduler(shiftedCosineLearning)]\n",
    "    \n",
    "# choose the optimizer\n",
    "opt = SGD(lr = initialLearningRate)\n",
    "#pt = Adam()\n",
    "    \n",
    "# compile the model\n",
    "model = MiniVGGNet.build(32, 32, 3, 10)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "# train the model\n",
    "checkpoint = ModelCheckpoint('model{epoch:08d}.model', period=np.ceil(epochs/models)) \n",
    "H = model.fit(aug.flow(trainX, trainY, batch_size = 64), validation_data = (testX, testY), epochs = epochs,\n",
    "              callbacks = callbacks, steps_per_epoch = len(trainX) // 64, verbose = 1)\n",
    "    \n",
    "# evaluate the network\n",
    "predictions = model.predict(testX, batch_size=64)\n",
    "report = classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we test the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ensemble...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\lib\\function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "C:\\Users\\Ryan\\anaconda3\\envs\\DL\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-110d063a80b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# a classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabelNames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'testY' is not defined"
     ]
    }
   ],
   "source": [
    "# THERE IS SOME PREPROCESSING ERROR HERE IN THE LAST STEP...\n",
    "\n",
    "# construct the path used to collect the models then initialize the\n",
    "# models list\n",
    "modelPaths = os.path.sep.join(['*.model'])\n",
    "modelPaths = list(glob.glob(modelPaths))\n",
    "models = []\n",
    "\n",
    "# loop over the model paths, loading the model, and adding it to\n",
    "# the list of models\n",
    "for (i, modelPath) in enumerate(modelPaths):\n",
    "\tprint('Loading model {}/{}'.format(i + 1, len(modelPaths)))\n",
    "\tmodels.append(load_model(modelPath))\n",
    "\n",
    "# initialize the list of predictions\n",
    "print('Evaluating ensemble...')\n",
    "predictions = []\n",
    "\n",
    "# loop over the models\n",
    "for model in models:\n",
    "\t# use the current model to make predictions on the testing data,\n",
    "\t# then store these predictions in the aggregate predictions list\n",
    "\tpredictions.append(model.predict(testX, batch_size=64))\n",
    "\n",
    "# average the probabilities across all model predictions, then show\n",
    "# a classification report\n",
    "predictions = np.average(predictions, axis=0)\n",
    "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=labelNames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Transfer learning is where we take nets pre-trained on huge datasets like ImageNet, load the weights, and use those as starting points for training on a new dataset. The idea is that knowledge learning about one dataset, if it is somewhat related to your dataset, can *transfer* to knowledge of your dataset with only partial training.\n",
    "\n",
    "For example, a neural net that is effective at classifying words spoken in English might be effective for learning words spoken in Spanish without totally starting from scratch with training and hyperparameter tuning. After all, both languages have roughly the same alphabet and many similarities in pronunciation of letters.\n",
    "\n",
    "One approach takes a pre-trained convolutional neural net containing all the parameters successful at classifying the intended dataset, remove the fully-connected layers at the end, and feed your *different* dataset through the net to the end of the last pooling layer. Then, treat the outputs from each input as a new dataset that has been preprocessed by ths pre-trained CNN. Lastly, apply another classifier to this dataset.\n",
    "\n",
    "Another common approach is to take the same kind of pre-trained CNN and re-initialize the weights of the fully-connected layers at the end. Then, \"freeze\" all the parameters before the fully-connected layers. Last, train the parameters of the last few layers with a small learning rate on the new dataset, but with one caveat: as backpropagation moves backward through the network determining weight updates, it stops when it reaches the frozen layers and does not adjust those weights at all.\n",
    "\n",
    "Let's learn how to implement both approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDF5\n",
    "\n",
    "If we want to work with huge pre-trained neural nets like VGG19 or other deep CNNs, storing them takes far more space than our RAM is likely to support, so we need to store them on HDD/SDDs in an efficient way. Keras's model format is pretty large, but HDF5 is a good data format for this, but we need some code to be able to interface with this format, which we write below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5DatasetWriter:\n",
    "    def __init__(self, dims, outputPath, dataKey = 'images', bufferSize = 1000):\n",
    "        # check if outputpath exists\n",
    "        if os.path.exists(outputPath):\n",
    "            raise ValueError('The supplied `outputPath` already exists and cannot be overwritten.'\n",
    "                            'Delete the file manually before continuing.', outputPath)\n",
    "            \n",
    "        # open the HDF5 database for writing and create two datasets: one to store the\n",
    "        # images/features and one to store the labels\n",
    "        self.db = h5py.File(outputPath, 'w')\n",
    "        self.data = self.db.create_dataset(dataKey, dims, dtype = 'float')\n",
    "        self.labels = self.db.create_dataset('labels', (dims[0],), dtype = 'float')\n",
    "        \n",
    "        # store the buffer size and initialize the buffer and index\n",
    "        self.bufferSize = bufferSize\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        self.index = 0\n",
    "        \n",
    "    def add(self, rows, labels):\n",
    "        # add the rows and labels to the buffer\n",
    "        self.buffer['data'].extend(rows)\n",
    "        self.buffer['labels'].extend(labels)\n",
    "        \n",
    "        # check if the buffer needs to be flushed to disk\n",
    "        if len(self.buffer['data']) >= self.bufferSize:\n",
    "            self.flush()\n",
    "            \n",
    "    def flush(self):\n",
    "        # write the buffer to disk and reset buffer\n",
    "        i = self.index + len(self.buffer['data'])\n",
    "        self.data[self.index:i] = self.buffer['data']\n",
    "        self.labels[self.index:i] = self.buffer['labels']\n",
    "        \n",
    "        self.index = i\n",
    "        self.buffer = {'data': [], 'labels': []}\n",
    "        \n",
    "    def storeClassLabels(self, classLabels):\n",
    "        # create a dataset to store class label names, then store them\n",
    "        dt = h5py.special_dtype(vlen = str)\n",
    "        labelSet = self.db.create_dataset('label_names', (len(classLabels),), dtype = dt)\n",
    "        labelSet[:] = classLabels\n",
    "        \n",
    "    def close(self):\n",
    "        # flush entries to disk if needed\n",
    "        if len(self.buffer['data']) > 0:\n",
    "            self.flush()\n",
    "            \n",
    "        # close the dataset\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "Let's write some code to extract features from an arbitrary image dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeatures(batch_size, dataset, output, buffer_size = 1000):\n",
    "\n",
    "    bs = batch_size\n",
    "    \n",
    "    # grab the list of images that we'll be describing then randomly\n",
    "    # shuffle them to allow for easy training and testing splits via\n",
    "    # array slicing during training time\n",
    "    print(\"[INFO] loading images...\")\n",
    "    imagePaths = list(paths.list_images(dataset))\n",
    "    random.shuffle(imagePaths)\n",
    "\n",
    "    # extract the class labels from the image paths then encode the\n",
    "    # labels\n",
    "    labels = [p.split(os.path.sep)[-2] for p in imagePaths]\n",
    "    le = LabelEncoder()\n",
    "    labels = le.fit_transform(labels)\n",
    "\n",
    "    # load the VGG16 network\n",
    "    print(\"[INFO] loading network...\")\n",
    "    model = VGG16(weights=\"imagenet\", include_top=False)\n",
    "\n",
    "    # initialize the HDF5 dataset writer, then store the class label\n",
    "    # names in the dataset\n",
    "    dataset = HDF5DatasetWriter((len(imagePaths), 512 * 7 * 7),\n",
    "        output, dataKey=\"features\", bufferSize=buffer_size)\n",
    "    dataset.storeClassLabels(le.classes_)\n",
    "\n",
    "    # initialize the progress bar\n",
    "    widgets = [\"Extracting Features: \", progressbar.Percentage(), \" \",\n",
    "               progressbar.Bar(), \" \", progressbar.ETA()]\n",
    "    pbar = progressbar.ProgressBar(maxval=len(imagePaths), widgets=widgets).start()\n",
    "\n",
    "    # loop over the images in batches\n",
    "    for i in np.arange(0, len(imagePaths), bs):\n",
    "        # extract the batch of images and labels, then initialize the\n",
    "        # list of actual images that will be passed through the network\n",
    "        # for feature extraction\n",
    "        batchPaths = imagePaths[i:i + bs]\n",
    "        batchLabels = labels[i:i + bs]\n",
    "        batchImages = []\n",
    "\n",
    "        # loop over the images and labels in the current batch\n",
    "        for (j, imagePath) in enumerate(batchPaths):\n",
    "            # load the input image using the Keras helper utility\n",
    "            # while ensuring the image is resized to 224x224 pixels\n",
    "            image = load_img(imagePath, target_size=(224, 224))\n",
    "            image = img_to_array(image)\n",
    "\n",
    "            # preprocess the image by (1) expanding the dimensions and\n",
    "            # (2) subtracting the mean RGB pixel intensity from the\n",
    "            # ImageNet dataset\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = imagenet_utils.preprocess_input(image)\n",
    "\n",
    "            # add the image to the batch\n",
    "            batchImages.append(image)\n",
    "\n",
    "        # pass the images through the network and use the outputs as\n",
    "        # our actual features\n",
    "        batchImages = np.vstack(batchImages)\n",
    "        features = model.predict(batchImages, batch_size=bs)\n",
    "\n",
    "        # reshape the features so that each image is represented by\n",
    "        # a flattened feature vector of the `MaxPooling2D` outputs\n",
    "        features = features.reshape((features.shape[0], 512 * 7 * 7))\n",
    "\n",
    "        # add the features and labels to our HDF5 dataset\n",
    "        dataset.add(features, batchLabels)\n",
    "        pbar.update(i)\n",
    "\n",
    "    # close the dataset\n",
    "    dataset.close()\n",
    "    pbar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:32\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/animals/images', '../datasets/animals/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:01:37\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/caltech-101/images', '../datasets/caltech-101/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO] loading network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100% |#####################################| Time: 0:00:16\n"
     ]
    }
   ],
   "source": [
    "extractFeatures(32, '../datasets/flowers17/images', '../datasets/flowers17/hdf5/features.hdf5', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Classifier on Extracted Features\n",
    "\n",
    "We have used a VGG16 net pre-trained on ImageNet and used it to extract features from three *different* datasets. We will now train a simple classifier on this new dataset of features extracted by the VGG16 net and see if the learning can actually be transferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transferClassify(dbPath, modelPath, jobs = -1):\n",
    "    # open HDF5\n",
    "    db = h5py.File(dbPath, 'r')\n",
    "    \n",
    "    # 75% to training (recall, we shuffled the data before writing to HDF5) is\n",
    "    # before index i\n",
    "    i = int(db['labels'].shape[0] * 0.75)\n",
    "    \n",
    "    # define parameters we want to tune\n",
    "    parameters = {'C': [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]}\n",
    "    \n",
    "    # evaluate the model at each value of C\n",
    "    model = GridSearchCV(LogisticRegression(solver = 'lbfgs', multi_class = 'auto'),\n",
    "                        parameters, cv = 3, n_jobs = jobs)\n",
    "    \n",
    "    # fit the model to the training set\n",
    "    model.fit(db['features'][:i], db['labels'][:i])\n",
    "    print('Best hyperparameters: {}'.format(model.best_params_))\n",
    "    \n",
    "    # evaluate the model\n",
    "    print('Evaluating...')\n",
    "    # predict on the testing set\n",
    "    predictions = model.predict(db['features'][i:])\n",
    "    print(classification_report(db['labels'][i:], predictions,\n",
    "                                target_names = db['label_names']))\n",
    "    \n",
    "    # save the model\n",
    "    print('Saving model...')\n",
    "    f = open(modelPath, 'wb')\n",
    "    f.write(pickle.dumps(model.best_estimator_))\n",
    "    f.close()\n",
    "    \n",
    "    # cloes the database\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 0.1}\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.97      0.98      0.98       255\n",
      "        dogs       0.99      0.98      0.98       247\n",
      "       panda       1.00      0.99      0.99       248\n",
      "\n",
      "    accuracy                           0.98       750\n",
      "   macro avg       0.98      0.98      0.98       750\n",
      "weighted avg       0.98      0.98      0.98       750\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../datasets/animals/hdf5/features.hdf5', 'animals.cpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ryan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 0.1}\n",
      "Evaluating...\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "          Faces       0.96      0.98      0.97       114\n",
      "     Faces_easy       0.98      0.98      0.98       121\n",
      "       Leopards       0.98      1.00      0.99        42\n",
      "     Motorbikes       1.00      1.00      1.00       202\n",
      "      accordion       1.00      1.00      1.00        15\n",
      "      airplanes       1.00      0.99      1.00       209\n",
      "         anchor       1.00      0.86      0.92        14\n",
      "            ant       0.67      0.50      0.57         8\n",
      "         barrel       0.89      1.00      0.94         8\n",
      "           bass       0.84      0.89      0.86        18\n",
      "         beaver       0.83      0.71      0.77         7\n",
      "      binocular       0.89      0.89      0.89         9\n",
      "         bonsai       0.91      1.00      0.96        32\n",
      "          brain       0.96      0.93      0.95        29\n",
      "   brontosaurus       0.91      0.77      0.83        13\n",
      "         buddha       1.00      1.00      1.00        19\n",
      "      butterfly       0.96      0.92      0.94        24\n",
      "         camera       1.00      0.92      0.96        13\n",
      "         cannon       1.00      0.82      0.90        11\n",
      "       car_side       1.00      1.00      1.00        25\n",
      "    ceiling_fan       0.93      0.87      0.90        15\n",
      "      cellphone       0.90      1.00      0.95         9\n",
      "          chair       0.93      0.88      0.90        16\n",
      "     chandelier       0.97      1.00      0.98        29\n",
      "    cougar_body       0.85      0.79      0.81        14\n",
      "    cougar_face       0.91      0.95      0.93        21\n",
      "           crab       0.75      0.90      0.82        20\n",
      "       crayfish       0.71      0.94      0.81        16\n",
      "      crocodile       1.00      0.86      0.92        14\n",
      " crocodile_head       0.83      1.00      0.91        10\n",
      "            cup       1.00      0.82      0.90        17\n",
      "      dalmatian       1.00      1.00      1.00        21\n",
      "    dollar_bill       0.89      0.89      0.89         9\n",
      "        dolphin       0.87      1.00      0.93        13\n",
      "      dragonfly       0.95      0.95      0.95        20\n",
      "electric_guitar       1.00      1.00      1.00        15\n",
      "       elephant       0.93      0.93      0.93        14\n",
      "            emu       1.00      0.93      0.97        15\n",
      "      euphonium       1.00      1.00      1.00        18\n",
      "           ewer       1.00      1.00      1.00        20\n",
      "          ferry       0.95      1.00      0.98        21\n",
      "       flamingo       0.93      0.87      0.90        15\n",
      "  flamingo_head       1.00      1.00      1.00         8\n",
      "       garfield       0.88      1.00      0.93         7\n",
      "        gerenuk       1.00      0.62      0.76        13\n",
      "     gramophone       0.89      1.00      0.94         8\n",
      "    grand_piano       1.00      1.00      1.00        29\n",
      "      hawksbill       0.89      1.00      0.94        31\n",
      "      headphone       0.93      1.00      0.96        13\n",
      "       hedgehog       0.93      1.00      0.96        13\n",
      "     helicopter       0.96      0.96      0.96        23\n",
      "           ibis       0.95      0.95      0.95        20\n",
      "   inline_skate       0.83      1.00      0.91        10\n",
      "    joshua_tree       0.93      1.00      0.97        14\n",
      "       kangaroo       1.00      0.95      0.97        19\n",
      "          ketch       0.94      0.71      0.81        24\n",
      "           lamp       0.93      1.00      0.97        14\n",
      "         laptop       1.00      1.00      1.00        19\n",
      "          llama       0.83      1.00      0.91        15\n",
      "        lobster       1.00      0.64      0.78        14\n",
      "          lotus       0.80      0.71      0.75        17\n",
      "       mandolin       1.00      0.90      0.95        10\n",
      "         mayfly       0.73      0.89      0.80         9\n",
      "        menorah       1.00      0.94      0.97        16\n",
      "      metronome       1.00      1.00      1.00         9\n",
      "        minaret       1.00      1.00      1.00        26\n",
      "       nautilus       1.00      0.94      0.97        18\n",
      "        octopus       0.78      0.88      0.82         8\n",
      "          okapi       1.00      1.00      1.00         8\n",
      "         pagoda       1.00      1.00      1.00         9\n",
      "          panda       1.00      1.00      1.00         7\n",
      "         pigeon       0.93      0.93      0.93        14\n",
      "          pizza       1.00      0.94      0.97        17\n",
      "       platypus       0.86      0.46      0.60        13\n",
      "        pyramid       1.00      1.00      1.00        12\n",
      "       revolver       1.00      0.93      0.96        14\n",
      "          rhino       1.00      0.86      0.92        14\n",
      "        rooster       1.00      0.78      0.88         9\n",
      "      saxophone       1.00      1.00      1.00         9\n",
      "       schooner       0.65      1.00      0.79        13\n",
      "       scissors       0.75      1.00      0.86         6\n",
      "       scorpion       0.88      0.97      0.92        29\n",
      "      sea_horse       0.84      0.89      0.86        18\n",
      "         snoopy       1.00      0.78      0.88         9\n",
      "    soccer_ball       1.00      1.00      1.00        12\n",
      "        stapler       1.00      0.79      0.88        14\n",
      "       starfish       0.95      1.00      0.97        19\n",
      "    stegosaurus       0.75      1.00      0.86        12\n",
      "      stop_sign       1.00      1.00      1.00        13\n",
      "     strawberry       0.73      1.00      0.84         8\n",
      "      sunflower       0.95      0.86      0.90        22\n",
      "           tick       1.00      1.00      1.00        10\n",
      "      trilobite       1.00      1.00      1.00        13\n",
      "       umbrella       0.94      0.94      0.94        17\n",
      "          watch       1.00      0.98      0.99        58\n",
      "    water_lilly       0.55      0.75      0.63         8\n",
      "     wheelchair       1.00      0.94      0.97        17\n",
      "       wild_cat       0.75      1.00      0.86         6\n",
      "  windsor_chair       0.92      0.92      0.92        13\n",
      "         wrench       0.92      0.73      0.81        15\n",
      "       yin_yang       0.89      0.80      0.84        10\n",
      "\n",
      "       accuracy                           0.95      2170\n",
      "      macro avg       0.93      0.92      0.92      2170\n",
      "   weighted avg       0.95      0.95      0.95      2170\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../datasets/caltech-101/hdf5/features.hdf5', 'caltech101.cpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 100.0}\n",
      "Evaluating...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.83      0.95      0.89        21\n",
      "   buttercup       1.00      0.91      0.95        22\n",
      "   coltsfoot       1.00      0.88      0.94        26\n",
      "     cowslip       0.62      0.90      0.73        20\n",
      "      crocus       0.83      0.94      0.88        16\n",
      "    daffodil       0.95      0.87      0.91        23\n",
      "       daisy       1.00      0.93      0.96        28\n",
      "   dandelion       0.93      1.00      0.97        14\n",
      "  fritillary       0.95      0.95      0.95        20\n",
      "        iris       1.00      0.78      0.88        27\n",
      "  lilyvalley       0.89      0.94      0.92        18\n",
      "       pansy       0.94      0.89      0.92        19\n",
      "    snowdrop       0.86      0.80      0.83        15\n",
      "   sunflower       1.00      1.00      1.00        19\n",
      "   tigerlily       1.00      1.00      1.00        18\n",
      "       tulip       0.88      0.88      0.88        17\n",
      "  windflower       0.89      0.94      0.91        17\n",
      "\n",
      "    accuracy                           0.91       340\n",
      "   macro avg       0.92      0.92      0.91       340\n",
      "weighted avg       0.92      0.91      0.91       340\n",
      "\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "transferClassify('../datasets/flowers17/hdf5/features.hdf5', 'flowers17.cpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning By Fine-Tuning\n",
    "\n",
    "To operate on individual layers, we need to determine how to access them with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printLayerNames(include_top = True):\n",
    "    # load VGG16 pre-trained on ImageNet\n",
    "    print('Loading network...')\n",
    "    model = VGG16(weights = 'imagenet', include_top = include_top)\n",
    "    \n",
    "    # loop over the layers and display them\n",
    "    for (i, layer) in enumerate(model.layers):\n",
    "        print('{}\\t{}'.format(i, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer names with the head\n",
      "Loading network...\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n",
      "19\tFlatten\n",
      "20\tDense\n",
      "21\tDense\n",
      "22\tDense\n",
      "\n",
      "Layer names without the head\n",
      "Loading network...\n",
      "0\tInputLayer\n",
      "1\tConv2D\n",
      "2\tConv2D\n",
      "3\tMaxPooling2D\n",
      "4\tConv2D\n",
      "5\tConv2D\n",
      "6\tMaxPooling2D\n",
      "7\tConv2D\n",
      "8\tConv2D\n",
      "9\tConv2D\n",
      "10\tMaxPooling2D\n",
      "11\tConv2D\n",
      "12\tConv2D\n",
      "13\tConv2D\n",
      "14\tMaxPooling2D\n",
      "15\tConv2D\n",
      "16\tConv2D\n",
      "17\tConv2D\n",
      "18\tMaxPooling2D\n"
     ]
    }
   ],
   "source": [
    "print('Layer names with the head')\n",
    "printLayerNames()\n",
    "\n",
    "print('\\nLayer names without the head')\n",
    "printLayerNames(include_top = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the first 18 layers are the same, but the full network, including the top of the net, i.e. the fully connected layers. This \"top\" is also known as the \"head\".\n",
    "\n",
    "### Network Surgery\n",
    "\n",
    "Once we've chopped off the head of the net, we need to replace it with a newly initialized head so we can train it and attempt to transfer the knowledge of the lower layers of the net to a new dataset by retraining this new head.\n",
    "\n",
    "Let's create a net to go at the head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCHeadNet:\n",
    "    def build(baseModel, classes, D):\n",
    "        # initialize the head model and add a fully-connected layer\n",
    "        headModel = baseModel.output\n",
    "        headModel = Flatten(name = 'flatten')(headModel)\n",
    "        headModel = Dense(D, activation = 'relu')(headModel)\n",
    "        headModel = Dropout(0.5)(headModel)\n",
    "        \n",
    "        # add a softmax layer\n",
    "        headModel = Dense(classes, activation = 'softmax')(headModel)\n",
    "        \n",
    "        # return the model\n",
    "        return headModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Preprocessing Code\n",
    "\n",
    "This code comes from Adrian Rosebrock's *Deep Learning for Computer Vision* book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset loader that applies specified preprocessors\n",
    "class SimpleDatasetLoader:\n",
    "\tdef __init__(self, preprocessors=None):\n",
    "\t\t# store the image preprocessor\n",
    "\t\tself.preprocessors = preprocessors\n",
    "\n",
    "\t\t# if the preprocessors are None, initialize them as an\n",
    "\t\t# empty list\n",
    "\t\tif self.preprocessors is None:\n",
    "\t\t\tself.preprocessors = []\n",
    "\n",
    "\tdef load(self, imagePaths, verbose=-1):\n",
    "\t\t# initialize the list of features and labels\n",
    "\t\tdata = []\n",
    "\t\tlabels = []\n",
    "\n",
    "\t\t# loop over the input images\n",
    "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
    "\t\t\t# load the image and extract the class label assuming\n",
    "\t\t\t# that our path has the following format:\n",
    "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
    "\t\t\timage = cv2.imread(imagePath)\n",
    "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "\t\t\t# check to see if our preprocessors are not None\n",
    "\t\t\tif self.preprocessors is not None:\n",
    "\t\t\t\t# loop over the preprocessors and apply each to\n",
    "\t\t\t\t# the image\n",
    "\t\t\t\tfor p in self.preprocessors:\n",
    "\t\t\t\t\timage = p.preprocess(image)\n",
    "\n",
    "\t\t\t# treat our processed image as a \"feature vector\"\n",
    "\t\t\t# by updating the data list followed by the labels\n",
    "\t\t\tdata.append(image)\n",
    "\t\t\tlabels.append(label)\n",
    "\n",
    "\t\t\t# show an update every `verbose` images\n",
    "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
    "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
    "\t\t\t\t\tlen(imagePaths)))\n",
    "\n",
    "\t\t# return a tuple of the data and labels\n",
    "\t\treturn (np.array(data), np.array(labels))\n",
    "\n",
    "# resize images while maintaining aspect ratio\n",
    "class AspectAwarePreprocessor:\n",
    "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
    "\t\t# store the target image width, height, and interpolation\n",
    "\t\t# method used when resizing\n",
    "\t\tself.width = width\n",
    "\t\tself.height = height\n",
    "\t\tself.inter = inter\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# grab the dimensions of the image and then initialize\n",
    "\t\t# the deltas to use when cropping\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\tdW = 0\n",
    "\t\tdH = 0\n",
    "\n",
    "\t\t# if the width is smaller than the height, then resize\n",
    "\t\t# along the width (i.e., the smaller dimension) and then\n",
    "\t\t# update the deltas to crop the height to the desired\n",
    "\t\t# dimension\n",
    "\t\tif w < h:\n",
    "\t\t\timage = imutils.resize(image, width=self.width,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
    "\n",
    "\t\t# otherwise, the height is smaller than the width so\n",
    "\t\t# resize along the height and then update the deltas\n",
    "\t\t# crop along the width\n",
    "\t\telse:\n",
    "\t\t\timage = imutils.resize(image, height=self.height,\n",
    "\t\t\t\tinter=self.inter)\n",
    "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
    "\n",
    "\t\t# now that our images have been resized, we need to\n",
    "\t\t# re-grab the width and height, followed by performing\n",
    "\t\t# the crop\n",
    "\t\t(h, w) = image.shape[:2]\n",
    "\t\timage = image[dH:h - dH, dW:w - dW]\n",
    "\n",
    "\t\t# finally, resize the image to the provided spatial\n",
    "\t\t# dimensions to ensure our output image is always a fixed\n",
    "\t\t# size\n",
    "\t\treturn cv2.resize(image, (self.width, self.height),\n",
    "\t\t\tinterpolation=self.inter)\n",
    "\n",
    "# convert images to arrays\n",
    "class ImageToArrayPreprocessor:\n",
    "\tdef __init__(self, dataFormat=None):\n",
    "\t\t# store the image data format\n",
    "\t\tself.dataFormat = dataFormat\n",
    "\n",
    "\tdef preprocess(self, image):\n",
    "\t\t# apply the Keras utility function that correctly rearranges\n",
    "\t\t# the dimensions of the image\n",
    "\t\treturn img_to_array(image, data_format=self.dataFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning\n",
    "\n",
    "Now, let's try to implement some fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fineTune(datasetPath, modelPath):\n",
    "    # initialize an image generator for data augmentation\n",
    "    aug = ImageDataGenerator(rotation_range = 30, width_shift_range = 0.1, height_shift_range = 0.1,\n",
    "                             shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True,\n",
    "                             fill_mode = 'nearest')\n",
    "    \n",
    "    # get the list of image paths and extract class labels\n",
    "    imagePaths = list(paths.list_images(datasetPath))\n",
    "    classNames = [pt.split(os.path.sep)[-2] for pt in imagePaths]\n",
    "    classNames = [str(x) for x in np.unique(classNames)]\n",
    "    \n",
    "    # initialize image preprocessors\n",
    "    aap = AspectAwarePreprocessor(224, 224)\n",
    "    iap = ImageToArrayPreprocessor()\n",
    "    \n",
    "    # load dataset and scale pixel intensities to [0, 1]\n",
    "    sdl = SimpleDatasetLoader(preprocessors = [aap, iap])\n",
    "    (data, labels) = sdl.load(imagePaths, verbose = 500)\n",
    "    data = data.astype('float') / 255.0\n",
    "    \n",
    "    # partition into training/test splits\n",
    "    (trainX, testX, trainY, testY) = train_test_split(data, labels, test_size = 0.25)\n",
    "    \n",
    "    trainY = LabelBinarizer().fit_transform(trainY)\n",
    "    testY = LabelBinarizer().fit_transform(testY)\n",
    "    \n",
    "    # load VGG16 net without the head\n",
    "    baseModel = VGG16(weights = 'imagenet', include_top = False,\n",
    "                      input_tensor = Input(shape = (224, 224, 3)))\n",
    "    \n",
    "    # initialize the new head\n",
    "    headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
    "    \n",
    "    # build the model with the new head\n",
    "    model = Model(inputs = baseModel.input, outputs = headModel)\n",
    "    \n",
    "    # loop over the layers of the base model and \"freeze\" them so they are not\n",
    "    # updated during training\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # compile the model with a small learning rate\n",
    "    print('Compiling model...')\n",
    "    opt = RMSprop(lr = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the head of the network for a few iterations to initialize it a little better\n",
    "    # than randomly\n",
    "    print('Training head...')\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "              epochs = 25, steps_per_epoch = len(trainX) // 32, verbose = 1)\n",
    "    \n",
    "    # evaluate the net after this \"smart\" initialization\n",
    "    print('Evaluating after initialization...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                                target_names = classNames))\n",
    "    \n",
    "    # unfreeze the final set of convolutional layers\n",
    "    for layer in baseModel.layers[15:]:\n",
    "        layer.trainable = True\n",
    "        \n",
    "    # recompile the model\n",
    "    print('Re-compiling model...')\n",
    "    opt = SGD(lr = 0.001)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "    \n",
    "    # train the model again, fine-tuning the last few convolutional layers plus the head\n",
    "    model.fit(aug.flow(trainX, trainY, batch_size = 32), validation_data = (testX, testY),\n",
    "              epochs = 100, steps_per_epoch = len(trainX) // 32, verbose = 1)\n",
    "    \n",
    "    # evaluate the net\n",
    "    print('Evaluating after fine-tuning...')\n",
    "    predictions = model.predict(testX, batch_size = 32)\n",
    "    print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1),\n",
    "                                target_names = classNames))\n",
    "    \n",
    "    # save the model\n",
    "    print('Saving the model to disk...')\n",
    "    model.save(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 500/1360\n",
      "[INFO] processed 1000/1360\n",
      "Compiling model...\n",
      "Training head...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 31 steps, validate on 340 samples\n",
      "Epoch 1/25\n",
      "31/31 [==============================] - 51s 2s/step - loss: 4.6027 - accuracy: 0.1579 - val_loss: 1.9896 - val_accuracy: 0.3706\n",
      "Epoch 2/25\n",
      "31/31 [==============================] - 57s 2s/step - loss: 2.2506 - accuracy: 0.2844 - val_loss: 1.6239 - val_accuracy: 0.5294\n",
      "Epoch 3/25\n",
      "31/31 [==============================] - 59s 2s/step - loss: 2.0523 - accuracy: 0.3654 - val_loss: 1.3225 - val_accuracy: 0.6618\n",
      "Epoch 4/25\n",
      "31/31 [==============================] - 34s 1s/step - loss: 1.8100 - accuracy: 0.4190 - val_loss: 1.1798 - val_accuracy: 0.6882\n",
      "Epoch 5/25\n",
      "31/31 [==============================] - 35s 1s/step - loss: 1.5937 - accuracy: 0.4879 - val_loss: 0.9705 - val_accuracy: 0.7088\n",
      "Epoch 6/25\n",
      "31/31 [==============================] - 35s 1s/step - loss: 1.3919 - accuracy: 0.5405 - val_loss: 0.9297 - val_accuracy: 0.7147\n",
      "Epoch 7/25\n",
      "31/31 [==============================] - 34s 1s/step - loss: 1.3592 - accuracy: 0.5615 - val_loss: 0.7367 - val_accuracy: 0.8382\n",
      "Epoch 8/25\n",
      "31/31 [==============================] - 30s 978ms/step - loss: 1.2804 - accuracy: 0.5688 - val_loss: 0.8046 - val_accuracy: 0.7941\n",
      "Epoch 9/25\n",
      "31/31 [==============================] - 24s 785ms/step - loss: 1.1256 - accuracy: 0.6306 - val_loss: 0.5954 - val_accuracy: 0.8353\n",
      "Epoch 10/25\n",
      "31/31 [==============================] - 17s 558ms/step - loss: 1.2059 - accuracy: 0.6073 - val_loss: 0.6059 - val_accuracy: 0.8412\n",
      "Epoch 11/25\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 1.0168 - accuracy: 0.6407 - val_loss: 0.5775 - val_accuracy: 0.8353\n",
      "Epoch 12/25\n",
      "31/31 [==============================] - 11s 345ms/step - loss: 0.9964 - accuracy: 0.6711 - val_loss: 0.5646 - val_accuracy: 0.8412\n",
      "Epoch 13/25\n",
      "31/31 [==============================] - 12s 375ms/step - loss: 0.9804 - accuracy: 0.6690 - val_loss: 0.5887 - val_accuracy: 0.8324\n",
      "Epoch 14/25\n",
      "31/31 [==============================] - 11s 360ms/step - loss: 0.8917 - accuracy: 0.7034 - val_loss: 0.4440 - val_accuracy: 0.8706\n",
      "Epoch 15/25\n",
      "31/31 [==============================] - 11s 343ms/step - loss: 0.8846 - accuracy: 0.6984 - val_loss: 0.5010 - val_accuracy: 0.8647\n",
      "Epoch 16/25\n",
      "31/31 [==============================] - 11s 355ms/step - loss: 0.9566 - accuracy: 0.6872 - val_loss: 0.4309 - val_accuracy: 0.8941\n",
      "Epoch 17/25\n",
      "31/31 [==============================] - 10s 334ms/step - loss: 0.8176 - accuracy: 0.7267 - val_loss: 0.4382 - val_accuracy: 0.8912\n",
      "Epoch 18/25\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 0.8316 - accuracy: 0.7196 - val_loss: 0.4257 - val_accuracy: 0.8794\n",
      "Epoch 19/25\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.8397 - accuracy: 0.7087 - val_loss: 0.4279 - val_accuracy: 0.8765\n",
      "Epoch 20/25\n",
      "31/31 [==============================] - 11s 347ms/step - loss: 0.8603 - accuracy: 0.7034 - val_loss: 0.4167 - val_accuracy: 0.8824\n",
      "Epoch 21/25\n",
      "31/31 [==============================] - 11s 353ms/step - loss: 0.7973 - accuracy: 0.7359 - val_loss: 0.3874 - val_accuracy: 0.8824\n",
      "Epoch 22/25\n",
      "31/31 [==============================] - 11s 345ms/step - loss: 0.7471 - accuracy: 0.7419 - val_loss: 0.3886 - val_accuracy: 0.8882\n",
      "Epoch 23/25\n",
      "31/31 [==============================] - 12s 386ms/step - loss: 0.7326 - accuracy: 0.7540 - val_loss: 0.4485 - val_accuracy: 0.8765\n",
      "Epoch 24/25\n",
      "31/31 [==============================] - 11s 366ms/step - loss: 0.7358 - accuracy: 0.7642 - val_loss: 0.4744 - val_accuracy: 0.8735\n",
      "Epoch 25/25\n",
      "31/31 [==============================] - 11s 355ms/step - loss: 0.7132 - accuracy: 0.7530 - val_loss: 0.3881 - val_accuracy: 0.8853\n",
      "Evaluating after initialization...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.81      0.81      0.81        16\n",
      "   buttercup       1.00      0.83      0.91        24\n",
      "   coltsfoot       1.00      0.85      0.92        20\n",
      "     cowslip       1.00      0.69      0.82        13\n",
      "      crocus       0.85      0.94      0.89        18\n",
      "    daffodil       0.81      0.96      0.88        23\n",
      "       daisy       1.00      1.00      1.00        22\n",
      "   dandelion       0.94      1.00      0.97        17\n",
      "  fritillary       0.91      0.88      0.89        24\n",
      "        iris       0.89      0.94      0.92        18\n",
      "  lilyvalley       1.00      0.83      0.91        18\n",
      "       pansy       0.93      0.87      0.90        15\n",
      "    snowdrop       0.65      0.94      0.77        18\n",
      "   sunflower       1.00      0.91      0.95        23\n",
      "   tigerlily       0.96      0.96      0.96        26\n",
      "       tulip       0.54      0.68      0.60        19\n",
      "  windflower       0.96      0.85      0.90        26\n",
      "\n",
      "    accuracy                           0.89       340\n",
      "   macro avg       0.90      0.88      0.88       340\n",
      "weighted avg       0.90      0.89      0.89       340\n",
      "\n",
      "Re-compiling model...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 31 steps, validate on 340 samples\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 12s 373ms/step - loss: 0.5760 - accuracy: 0.8006 - val_loss: 0.3208 - val_accuracy: 0.8971\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 11s 343ms/step - loss: 0.4744 - accuracy: 0.8330 - val_loss: 0.3241 - val_accuracy: 0.9088\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.4796 - accuracy: 0.8431 - val_loss: 0.2780 - val_accuracy: 0.9088\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 10s 337ms/step - loss: 0.4876 - accuracy: 0.8350 - val_loss: 0.3240 - val_accuracy: 0.9000\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 11s 353ms/step - loss: 0.4387 - accuracy: 0.8482 - val_loss: 0.2860 - val_accuracy: 0.9147\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 11s 362ms/step - loss: 0.4083 - accuracy: 0.8573 - val_loss: 0.3198 - val_accuracy: 0.9029\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.4321 - accuracy: 0.8512 - val_loss: 0.2958 - val_accuracy: 0.9206\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 0.4017 - accuracy: 0.8664 - val_loss: 0.2964 - val_accuracy: 0.9059\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.4136 - accuracy: 0.8522 - val_loss: 0.2810 - val_accuracy: 0.9088\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 11s 347ms/step - loss: 0.4196 - accuracy: 0.8492 - val_loss: 0.2588 - val_accuracy: 0.9176\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 11s 345ms/step - loss: 0.3717 - accuracy: 0.8684 - val_loss: 0.2873 - val_accuracy: 0.9235\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 10s 338ms/step - loss: 0.3896 - accuracy: 0.8522 - val_loss: 0.2833 - val_accuracy: 0.9118\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.3677 - accuracy: 0.8684 - val_loss: 0.2797 - val_accuracy: 0.9176\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.3564 - accuracy: 0.8563 - val_loss: 0.2955 - val_accuracy: 0.9176\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 10s 338ms/step - loss: 0.3830 - accuracy: 0.8674 - val_loss: 0.2468 - val_accuracy: 0.9235\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 11s 345ms/step - loss: 0.3560 - accuracy: 0.8674 - val_loss: 0.2716 - val_accuracy: 0.9147\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.3653 - accuracy: 0.8644 - val_loss: 0.2507 - val_accuracy: 0.9235\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 11s 365ms/step - loss: 0.3672 - accuracy: 0.8715 - val_loss: 0.2584 - val_accuracy: 0.9176\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.3442 - accuracy: 0.8806 - val_loss: 0.3059 - val_accuracy: 0.9176\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.3788 - accuracy: 0.8603 - val_loss: 0.2341 - val_accuracy: 0.9265\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.3267 - accuracy: 0.8826 - val_loss: 0.2445 - val_accuracy: 0.9294\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 11s 347ms/step - loss: 0.3123 - accuracy: 0.8957 - val_loss: 0.2762 - val_accuracy: 0.9118\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 0.2993 - accuracy: 0.8887 - val_loss: 0.2318 - val_accuracy: 0.9324\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 11s 340ms/step - loss: 0.3166 - accuracy: 0.8937 - val_loss: 0.2548 - val_accuracy: 0.9353\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.2957 - accuracy: 0.8968 - val_loss: 0.2588 - val_accuracy: 0.9206\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.3370 - accuracy: 0.8806 - val_loss: 0.2396 - val_accuracy: 0.9294\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.3368 - accuracy: 0.8775 - val_loss: 0.2235 - val_accuracy: 0.9235\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.3504 - accuracy: 0.8634 - val_loss: 0.2373 - val_accuracy: 0.9265\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 11s 364ms/step - loss: 0.3118 - accuracy: 0.8887 - val_loss: 0.2498 - val_accuracy: 0.9382\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.3029 - accuracy: 0.8937 - val_loss: 0.2189 - val_accuracy: 0.9382\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 11s 360ms/step - loss: 0.3016 - accuracy: 0.8917 - val_loss: 0.2283 - val_accuracy: 0.9324\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 0.2906 - accuracy: 0.8907 - val_loss: 0.2627 - val_accuracy: 0.9206\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.2766 - accuracy: 0.8957 - val_loss: 0.2233 - val_accuracy: 0.9265\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.2727 - accuracy: 0.8998 - val_loss: 0.2636 - val_accuracy: 0.9382\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.2883 - accuracy: 0.9079 - val_loss: 0.2417 - val_accuracy: 0.9235\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 10s 332ms/step - loss: 0.2760 - accuracy: 0.8998 - val_loss: 0.2517 - val_accuracy: 0.9235\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 11s 358ms/step - loss: 0.2820 - accuracy: 0.9008 - val_loss: 0.2426 - val_accuracy: 0.9382\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.2808 - accuracy: 0.9059 - val_loss: 0.2274 - val_accuracy: 0.9235\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 10s 336ms/step - loss: 0.3125 - accuracy: 0.8856 - val_loss: 0.2606 - val_accuracy: 0.9235\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 11s 355ms/step - loss: 0.2588 - accuracy: 0.9160 - val_loss: 0.2104 - val_accuracy: 0.9353\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.2494 - accuracy: 0.9150 - val_loss: 0.2485 - val_accuracy: 0.9294\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 11s 365ms/step - loss: 0.3033 - accuracy: 0.8877 - val_loss: 0.2209 - val_accuracy: 0.9441\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.2619 - accuracy: 0.9038 - val_loss: 0.2043 - val_accuracy: 0.9382\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.2351 - accuracy: 0.9190 - val_loss: 0.2245 - val_accuracy: 0.9206\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 11s 361ms/step - loss: 0.2702 - accuracy: 0.8947 - val_loss: 0.2393 - val_accuracy: 0.9294\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 11s 361ms/step - loss: 0.2717 - accuracy: 0.9008 - val_loss: 0.2319 - val_accuracy: 0.9353\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 12s 392ms/step - loss: 0.2503 - accuracy: 0.9180 - val_loss: 0.2133 - val_accuracy: 0.9294\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 11s 364ms/step - loss: 0.2549 - accuracy: 0.9190 - val_loss: 0.2076 - val_accuracy: 0.9324\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 11s 367ms/step - loss: 0.2350 - accuracy: 0.9150 - val_loss: 0.2204 - val_accuracy: 0.9382\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 11s 362ms/step - loss: 0.2399 - accuracy: 0.9271 - val_loss: 0.2405 - val_accuracy: 0.9324\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 11s 361ms/step - loss: 0.2413 - accuracy: 0.9170 - val_loss: 0.2353 - val_accuracy: 0.9324\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.2449 - accuracy: 0.9079 - val_loss: 0.2075 - val_accuracy: 0.9294\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 11s 363ms/step - loss: 0.3042 - accuracy: 0.8988 - val_loss: 0.1949 - val_accuracy: 0.9382\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 11s 357ms/step - loss: 0.2812 - accuracy: 0.8988 - val_loss: 0.2348 - val_accuracy: 0.9324\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 11s 351ms/step - loss: 0.2253 - accuracy: 0.9200 - val_loss: 0.2272 - val_accuracy: 0.9382\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 11s 369ms/step - loss: 0.2293 - accuracy: 0.9170 - val_loss: 0.2156 - val_accuracy: 0.9353\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 11s 349ms/step - loss: 0.2218 - accuracy: 0.9302 - val_loss: 0.2269 - val_accuracy: 0.9382\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.2464 - accuracy: 0.9089 - val_loss: 0.2345 - val_accuracy: 0.9353\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 11s 341ms/step - loss: 0.2660 - accuracy: 0.9008 - val_loss: 0.2136 - val_accuracy: 0.9382\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 11s 352ms/step - loss: 0.2515 - accuracy: 0.9079 - val_loss: 0.2270 - val_accuracy: 0.9412\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 11s 357ms/step - loss: 0.1950 - accuracy: 0.9352 - val_loss: 0.2300 - val_accuracy: 0.9324\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.2331 - accuracy: 0.9251 - val_loss: 0.2140 - val_accuracy: 0.9471\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 11s 341ms/step - loss: 0.2509 - accuracy: 0.9119 - val_loss: 0.2114 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.2611 - accuracy: 0.8998 - val_loss: 0.2303 - val_accuracy: 0.9324\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 11s 343ms/step - loss: 0.2544 - accuracy: 0.9079 - val_loss: 0.2256 - val_accuracy: 0.9412\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 11s 359ms/step - loss: 0.2511 - accuracy: 0.9119 - val_loss: 0.2264 - val_accuracy: 0.9353\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.2159 - accuracy: 0.9291 - val_loss: 0.2302 - val_accuracy: 0.9441\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 11s 358ms/step - loss: 0.2323 - accuracy: 0.9130 - val_loss: 0.2120 - val_accuracy: 0.9294\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 11s 343ms/step - loss: 0.2151 - accuracy: 0.9251 - val_loss: 0.2279 - val_accuracy: 0.9382\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 11s 356ms/step - loss: 0.2072 - accuracy: 0.9231 - val_loss: 0.2154 - val_accuracy: 0.9382\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 11s 343ms/step - loss: 0.2222 - accuracy: 0.9271 - val_loss: 0.1844 - val_accuracy: 0.9559\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 12s 382ms/step - loss: 0.1863 - accuracy: 0.9342 - val_loss: 0.2190 - val_accuracy: 0.9471\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.2121 - accuracy: 0.9231 - val_loss: 0.2128 - val_accuracy: 0.9412\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 11s 364ms/step - loss: 0.2163 - accuracy: 0.9251 - val_loss: 0.2038 - val_accuracy: 0.9412\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 12s 376ms/step - loss: 0.2177 - accuracy: 0.9150 - val_loss: 0.2032 - val_accuracy: 0.9412\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 12s 388ms/step - loss: 0.2070 - accuracy: 0.9271 - val_loss: 0.2052 - val_accuracy: 0.9382\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 12s 396ms/step - loss: 0.2065 - accuracy: 0.9322 - val_loss: 0.2156 - val_accuracy: 0.9382\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 11s 367ms/step - loss: 0.1637 - accuracy: 0.9383 - val_loss: 0.2203 - val_accuracy: 0.9382\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 11s 350ms/step - loss: 0.2191 - accuracy: 0.9160 - val_loss: 0.2079 - val_accuracy: 0.9382\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 10s 338ms/step - loss: 0.2109 - accuracy: 0.9261 - val_loss: 0.1882 - val_accuracy: 0.9441\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 11s 354ms/step - loss: 0.2115 - accuracy: 0.9221 - val_loss: 0.2042 - val_accuracy: 0.9471\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 11s 348ms/step - loss: 0.2162 - accuracy: 0.9143 - val_loss: 0.1932 - val_accuracy: 0.9500\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 11s 357ms/step - loss: 0.2102 - accuracy: 0.9281 - val_loss: 0.2119 - val_accuracy: 0.9441\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 12s 373ms/step - loss: 0.2211 - accuracy: 0.9261 - val_loss: 0.2411 - val_accuracy: 0.9353\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 11s 367ms/step - loss: 0.2012 - accuracy: 0.9312 - val_loss: 0.1991 - val_accuracy: 0.9353\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 12s 376ms/step - loss: 0.1986 - accuracy: 0.9281 - val_loss: 0.1880 - val_accuracy: 0.9441\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 11s 364ms/step - loss: 0.1742 - accuracy: 0.9342 - val_loss: 0.2025 - val_accuracy: 0.9441\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 11s 367ms/step - loss: 0.2104 - accuracy: 0.9190 - val_loss: 0.1815 - val_accuracy: 0.9412\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 11s 368ms/step - loss: 0.1999 - accuracy: 0.9332 - val_loss: 0.1868 - val_accuracy: 0.9412\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 12s 372ms/step - loss: 0.1941 - accuracy: 0.9231 - val_loss: 0.2209 - val_accuracy: 0.9294\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 11s 355ms/step - loss: 0.1816 - accuracy: 0.9383 - val_loss: 0.1927 - val_accuracy: 0.9471\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 11s 352ms/step - loss: 0.2095 - accuracy: 0.9312 - val_loss: 0.1763 - val_accuracy: 0.9529\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 11s 353ms/step - loss: 0.1934 - accuracy: 0.9302 - val_loss: 0.2052 - val_accuracy: 0.9412\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 11s 357ms/step - loss: 0.1947 - accuracy: 0.9271 - val_loss: 0.2112 - val_accuracy: 0.9471\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 11s 357ms/step - loss: 0.2146 - accuracy: 0.9190 - val_loss: 0.2006 - val_accuracy: 0.9382\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 12s 376ms/step - loss: 0.1924 - accuracy: 0.9241 - val_loss: 0.1964 - val_accuracy: 0.9382\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 11s 370ms/step - loss: 0.1835 - accuracy: 0.9332 - val_loss: 0.1984 - val_accuracy: 0.9471\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 12s 380ms/step - loss: 0.2136 - accuracy: 0.9241 - val_loss: 0.2077 - val_accuracy: 0.9441\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 12s 378ms/step - loss: 0.2232 - accuracy: 0.9261 - val_loss: 0.1858 - val_accuracy: 0.9441\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 11s 370ms/step - loss: 0.1919 - accuracy: 0.9271 - val_loss: 0.2328 - val_accuracy: 0.9294\n",
      "Evaluating after fine-tuning...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    bluebell       0.86      0.75      0.80        16\n",
      "   buttercup       0.96      1.00      0.98        24\n",
      "   coltsfoot       1.00      0.90      0.95        20\n",
      "     cowslip       0.86      0.92      0.89        13\n",
      "      crocus       0.86      1.00      0.92        18\n",
      "    daffodil       0.91      0.91      0.91        23\n",
      "       daisy       1.00      1.00      1.00        22\n",
      "   dandelion       1.00      1.00      1.00        17\n",
      "  fritillary       0.96      0.92      0.94        24\n",
      "        iris       0.95      1.00      0.97        18\n",
      "  lilyvalley       1.00      0.89      0.94        18\n",
      "       pansy       1.00      0.87      0.93        15\n",
      "    snowdrop       0.80      0.89      0.84        18\n",
      "   sunflower       1.00      0.91      0.95        23\n",
      "   tigerlily       1.00      0.92      0.96        26\n",
      "       tulip       0.72      0.95      0.82        19\n",
      "  windflower       0.96      0.92      0.94        26\n",
      "\n",
      "    accuracy                           0.93       340\n",
      "   macro avg       0.93      0.93      0.93       340\n",
      "weighted avg       0.94      0.93      0.93       340\n",
      "\n",
      "Saving the model to disk...\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: flowers17.model\\assets\n"
     ]
    }
   ],
   "source": [
    "fineTune('../datasets/flowers17/images', 'flowers17.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 500/3000\n",
      "[INFO] processed 1000/3000\n",
      "[INFO] processed 1500/3000\n",
      "[INFO] processed 2000/3000\n",
      "[INFO] processed 2500/3000\n",
      "[INFO] processed 3000/3000\n",
      "Compiling model...\n",
      "Training head...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 70 steps, validate on 750 samples\n",
      "Epoch 1/25\n",
      "70/70 [==============================] - 26s 373ms/step - loss: 2.0109 - accuracy: 0.5821 - val_loss: 0.5065 - val_accuracy: 0.7573\n",
      "Epoch 2/25\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.6699 - accuracy: 0.7187 - val_loss: 0.2946 - val_accuracy: 0.8813\n",
      "Epoch 3/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.5606 - accuracy: 0.7705 - val_loss: 0.2877 - val_accuracy: 0.9000\n",
      "Epoch 4/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.4906 - accuracy: 0.8057 - val_loss: 0.2400 - val_accuracy: 0.9053\n",
      "Epoch 5/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.4733 - accuracy: 0.8165 - val_loss: 0.2946 - val_accuracy: 0.8960\n",
      "Epoch 6/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.4043 - accuracy: 0.8463 - val_loss: 0.2622 - val_accuracy: 0.9080\n",
      "Epoch 7/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.4069 - accuracy: 0.8314 - val_loss: 0.2836 - val_accuracy: 0.8787\n",
      "Epoch 8/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.3814 - accuracy: 0.8476 - val_loss: 0.1962 - val_accuracy: 0.9240\n",
      "Epoch 9/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.3509 - accuracy: 0.8589 - val_loss: 0.2355 - val_accuracy: 0.9147\n",
      "Epoch 10/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.3361 - accuracy: 0.8693 - val_loss: 0.2144 - val_accuracy: 0.9200\n",
      "Epoch 11/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.3364 - accuracy: 0.8616 - val_loss: 0.1932 - val_accuracy: 0.9373\n",
      "Epoch 12/25\n",
      "70/70 [==============================] - 22s 308ms/step - loss: 0.3519 - accuracy: 0.8629 - val_loss: 0.2582 - val_accuracy: 0.9133\n",
      "Epoch 13/25\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.3217 - accuracy: 0.8796 - val_loss: 0.2835 - val_accuracy: 0.9040\n",
      "Epoch 14/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2921 - accuracy: 0.8864 - val_loss: 0.2739 - val_accuracy: 0.9080\n",
      "Epoch 15/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.2901 - accuracy: 0.8778 - val_loss: 0.2278 - val_accuracy: 0.9213\n",
      "Epoch 16/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.3069 - accuracy: 0.8787 - val_loss: 0.2398 - val_accuracy: 0.9173\n",
      "Epoch 17/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2878 - accuracy: 0.8868 - val_loss: 0.2624 - val_accuracy: 0.9147\n",
      "Epoch 18/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.2775 - accuracy: 0.8954 - val_loss: 0.2330 - val_accuracy: 0.9213\n",
      "Epoch 19/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2952 - accuracy: 0.8922 - val_loss: 0.2173 - val_accuracy: 0.9293\n",
      "Epoch 20/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2846 - accuracy: 0.8936 - val_loss: 0.2076 - val_accuracy: 0.9413\n",
      "Epoch 21/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2827 - accuracy: 0.8877 - val_loss: 0.2781 - val_accuracy: 0.9067\n",
      "Epoch 22/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2817 - accuracy: 0.8954 - val_loss: 0.2258 - val_accuracy: 0.9293\n",
      "Epoch 23/25\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.2750 - accuracy: 0.8945 - val_loss: 0.2014 - val_accuracy: 0.9320\n",
      "Epoch 24/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2789 - accuracy: 0.8918 - val_loss: 0.2118 - val_accuracy: 0.9307\n",
      "Epoch 25/25\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2797 - accuracy: 0.8936 - val_loss: 0.3449 - val_accuracy: 0.9040\n",
      "Evaluating after initialization...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.97      0.79      0.87       246\n",
      "        dogs       0.84      0.93      0.88       244\n",
      "       panda       0.91      0.98      0.95       260\n",
      "\n",
      "    accuracy                           0.90       750\n",
      "   macro avg       0.91      0.90      0.90       750\n",
      "weighted avg       0.91      0.90      0.90       750\n",
      "\n",
      "Re-compiling model...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 70 steps, validate on 750 samples\n",
      "Epoch 1/100\n",
      "70/70 [==============================] - 22s 318ms/step - loss: 0.2368 - accuracy: 0.9062 - val_loss: 0.2360 - val_accuracy: 0.9333\n",
      "Epoch 2/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.2363 - accuracy: 0.9089 - val_loss: 0.2835 - val_accuracy: 0.9320\n",
      "Epoch 3/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.2218 - accuracy: 0.9143 - val_loss: 0.2272 - val_accuracy: 0.9240\n",
      "Epoch 4/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.1604 - accuracy: 0.9288 - val_loss: 0.1923 - val_accuracy: 0.9480\n",
      "Epoch 5/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1599 - accuracy: 0.9382 - val_loss: 0.1968 - val_accuracy: 0.9480\n",
      "Epoch 6/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1546 - accuracy: 0.9360 - val_loss: 0.1945 - val_accuracy: 0.9467\n",
      "Epoch 7/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1439 - accuracy: 0.9459 - val_loss: 0.1987 - val_accuracy: 0.9467\n",
      "Epoch 8/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1484 - accuracy: 0.9405 - val_loss: 0.1865 - val_accuracy: 0.9467\n",
      "Epoch 9/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1199 - accuracy: 0.9504 - val_loss: 0.2177 - val_accuracy: 0.9467\n",
      "Epoch 10/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.1406 - accuracy: 0.9495 - val_loss: 0.1876 - val_accuracy: 0.9507\n",
      "Epoch 11/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.1152 - accuracy: 0.9540 - val_loss: 0.2071 - val_accuracy: 0.9467\n",
      "Epoch 12/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.1186 - accuracy: 0.9540 - val_loss: 0.1988 - val_accuracy: 0.9493\n",
      "Epoch 13/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.1229 - accuracy: 0.9527 - val_loss: 0.1731 - val_accuracy: 0.9547\n",
      "Epoch 14/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.1192 - accuracy: 0.9500 - val_loss: 0.2258 - val_accuracy: 0.9467\n",
      "Epoch 15/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.1085 - accuracy: 0.9572 - val_loss: 0.1721 - val_accuracy: 0.9573\n",
      "Epoch 16/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.1055 - accuracy: 0.9576 - val_loss: 0.1913 - val_accuracy: 0.9440\n",
      "Epoch 17/100\n",
      "70/70 [==============================] - 22s 313ms/step - loss: 0.0921 - accuracy: 0.9612 - val_loss: 0.1690 - val_accuracy: 0.9587\n",
      "Epoch 18/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0990 - accuracy: 0.9621 - val_loss: 0.2000 - val_accuracy: 0.9573\n",
      "Epoch 19/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0940 - accuracy: 0.9644 - val_loss: 0.1603 - val_accuracy: 0.9573\n",
      "Epoch 20/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.1007 - accuracy: 0.9639 - val_loss: 0.1888 - val_accuracy: 0.9547\n",
      "Epoch 21/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0972 - accuracy: 0.9612 - val_loss: 0.1672 - val_accuracy: 0.9560\n",
      "Epoch 22/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0868 - accuracy: 0.9675 - val_loss: 0.1906 - val_accuracy: 0.9573\n",
      "Epoch 23/100\n",
      "70/70 [==============================] - 22s 314ms/step - loss: 0.0990 - accuracy: 0.9647 - val_loss: 0.1917 - val_accuracy: 0.9587\n",
      "Epoch 24/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0870 - accuracy: 0.9671 - val_loss: 0.1958 - val_accuracy: 0.9587\n",
      "Epoch 25/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.1024 - accuracy: 0.9608 - val_loss: 0.1640 - val_accuracy: 0.9600\n",
      "Epoch 26/100\n",
      "70/70 [==============================] - 22s 313ms/step - loss: 0.0923 - accuracy: 0.9653 - val_loss: 0.1938 - val_accuracy: 0.9560\n",
      "Epoch 27/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0665 - accuracy: 0.9775 - val_loss: 0.1815 - val_accuracy: 0.9587\n",
      "Epoch 28/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0738 - accuracy: 0.9739 - val_loss: 0.1819 - val_accuracy: 0.9533\n",
      "Epoch 29/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0829 - accuracy: 0.9675 - val_loss: 0.2363 - val_accuracy: 0.9587\n",
      "Epoch 30/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0849 - accuracy: 0.9698 - val_loss: 0.1852 - val_accuracy: 0.9600\n",
      "Epoch 31/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0665 - accuracy: 0.9766 - val_loss: 0.1837 - val_accuracy: 0.9560\n",
      "Epoch 32/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0730 - accuracy: 0.9720 - val_loss: 0.1598 - val_accuracy: 0.9640\n",
      "Epoch 33/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0595 - accuracy: 0.9770 - val_loss: 0.1811 - val_accuracy: 0.9613\n",
      "Epoch 34/100\n",
      "70/70 [==============================] - 22s 313ms/step - loss: 0.0685 - accuracy: 0.9766 - val_loss: 0.2000 - val_accuracy: 0.9573\n",
      "Epoch 35/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0691 - accuracy: 0.9734 - val_loss: 0.1928 - val_accuracy: 0.9547\n",
      "Epoch 36/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0836 - accuracy: 0.9734 - val_loss: 0.1500 - val_accuracy: 0.9627\n",
      "Epoch 37/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0565 - accuracy: 0.9806 - val_loss: 0.1637 - val_accuracy: 0.9613\n",
      "Epoch 38/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0620 - accuracy: 0.9775 - val_loss: 0.1736 - val_accuracy: 0.9600\n",
      "Epoch 39/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0474 - accuracy: 0.9820 - val_loss: 0.1836 - val_accuracy: 0.9627\n",
      "Epoch 40/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0606 - accuracy: 0.9797 - val_loss: 0.1806 - val_accuracy: 0.9587\n",
      "Epoch 41/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0503 - accuracy: 0.9806 - val_loss: 0.1691 - val_accuracy: 0.9587\n",
      "Epoch 42/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0629 - accuracy: 0.9775 - val_loss: 0.1637 - val_accuracy: 0.9613\n",
      "Epoch 43/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0543 - accuracy: 0.9802 - val_loss: 0.1872 - val_accuracy: 0.9573\n",
      "Epoch 44/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0572 - accuracy: 0.9757 - val_loss: 0.1715 - val_accuracy: 0.9587\n",
      "Epoch 45/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0657 - accuracy: 0.9743 - val_loss: 0.1707 - val_accuracy: 0.9627\n",
      "Epoch 46/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0417 - accuracy: 0.9838 - val_loss: 0.1999 - val_accuracy: 0.9587\n",
      "Epoch 47/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0416 - accuracy: 0.9860 - val_loss: 0.2006 - val_accuracy: 0.9587\n",
      "Epoch 48/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0539 - accuracy: 0.9829 - val_loss: 0.1660 - val_accuracy: 0.9627\n",
      "Epoch 49/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0673 - accuracy: 0.9779 - val_loss: 0.1710 - val_accuracy: 0.9613\n",
      "Epoch 50/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0468 - accuracy: 0.9829 - val_loss: 0.1834 - val_accuracy: 0.9560\n",
      "Epoch 51/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0434 - accuracy: 0.9856 - val_loss: 0.2138 - val_accuracy: 0.9573\n",
      "Epoch 52/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0441 - accuracy: 0.9811 - val_loss: 0.1943 - val_accuracy: 0.9627\n",
      "Epoch 53/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0426 - accuracy: 0.9833 - val_loss: 0.1636 - val_accuracy: 0.9653\n",
      "Epoch 54/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0586 - accuracy: 0.9802 - val_loss: 0.1703 - val_accuracy: 0.9627\n",
      "Epoch 55/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0580 - accuracy: 0.9802 - val_loss: 0.1794 - val_accuracy: 0.9573\n",
      "Epoch 56/100\n",
      "70/70 [==============================] - 22s 314ms/step - loss: 0.0402 - accuracy: 0.9847 - val_loss: 0.1623 - val_accuracy: 0.9627\n",
      "Epoch 57/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0428 - accuracy: 0.9833 - val_loss: 0.1884 - val_accuracy: 0.9627\n",
      "Epoch 58/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0391 - accuracy: 0.9860 - val_loss: 0.1724 - val_accuracy: 0.9613\n",
      "Epoch 59/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0459 - accuracy: 0.9865 - val_loss: 0.1884 - val_accuracy: 0.9613\n",
      "Epoch 60/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0384 - accuracy: 0.9860 - val_loss: 0.1532 - val_accuracy: 0.9667\n",
      "Epoch 61/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0411 - accuracy: 0.9842 - val_loss: 0.1553 - val_accuracy: 0.9680\n",
      "Epoch 62/100\n",
      "70/70 [==============================] - 22s 313ms/step - loss: 0.0354 - accuracy: 0.9902 - val_loss: 0.2010 - val_accuracy: 0.9573\n",
      "Epoch 63/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0387 - accuracy: 0.9847 - val_loss: 0.1804 - val_accuracy: 0.9640\n",
      "Epoch 64/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0266 - accuracy: 0.9910 - val_loss: 0.2057 - val_accuracy: 0.9653\n",
      "Epoch 65/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0432 - accuracy: 0.9851 - val_loss: 0.2152 - val_accuracy: 0.9560\n",
      "Epoch 66/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0416 - accuracy: 0.9851 - val_loss: 0.1581 - val_accuracy: 0.9627\n",
      "Epoch 67/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0304 - accuracy: 0.9901 - val_loss: 0.1917 - val_accuracy: 0.9613\n",
      "Epoch 68/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0354 - accuracy: 0.9865 - val_loss: 0.1617 - val_accuracy: 0.9653\n",
      "Epoch 69/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0304 - accuracy: 0.9860 - val_loss: 0.1746 - val_accuracy: 0.9640\n",
      "Epoch 70/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0498 - accuracy: 0.9851 - val_loss: 0.2125 - val_accuracy: 0.9600\n",
      "Epoch 71/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0301 - accuracy: 0.9896 - val_loss: 0.1929 - val_accuracy: 0.9627\n",
      "Epoch 72/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.0344 - accuracy: 0.9878 - val_loss: 0.1835 - val_accuracy: 0.9653\n",
      "Epoch 73/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.0338 - accuracy: 0.9865 - val_loss: 0.2058 - val_accuracy: 0.9573\n",
      "Epoch 74/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.0360 - accuracy: 0.9847 - val_loss: 0.1960 - val_accuracy: 0.9640\n",
      "Epoch 75/100\n",
      "70/70 [==============================] - 22s 309ms/step - loss: 0.0336 - accuracy: 0.9919 - val_loss: 0.2230 - val_accuracy: 0.9587\n",
      "Epoch 76/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0398 - accuracy: 0.9856 - val_loss: 0.1796 - val_accuracy: 0.9573\n",
      "Epoch 77/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0215 - accuracy: 0.9910 - val_loss: 0.1654 - val_accuracy: 0.9667\n",
      "Epoch 78/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0472 - accuracy: 0.9842 - val_loss: 0.1744 - val_accuracy: 0.9613\n",
      "Epoch 79/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0278 - accuracy: 0.9923 - val_loss: 0.1962 - val_accuracy: 0.9613\n",
      "Epoch 80/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0361 - accuracy: 0.9887 - val_loss: 0.1548 - val_accuracy: 0.9680\n",
      "Epoch 81/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0342 - accuracy: 0.9892 - val_loss: 0.1736 - val_accuracy: 0.9627\n",
      "Epoch 82/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0313 - accuracy: 0.9878 - val_loss: 0.1663 - val_accuracy: 0.9600\n",
      "Epoch 83/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0201 - accuracy: 0.9950 - val_loss: 0.1774 - val_accuracy: 0.9653\n",
      "Epoch 84/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0407 - accuracy: 0.9860 - val_loss: 0.1837 - val_accuracy: 0.9600\n",
      "Epoch 85/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.1783 - val_accuracy: 0.9600\n",
      "Epoch 86/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.1758 - val_accuracy: 0.9613\n",
      "Epoch 87/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0360 - accuracy: 0.9883 - val_loss: 0.1540 - val_accuracy: 0.9627\n",
      "Epoch 88/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0336 - accuracy: 0.9910 - val_loss: 0.1483 - val_accuracy: 0.9680\n",
      "Epoch 89/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0391 - accuracy: 0.9878 - val_loss: 0.1547 - val_accuracy: 0.9640\n",
      "Epoch 90/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0272 - accuracy: 0.9914 - val_loss: 0.5163 - val_accuracy: 0.9133\n",
      "Epoch 91/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0375 - accuracy: 0.9896 - val_loss: 0.1611 - val_accuracy: 0.9587\n",
      "Epoch 92/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0322 - accuracy: 0.9860 - val_loss: 0.1436 - val_accuracy: 0.9680\n",
      "Epoch 93/100\n",
      "70/70 [==============================] - 22s 310ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.1937 - val_accuracy: 0.9613\n",
      "Epoch 94/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.1778 - val_accuracy: 0.9640\n",
      "Epoch 95/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0183 - accuracy: 0.9946 - val_loss: 0.1666 - val_accuracy: 0.9680\n",
      "Epoch 96/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0319 - accuracy: 0.9892 - val_loss: 0.1599 - val_accuracy: 0.9667\n",
      "Epoch 97/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0254 - accuracy: 0.9919 - val_loss: 0.1952 - val_accuracy: 0.9600\n",
      "Epoch 98/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.1711 - val_accuracy: 0.9680\n",
      "Epoch 99/100\n",
      "70/70 [==============================] - 22s 312ms/step - loss: 0.0200 - accuracy: 0.9919 - val_loss: 0.1633 - val_accuracy: 0.9680\n",
      "Epoch 100/100\n",
      "70/70 [==============================] - 22s 311ms/step - loss: 0.0201 - accuracy: 0.9919 - val_loss: 0.2474 - val_accuracy: 0.9533\n",
      "Evaluating after fine-tuning...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        cats       0.99      0.91      0.95       246\n",
      "        dogs       0.92      0.96      0.94       244\n",
      "       panda       0.96      0.99      0.97       260\n",
      "\n",
      "    accuracy                           0.95       750\n",
      "   macro avg       0.95      0.95      0.95       750\n",
      "weighted avg       0.95      0.95      0.95       750\n",
      "\n",
      "Saving the model to disk...\n",
      "WARNING:tensorflow:From C:\\Users\\Ryan\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: animals.model\\assets\n"
     ]
    }
   ],
   "source": [
    "fineTune('../datasets/animals/images', 'animals.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
