{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import scale\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas\n",
    "\n",
    "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
    "np.set_printoptions(linewidth=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "The gradient OLS code from class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresGradient:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, x0, alpha, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.initialGuess = x0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)\n",
    "        # self.beta = self.gradientDescent(L,(self.d +  1) * [0], h, tolerance, maxIterations)\n",
    "        self.beta = self.gradientDescent(L, self.initialGuess, self.alpha, self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, h, tolerance, maxIterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == maxIterations-1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            x -= alpha*gradient\n",
    "            \n",
    "    # compute the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "        fx = f(x)\n",
    "\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - fx)/h\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient method for OLS with momentum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresGradientMomentum:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, x0, alpha, gamma, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.initialGuess = x0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y)\n",
    "        # self.beta = self.gradientDescent(L,(self.d +  1) * [0], h, tolerance, maxIterations)\n",
    "        self.beta = self.gradientDescent(L, self.initialGuess, self.alpha, self.gamma, self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, gamma, h, tolerance, maxIterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "        v = 0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == maxIterations-1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            v = gamma*v + alpha*gradient\n",
    "            x -= v\n",
    "            \n",
    "    # compute the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "        fx = f(x)\n",
    "\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - fx)/h\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \n",
      "\n",
      "Gradient descent took 5976 iterations to converge\n",
      "The norm of the gradient is 0.0\n",
      "\n",
      "The r^2 score is 0.8383446613817547\n",
      "The mean absolute error on the test set is 102985.33347096098 \n",
      "\n",
      "The time elapsed was 1.9966447353363037 \n",
      "\n",
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \n",
      "\n",
      "Gradient descent took 2880 iterations to converge\n",
      "The norm of the gradient is 0.0\n",
      "\n",
      "The r^2 score is 0.8383446613815551\n",
      "The mean absolute error on the test set is 102985.32709164586 \n",
      "\n",
      "The time elapsed was 1.0202727317810059\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pandas.read_csv('Mount_Pleasant_Real_Estate_Data.csv', sep=',').to_numpy()\n",
    "\n",
    "X = np.array(data[:,2:-1], dtype=float)\n",
    "y = np.array(data[:,1], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "trainX = scale(trainX)\n",
    "testX = scale(testX)\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE \\n')\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradient()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY, np.zeros(31), alpha = 0.0001, h = 0.0001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "# print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the beta values\n",
    "# print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "print('The time elapsed was', time.time() - startTime, '\\n')\n",
    "\n",
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \\n')\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresGradientMomentum()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY, np.zeros(31), alpha = 0.00001, gamma = 0.9, h = 0.00001, tolerance = 0.01, maxIterations = 100000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "# print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the beta values\n",
    "# print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "print('The time elapsed was', time.time() - startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the gradient method, we get mean absolute error 102985 on the test set and it runs in 2.04 seconds. With momentum, we get roughly the same accuracy, but it runs in only 0.94 seconds.\n",
    "\n",
    "# Problem 3\n",
    "\n",
    "For elastic net, we need a similar code to the previous problem, except the loss function will have $L^1$ and $L^2$ penalties on the parameters $\\beta$ with corresponding hyperparameters $\\lambda_1$ and $\\lambda_2$. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticNetGradientMomentum:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y, x0, alpha, gamma, lambda1, lambda2, h, tolerance, maxIterations):\n",
    "        self.n = X.shape[0]\n",
    "        self.d = X.shape[1]\n",
    "        self.h = h\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "        self.initialGuess = x0\n",
    "        \n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors via gradient descent\n",
    "        X = self.data\n",
    "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y) + self.lambda1 * beta.T @ beta + self.lambda2 * np.sum(np.abs(beta))\n",
    "        # self.beta = self.gradientDescent(L,(self.d +  1) * [0], h, tolerance, maxIterations)\n",
    "        self.beta = self.gradientDescent(L, self.initialGuess, self.alpha, self.gamma, self.lambda1, self.lambda2,\n",
    "                                         self.h, tolerance, maxIterations)\n",
    "                \n",
    "    # predict the output from testing data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint (rows of X)\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted\n",
    "\n",
    "    # run gradient descent to minimize the loss function\n",
    "    def gradientDescent(self, f, x0, alpha, gamma, lambda1, lambda2, h, tolerance, maxIterations):\n",
    "        # set x equal to the initial guess\n",
    "        x = x0\n",
    "        v = 0\n",
    "\n",
    "        # take up to maxIterations number of steps\n",
    "        for counter in range(maxIterations):\n",
    "            # update the gradient\n",
    "            gradient = self.computeGradient(f, x, h)\n",
    "\n",
    "            # stop if the norm of the gradient is near 0\n",
    "            if np.linalg.norm(gradient) < tolerance:\n",
    "                print('Gradient descent took', counter, 'iterations to converge')\n",
    "                print('The norm of the gradient is', np.linalg.norm(gradient))\n",
    "                # return the approximate critical value x\n",
    "                return x\n",
    "\n",
    "            # if we do not converge, print a message\n",
    "            elif counter == maxIterations-1:\n",
    "                print(\"Gradient descent failed\")\n",
    "                print('The gradient is', gradient)\n",
    "                # return x, sometimes it is still pretty good\n",
    "                return x\n",
    "\n",
    "            # take a step in the opposite direction as the gradient\n",
    "            v = gamma*v + alpha*gradient\n",
    "            x -= v\n",
    "            \n",
    "    # compute the gradient\n",
    "    def computeGradient(self, f, x, h):\n",
    "        n = len(x)\n",
    "        gradient = np.zeros(n)\n",
    "        fx = f(x)\n",
    "\n",
    "        for counter in range(n):\n",
    "            xUp = x.copy()\n",
    "            xUp[counter] += h\n",
    "            gradient[counter] = (f(xUp) - fx)/h\n",
    "\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \n",
      "\n",
      "Gradient descent failed\n",
      "The gradient is [    0.          903.3203125  6992.1875      175.78125     122.0703125   205.078125   1313.4765625 -1865.234375  -1982.421875   3364.2578125 -9111.328125     92.7734375\n",
      "  3833.0078125   839.84375      97.65625    1372.0703125   170.8984375  -405.2734375  -439.453125    239.2578125   263.671875   -302.734375    234.375      -463.8671875\n",
      "     0.         1982.421875      0.            0.         1147.4609375  -297.8515625 -6025.390625 ]\n",
      "\n",
      "The r^2 score is 0.8383372951895538\n",
      "The mean absolute error on the training set is 88233.7453366127\n",
      "The mean absolute error on the test set is 102920.50090670281 \n",
      "\n",
      "The time elapsed was 0.6871771812438965\n"
     ]
    }
   ],
   "source": [
    "print('FOR THE GRADIENT-BASED ORDINARY LEAST SQUARES CODE WITH MOMENTUM \\n')\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = ElasticNetGradientMomentum()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY, np.zeros(31), alpha = 0.0001, gamma = 0, lambda1 = 0.2, lambda2 = 0.4, h = 0.0001, tolerance = 0.01, maxIterations = 1000)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('\\nThe r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the beta values\n",
    "# print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions), '\\n')\n",
    "\n",
    "print('The time elapsed was', time.time() - startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
